"""
ğŸ’¡ LocalSynapse V2.0 - LM Studio Neural Pathway

Connexions neuronales locales avec UCB, circuit-breaker et EMA
"""

import json
import logging
import re
import time
from typing import Any

import httpx

from intelligence.synapses.timeout_config import TimeoutConfig


class LocalSynapse:
    """
    ğŸ’¡ SYNAPSE LOCALE V2.0 (LM Studio/Ollama)

    MÃ©taphore : Connexions neuronales rapides avec UCB + circuit-breaker
    - UCB bandit pour adaptation intelligente
    - Circuit-breaker avec hystÃ©rÃ©sis
    - EMA smoothing des mÃ©triques
    - Timeouts adaptatifs par stimulus
    - Validation reward-based
    - Post-traitement anti-dÃ©rive pour Mistral 7B
    """

    # ğŸš« LISTE NOIRE: Mots qui dÃ©clenchent des divagations
    DERIVE_TRIGGERS = [
        "en rÃ©sumÃ©", "on peut Ã©galement", "il est intÃ©ressant de noter",
        "pour comprendre cela", "de plus", "en outre", "par ailleurs",
        "ce phÃ©nomÃ¨ne peut aussi", "d'autres exemples incluent",
        "il faut noter que", "ainsi", "donc", "en effet"
    ]

    def __init__(self, config: dict):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Configuration synapse
        llm_config = config.get("llm", {})
        neural_config = config.get("neural_llm", {})

        self.endpoint = llm_config.get(
            "model_endpoint", "http://127.0.0.1:1234/v1/chat/completions"
        )
        self.model_name = llm_config.get("model_name", "mistralai/mistral-7b-instruct-v0.3")
        self.language = llm_config.get("language", "fr")  # Langue des rÃ©ponses
        
        # âš¡ ACTIVATION/DÃ‰SACTIVATION selon provider strategy
        llm_provider = llm_config.get("provider", "auto")
        local_llm_enabled = llm_config.get("local_llm", True)
        
        # Logique d'activation selon provider
        if llm_provider == "local":
            # Force local : activÃ© si local_llm=true
            self.is_enabled = local_llm_enabled
            reason = "forcÃ© via provider=local"
        elif llm_provider == "cloud":
            # Force cloud : local dÃ©sactivÃ©
            self.is_enabled = False
            reason = "dÃ©sactivÃ© via provider=cloud"
        elif llm_provider == "auto":
            # Auto : activÃ© si local_llm=true (UCB dÃ©cide)
            self.is_enabled = local_llm_enabled
            reason = "UCB auto" if local_llm_enabled else "local_llm=false"
        else:
            # Provider inconnu : fallback auto
            self.logger.warning(f"âš ï¸ Provider inconnu '{llm_provider}', fallback 'auto'")
            self.is_enabled = local_llm_enabled
            reason = "fallback auto"
        
        if self.is_enabled:
            self.logger.info(f"ğŸ’¡ LocalSynapse V2.0 ACTIVÃ‰E - {reason}")
        else:
            self.logger.info(f"ğŸ’¡ LocalSynapse DÃ‰SACTIVÃ‰E - {reason}")

        # ğŸ¬ DEBUG MODE: Afficher chunks streaming en temps rÃ©el
        # Support: debug_streaming: true OU stream_response_debug: "on"
        self.debug_streaming = (
            llm_config.get("debug_streaming", False) or
            llm_config.get("stream_response_debug", "").lower() == "on"
        )

        # âš¡ CIRCUIT BREAKER STATE
        self.failure_threshold = neural_config.get("local_failure_threshold", 3)
        self.recovery_time = neural_config.get("local_recovery_time", 300)
        self.circuit_state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        self.consecutive_failures = 0
        self.last_failure_time = 0.0

        # ğŸ“ˆ EMA METRICS
        self.ema_alpha = neural_config.get("ema_alpha", 0.1)
        self.ema_success_rate = 0.8
        self.ema_latency = 800.0

        # ğŸ° BANDIT STATE
        self.total_trials = 0
        self.success_trials = 0
        self.total_reward = 0.0

        # â±ï¸ TIMEOUTS HTTPX (4 valeurs obligatoires)
        # ChargÃ©s depuis config avec fallbacks via TimeoutConfig dataclass
        self.timeouts = TimeoutConfig.from_config(neural_config)
        
        self.logger.info(f"â±ï¸ Timeouts httpx: {self.timeouts}")

        # ğŸ“Š MÃ‰TRIQUES LOCALES
        self.response_times: list[float] = []

    def can_execute(self) -> bool:
        """âš¡ CIRCUIT BREAKER CHECK"""
        if not self.is_enabled:
            return False

        current_time = time.time()

        if self.circuit_state == "CLOSED":
            return True
        elif self.circuit_state == "OPEN":
            if current_time - self.last_failure_time > self.recovery_time:
                self.circuit_state = "HALF_OPEN"
                self.logger.info("âš¡ Circuit breaker LOCAL: OPEN â†’ HALF_OPEN")
                return True
            return False
        elif self.circuit_state == "HALF_OPEN":
            return True
        return False

    def _hard_truncate(self, response: str, max_chars: int = 400) -> str:
        """
        ğŸ”ª TRONCATURE BRUTALE - Coupe Ã  la derniÃ¨re phrase complÃ¨te avant limite

        Recommandation Mistral AI: Post-traitement obligatoire car Mistral 7B
        ignore parfois les limites sur sujets complexes.
        """
        if len(response) <= max_chars:
            return response

        # Coupe Ã  la derniÃ¨re phrase complÃ¨te avant la limite
        truncated = response[:max_chars]
        last_period = truncated.rfind('.')
        last_exclamation = truncated.rfind('!')
        last_question = truncated.rfind('?')

        # Trouver la derniÃ¨re ponctuation forte
        last_punct = max(last_period, last_exclamation, last_question)

        if last_punct != -1:
            return truncated[:last_punct + 1] + " ğŸ”š"
        else:
            # Pas de ponctuation trouvÃ©e : coupe brutale
            return truncated.rstrip() + "... ğŸ”š"

    def _remove_derives(self, response: str) -> str:
        """
        ğŸš« SUPPRESSION DES MOTS DÃ‰RIVANTS

        Certains mots dÃ©clenchent des divagations chez Mistral 7B.
        Coupe la rÃ©ponse dÃ¨s qu'un trigger est dÃ©tectÃ©.
        """
        response_lower = response.lower()

        for trigger in self.DERIVE_TRIGGERS:
            if trigger in response_lower:
                # Coupe avant le trigger
                idx = response_lower.index(trigger)
                return response[:idx].rstrip() + " ğŸ”š"

        return response

    async def fire(
        self,
        stimulus: str,
        context: str = "general",
        stimulus_class: str = "gen_short",
        correlation_id: str = "",
    ) -> str | None:
        """ğŸ”¥ TRANSMISSION SYNAPTIQUE LOCALE V2.0"""
        if not self.can_execute():
            return None

        # Pas de asyncio.wait_for() - laisser httpx gÃ©rer son timeout
        optimized_prompt = self._optimize_signal_for_local(stimulus, context, stimulus_class)

        start_time = time.time()
        try:
            # Httpx timeout interne gÃ©rÃ© dans _transmit_local_signal
            response = await self._transmit_local_signal(
                optimized_prompt, context, correlation_id, stimulus_class
            )

            latency = time.time() - start_time

            if response and self._is_valid_response(response, stimulus):
                reward = self._calculate_reward(response, stimulus, latency, 0)
                self._record_success(latency, reward)

                self.logger.info(
                    f"ğŸ’¡âœ… [{correlation_id}] Success {latency:.2f}s - Reward: {reward:.2f}"
                )
                return response
            else:
                self._record_failure("RÃ©ponse invalide")
                return None
        except httpx.ConnectError:
            self._record_failure("Connexion LM Studio impossible")
            return None
        except httpx.ReadTimeout:
            self._record_failure("LM Studio read timeout")
            return None
        except httpx.RemoteProtocolError as e:
            # LM Studio peut fermer la connexion prÃ©maturÃ©ment
            self._record_failure(f"LM Studio protocol error: {str(e)[:50]}")
            return None
        except httpx.HTTPStatusError as e:
            self._record_failure(f"HTTP {e.response.status_code}: {e.response.text[:100]}")
            return None
        except Exception as e:
            error_msg = str(e)
            # Channel Error = bug LM Studio connu (client disconnect pendant gÃ©nÃ©ration)
            if "Channel Error" in error_msg or "channel" in error_msg.lower():
                self._record_failure("LM Studio Channel Error - client dÃ©connectÃ© trop tÃ´t")
                self.logger.warning("ğŸ’¡ LM Studio Channel Error: Augmenter timeout ou rÃ©duire max_tokens")
            else:
                self._record_failure(f"Erreur LM Studio: {error_msg}")
            return None

    def _optimize_signal_for_local(self, stimulus: str, context: str, stimulus_class: str = "gen_short") -> list[dict[str, str]]:
        """ğŸ¯ OPTIMISATION SIGNAL LOCAL V2.0"""

        # BYPASS: Si context="direct", pas de wrapping (pour !joke POC)
        if context == "direct":
            return [{"role": "user", "content": stimulus}]

        bot_config = self.config.get("bot", {})
        bot_name = bot_config.get("name", "KissBot")
        personality = bot_config.get("personality", "sympa, direct, et passionnÃ© de tech")

        llm_config = self.config.get("llm", {})
        use_personality_mention = llm_config.get("use_personality_on_mention", True)
        use_personality_ask = llm_config.get("use_personality_on_ask", False)

        # Carte de langue pour format naturel
        language_map = {
            "fr": "EN FRANÃ‡AIS",
            "en": "IN ENGLISH",
            "es": "EN ESPAÃ‘OL",
            "de": "AUF DEUTSCH"
        }
        lang_directive = language_map.get(self.language, "EN FRANÃ‡AIS")

        # PROMPT OPTIMISÃ‰ VERSION 3 - LIBÃ‰RÃ‰
        # Plus de contraintes strictes, focus sur l'authenticitÃ©
        if context == "ask":
            if use_personality_ask:
                system_prompt = (
                    f"Tu es {bot_name}. {personality}\n"
                    f"RÃ©ponds {lang_directive} avec TON opinion, sans te brider. "
                    f"Vannes, clash, opinions tranchÃ©es = OK. Max 250 chars : {stimulus}"
                )
            else:
                system_prompt = (
                    f"RÃ©ponds {lang_directive} directement, avec ton expertise gaming. "
                    f"Pas de langue de bois. Max 250 chars : {stimulus}"
                )
        else:
            # Mentions : diffÃ©rencier gen_short vs gen_long
            if stimulus_class == "gen_long":
                # PROMPT LIBÃ‰RÃ‰ - Plus de crÃ©ativitÃ©
                if use_personality_mention:
                    system_prompt = (
                        f"Tu es {bot_name}. {personality}\n\n"
                        f"RÃ©ponds {lang_directive} avec TON style unique:\n"
                        f"- Sois drÃ´le, authentique, prends position\n"
                        f"- Vannes et clash sont OK si appropriÃ©s\n"
                        f"- 2-3 phrases max (â‰¤450 caractÃ¨res)\n"
                        f"- Utilise des emojis si Ã§a ajoute du fun\n\n"
                        f"Question: {stimulus}\n"
                        f"RÃ©ponse:"
                    )
                else:
                    system_prompt = (
                        f"RÃ©ponds {lang_directive} directement et sans filtre.\n"
                        f"2-3 phrases max (â‰¤450 chars). Sois concret et fun.\n\n"
                        f"Q: {stimulus}\n"
                        f"R:"
                    )
            else:
                # Questions simples/salutations : rÃ©ponses courtes PUNCHY
                if use_personality_mention:
                    system_prompt = (
                        f"Tu es {bot_name}. {personality}\n"
                        f"RÃ©ponds {lang_directive} MODE CASH:\n"
                        f"- Punchline MORDANTE qui fait rire\n"
                        f"- Sarcasme second degrÃ© assumÃ©\n"
                        f"- INTERDIT: gentillesse, politesse, gÃ©nÃ©rique\n"
                        f"- Comparaisons osÃ©es et mÃ©taphores trash = OK\n"
                        f"- 1-2 phrases qui MARQUENT (max 200 chars)\n"
                        f"Question: {stimulus}\n"
                        f"RÃ©ponse:"
                    )
                else:
                    system_prompt = (
                        f"RÃ©ponds {lang_directive} avec sarcasme MORDANT et vannes CASH. "
                        f"ZÃ©ro filtre, humour noir OK. Max 200 chars : {stimulus}"
                    )

        # Format user-only avec prompt intÃ©grÃ© (pas de sÃ©paration system/user)
        return [{"role": "user", "content": system_prompt}]

    async def _transmit_local_signal(
        self, messages: list[dict[str, str]], context: str, correlation_id: str, stimulus_class: str = "gen_short"
    ) -> str | None:
        """ğŸ“¡ TRANSMISSION LOCAL OPTIMISÃ‰E - Compatible Mistral/Qwen"""

        # Mistral 7B n'accepte PAS le role "system" - toujours user only
        # Conversion prÃ©ventive pour Ã©viter Channel Error
        if "mistral" in self.model_name.lower():
            messages = self._convert_to_user_only(messages)

        # ğŸ§  PARAMÃˆTRES D'INFÃ‰RENCE depuis config (avec fallbacks = valeurs optimisÃ©es)
        # Ces valeurs ont Ã©tÃ© optimisÃ©es suite Ã  tests A/B Mistral 7B v0.3
        llm_config = self.config.get("llm", {})
        inference_config = llm_config.get("inference", {})
        
        if context == "ask":
            # ğŸ¯ CONFIG OPTIMALE !ask (Mistral 7B Instruct v0.3)
            # Tests : 30/30 rÃ©ussis (tech + sciences), 0% dÃ©passements, longueur moy: 140 chars
            ask_config = inference_config.get("ask", {})
            max_tokens = ask_config.get("max_tokens", 200)
            temperature = ask_config.get("temperature", 0.3)
            repeat_penalty = ask_config.get("repeat_penalty", 1.1)
            stop_tokens = ask_config.get("stop_tokens", ["\n", "ğŸ”š"])
        elif context == "mention" and stimulus_class == "gen_long":
            # ğŸ”¥ GEN_LONG OPTIMAL (Mistral 7B Instruct v0.3)
            # Tests : 5/5 rÃ©ussis, 0% dÃ©passements >400 chars, ~130 chars moy
            gen_long_config = inference_config.get("gen_long", {})
            max_tokens = gen_long_config.get("max_tokens", 100)
            temperature = gen_long_config.get("temperature", 0.4)
            repeat_penalty = gen_long_config.get("repeat_penalty", 1.2)
            stop_tokens = gen_long_config.get("stop_tokens", ["ğŸ”š", "\n", "400.", "Exemple :", "En rÃ©sumÃ©,"])
        elif context == "mention":
            # ğŸ¯ GEN_SHORT OPTIMAL (Mistral 7B Instruct v0.3)
            # Tests : 45/45 rÃ©ussis, 0% dÃ©passements >200 chars, 55 chars moy, 95.6% emojis
            mention_config = inference_config.get("mention", {})
            max_tokens = mention_config.get("max_tokens", 200)
            temperature = mention_config.get("temperature", 0.7)
            repeat_penalty = mention_config.get("repeat_penalty", 1.1)
            stop_tokens = mention_config.get("stop_tokens", ["\n"])
        elif stimulus_class == "gen_long":
            gen_long_config = inference_config.get("gen_long", {})
            max_tokens = gen_long_config.get("max_tokens", 100)
            temperature = gen_long_config.get("temperature", 0.4)
            repeat_penalty = gen_long_config.get("repeat_penalty", 1.2)
            stop_tokens = gen_long_config.get("stop_tokens", ["ğŸ”š", "\n"])
        else:
            # Fallback: joke ou gen_short
            joke_config = inference_config.get("joke", {})
            max_tokens = joke_config.get("max_tokens", 150)
            temperature = joke_config.get("temperature", 0.7)
            repeat_penalty = joke_config.get("repeat_penalty", 1.1)
            stop_tokens = joke_config.get("stop_tokens", ["\n"])

        payload = {
            "model": self.model_name,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "repeat_penalty": repeat_penalty,
            "stop": stop_tokens,
            "stream": True,  # â† STREAMING ACTIVÃ‰ (accumulation)
        }

        # â±ï¸ TIMEOUTS HTTPX (4 paramÃ¨tres obligatoires - chargÃ©s depuis TimeoutConfig)
        async with httpx.AsyncClient(
            timeout=httpx.Timeout(**self.timeouts.to_httpx_timeout())
        ) as client:
            try:
                # ğŸŒŠ STREAMING AVEC ACCUMULATION (pas de spam chat)
                full_response = ""
                finish_reason = "unknown"

                # ğŸ¬ DEBUG: Header streaming
                if self.debug_streaming:
                    print("\nğŸŒŠ [STREAMING START] ", end="", flush=True)

                async with client.stream("POST", self.endpoint, json=payload) as response:
                    if response.status_code == 400:
                        error_text = await response.aread()
                        error_str = error_text.decode().lower()
                        if "system" in error_str or "role" in error_str:
                            self.logger.info(
                                f"ğŸ”„ Model {self.model_name} ne supporte pas 'system' - fallback user only"
                            )
                            # Retry avec user only
                            fallback_messages = self._convert_to_user_only(messages)
                            fallback_payload = {**payload, "messages": fallback_messages}

                            async with client.stream("POST", self.endpoint, json=fallback_payload) as retry_response:
                                retry_response.raise_for_status()
                                async for line in retry_response.aiter_lines():
                                    if line.startswith("data: "):
                                        chunk_data = line[6:]  # Remove "data: " prefix
                                        if chunk_data == "[DONE]":
                                            break
                                        try:
                                            chunk_json = json.loads(chunk_data)
                                            if "choices" in chunk_json and chunk_json["choices"]:
                                                delta = chunk_json["choices"][0].get("delta", {})
                                                if "content" in delta:
                                                    chunk_text = delta["content"]
                                                    full_response += chunk_text
                                                    # ğŸ¬ DEBUG: Afficher chunk en temps rÃ©el
                                                    if self.debug_streaming:
                                                        print(chunk_text, end="", flush=True)
                                                # Capture finish_reason
                                                finish_reason = chunk_json["choices"][0].get("finish_reason", finish_reason)
                                        except json.JSONDecodeError:
                                            continue
                        else:
                            raise httpx.HTTPStatusError(f"HTTP {response.status_code}", request=response.request, response=response)
                    else:
                        response.raise_for_status()
                        # Stream normal
                        async for line in response.aiter_lines():
                            if line.startswith("data: "):
                                chunk_data = line[6:]
                                if chunk_data == "[DONE]":
                                    break
                                try:
                                    chunk_json = json.loads(chunk_data)
                                    if "choices" in chunk_json and chunk_json["choices"]:
                                        delta = chunk_json["choices"][0].get("delta", {})
                                        if "content" in delta:
                                            chunk_text = delta["content"]
                                            full_response += chunk_text
                                            # ğŸ¬ DEBUG: Afficher chunk en temps rÃ©el
                                            if self.debug_streaming:
                                                print(chunk_text, end="", flush=True)
                                        # Capture finish_reason
                                        finish_reason = chunk_json["choices"][0].get("finish_reason", finish_reason)
                                except json.JSONDecodeError:
                                    continue

                # ğŸ¯ ENVOYER MESSAGE COMPLET APRÃˆS STOP_REASON
                if not full_response:
                    return None

            except (httpx.RemoteProtocolError, httpx.ReadError):
                # LM Studio Channel Error = bug systÃ¨me/user incompatible
                # RÃ©essayer directement avec user only
                self.logger.warning(
                    "ğŸ’¡ LM Studio Channel Error (system+user incompatible) - retry user only"
                )

                fallback_messages = self._convert_to_user_only(messages)
                fallback_payload = {**payload, "messages": fallback_messages}

                # Retry avec streaming
                full_response = ""
                finish_reason = "unknown"
                async with client.stream("POST", self.endpoint, json=fallback_payload) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            chunk_data = line[6:]
                            if chunk_data == "[DONE]":
                                break
                            try:
                                chunk_json = json.loads(chunk_data)
                                if "choices" in chunk_json and chunk_json["choices"]:
                                    delta = chunk_json["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        chunk_text = delta["content"]
                                        full_response += chunk_text
                                        # ğŸ¬ DEBUG: Afficher chunk en temps rÃ©el
                                        if self.debug_streaming:
                                            print(chunk_text, end="", flush=True)
                                    finish_reason = chunk_json["choices"][0].get("finish_reason", finish_reason)
                            except json.JSONDecodeError:
                                continue

                                # ğŸ¬ DEBUG: Footer streaming
                if self.debug_streaming:
                    print(f" [STREAMING END] finish_reason={finish_reason}\n", flush=True)

            except (httpx.RemoteProtocolError, httpx.ReadError):
                # LM Studio Channel Error = bug systÃ¨me/user incompatible
                # RÃ©essayer directement avec user only
                self.logger.warning(
                    "ğŸ’¡ LM Studio Channel Error (system+user incompatible) - retry user only"
                )

                fallback_messages = self._convert_to_user_only(messages)
                fallback_payload = {**payload, "messages": fallback_messages}

                # Retry avec streaming
                full_response = ""
                finish_reason = "unknown"
                async with client.stream("POST", self.endpoint, json=fallback_payload) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            chunk_data = line[6:]
                            if chunk_data == "[DONE]":
                                break
                            try:
                                chunk_json = json.loads(chunk_data)
                                if "choices" in chunk_json and chunk_json["choices"]:
                                    delta = chunk_json["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        chunk_text = delta["content"]
                                        full_response += chunk_text
                                        # ğŸ¬ DEBUG: Afficher chunk en temps rÃ©el
                                        if self.debug_streaming:
                                            print(chunk_text, end="", flush=True)
                                    finish_reason = chunk_json["choices"][0].get("finish_reason", finish_reason)
                            except json.JSONDecodeError:
                                continue

                # ğŸ¬ DEBUG: Footer streaming
                if self.debug_streaming:
                    print(f" [STREAMING END] finish_reason={finish_reason}\n", flush=True)

            # ğŸš¨ DÃ‰TECTION TRUNCATION (finish_reason: length)
            if finish_reason == "length":
                self.logger.warning(
                    f"âš ï¸ [LocalSynapse] Response TRUNCATED (finish_reason: length) "
                    f"- max_tokens={max_tokens} atteint ! Consider increasing."
                )

            # ğŸ§¹ POST-TRAITEMENT COMPLET
            cleaned = full_response.strip() if full_response else ""
            cleaned = self._remove_self_introduction(cleaned)

            # ğŸ¯ POST-TRAITEMENT ANTI-DÃ‰RIVE (Recommandation Mistral AI)
            # OBLIGATOIRE pour gen_long car Mistral 7B dÃ©passe parfois les limites
            if stimulus_class == "gen_long":
                cleaned = self._remove_derives(cleaned)  # Coupe les divagations
                cleaned = self._hard_truncate(cleaned, max_chars=400)  # Force â‰¤400 chars

            # ğŸ¯ SYSTÃˆME Ã€ DOUBLE SÃ‰CURITÃ‰ POUR !ask (Mistral 7B Instruct v0.3)
            # - Limite souple (guidage) : max_tokens=200 guide le modÃ¨le (voir ligne ~313)
            # - Limite brute (hard-cut) : 250 chars (200 + 25% marge) coupe brutalement
            # Tests prouvÃ©s : 30/30 rÃ©ussis, 0% dÃ©passements (tech: 138.8 chars, sciences: 142.0 chars)
            elif context == "ask":
                cleaned = self._hard_truncate(cleaned, max_chars=250)  # 200 + 25% marge

            # Ajouter ellipse si tronquÃ©
            if finish_reason == "length" and cleaned and not cleaned.endswith("..."):
                cleaned = cleaned.rstrip(".!?,;:") + "..."

            if cleaned and len(cleaned) >= 3:
                return cleaned

        return None

    def _remove_self_introduction(self, response: str) -> str:
        """ğŸ§¹ POST-TRAITEMENT : Supprimer auto-prÃ©sentation (recommandation Mistral AI)"""

        bot_name = self.config.get("bot", {}).get("name", "KissBot")

        # Patterns d'auto-prÃ©sentation Ã  supprimer
        patterns = [
            rf"Bonjour.*?{bot_name}[^.]*\.?\s*",  # "Bonjour ! Je suis KissBot..."
            rf"^Je suis {bot_name}[^.]*\.?\s*",    # "Je suis KissBot..."
            rf"^Moi,?\s*{bot_name}[^,!.]*[,!.]\s*",  # "Moi KissBot, ..." ou "Moi, KissBot..."
            rf"^Salut.*?{bot_name}[^.]*\.?\s*",   # "Salut ! Je suis KissBot..."
            rf"^{bot_name},\s*[^.]*\.?\s*",        # "KissBot, le bot..."
        ]

        cleaned = response
        for pattern in patterns:
            cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE)

        # Nettoyer les espaces/ponctuation rÃ©siduels
        cleaned = cleaned.strip(" ,.!").capitalize() if cleaned else response

        return cleaned if len(cleaned) >= 10 else response  # Fallback si trop court

    def _convert_to_user_only(self, messages: list[dict[str, str]]) -> list[dict[str, str]]:
        """ğŸ”„ CONVERSION POUR MODÃˆLES LEGACY (Mistral)"""
        system_content = ""
        user_content = ""

        for msg in messages:
            if msg["role"] == "system":
                system_content = msg["content"]
            elif msg["role"] == "user":
                user_content = msg["content"]

        # Fusion systÃ¨me + user en un seul message user
        if system_content:
            combined_content = f"{system_content} {user_content}"
        else:
            combined_content = user_content

        return [{"role": "user", "content": combined_content}]

    def _is_valid_response(self, response: str, stimulus: str) -> bool:
        """ğŸ–ï¸ VALIDATION RÃ‰PONSE LOCALE - Assouplie pour Mistral 7B"""
        if not response or len(response.strip()) < 3:
            return False

        # Mistral 7B peut retourner des rÃ©ponses courtes mais valides
        # Accepter mÃªme sans ponctuation finale si suffisamment long
        if len(response.strip()) >= 10:
            return True

        # RÃ©ponses ultra-courtes = invalides
        if response.lower() in ["ok", "oui", "non", "yes", "no"]:
            return False

        return True

    def _calculate_reward(
        self, response: str, stimulus: str, latency: float, retries: int
    ) -> float:
        """ğŸ–ï¸ REWARD SHAPING LOCAL V2.0"""
        base_reward = 1.0
        target_latency = 1.0  # Local = rapide
        latency_penalty = min(latency / target_latency, 1.0) * 0.3

        quality_bonus = 0.0
        if len(response) > 20:
            quality_bonus += 0.2
        if any(marker in response for marker in [".", "!", "?"]):
            quality_bonus += 0.1
        if any(emoji in response for emoji in ["ğŸ˜„", "ğŸ®", "ğŸ‘", "ğŸ”¥", "âš¡"]):
            quality_bonus += 0.15

        return max(base_reward - latency_penalty + quality_bonus, 0.1)

    def _record_success(self, latency: float, reward: float):
        """ğŸŸ¢ ENREGISTREMENT SUCCÃˆS LOCAL V2.0"""
        self.total_trials += 1
        self.success_trials += 1
        self.total_reward += reward

        self.ema_latency = self.ema_alpha * latency + (1 - self.ema_alpha) * self.ema_latency
        self.ema_success_rate = self.ema_alpha * 1.0 + (1 - self.ema_alpha) * self.ema_success_rate

        if self.circuit_state == "HALF_OPEN":
            self.circuit_state = "CLOSED"
            self.logger.info("âš¡ Circuit breaker LOCAL: HALF_OPEN â†’ CLOSED")
        self.consecutive_failures = 0

        self.response_times.append(latency)
        if len(self.response_times) > 20:
            self.response_times.pop(0)

    def _record_failure(self, error: str):
        """ğŸ”´ ENREGISTREMENT Ã‰CHEC LOCAL V2.0"""
        self.total_trials += 1
        self.consecutive_failures += 1
        self.last_failure_time = time.time()

        self.ema_success_rate = self.ema_alpha * 0.0 + (1 - self.ema_alpha) * self.ema_success_rate

        if self.consecutive_failures >= self.failure_threshold:
            if self.circuit_state != "OPEN":
                self.circuit_state = "OPEN"
                self.logger.error(
                    f"âš¡ Circuit breaker LOCAL: â†’ OPEN ({self.consecutive_failures} Ã©checs)"
                )
        elif self.circuit_state == "HALF_OPEN":
            self.circuit_state = "OPEN"
            self.logger.warning("âš¡ Circuit breaker LOCAL: HALF_OPEN â†’ OPEN (Ã©chec sonde)")

    def get_bandit_stats(self) -> dict[str, float]:
        """ğŸ° STATISTIQUES BANDIT LOCAL"""
        if self.total_trials == 0:
            return {"avg_reward": 0.0, "trials": 0, "ucb_score": float("inf")}

        avg_reward = self.total_reward / self.total_trials
        return {"avg_reward": avg_reward, "trials": self.total_trials, "ucb_score": avg_reward}

    def get_neural_metrics(self) -> dict[str, Any]:
        """ğŸ“Š MÃ‰TRIQUES NEURAL LOCAL V2.0"""
        time.time()

        # Diagnostic auto si Ã©checs rÃ©cents
        health_status = "healthy"
        if self.consecutive_failures >= 2:
            health_status = "degraded"
        elif self.circuit_state == "OPEN":
            health_status = "circuit_open"

        return {
            "synapse_type": "local_llm_v2",
            "is_enabled": self.is_enabled,
            "can_execute": self.can_execute(),
            "health_status": health_status,
            "endpoint": self.endpoint,
            "model_name": self.model_name,
            # Circuit Breaker
            "circuit_state": self.circuit_state,
            "consecutive_failures": self.consecutive_failures,
            "last_failure_time": self.last_failure_time,
            "failure_threshold": self.failure_threshold,
            "recovery_time": self.recovery_time,
            # UCB Bandit Stats
            "total_trials": self.total_trials,
            "success_trials": self.success_trials,
            "total_reward": self.total_reward,
            "avg_reward": self.total_reward / self.total_trials if self.total_trials > 0 else 0.0,
            "success_rate": (
                self.success_trials / self.total_trials if self.total_trials > 0 else 0.0
            ),
            # EMA Tracking
            "ema_latency": self.ema_latency,
            "ema_success_rate": self.ema_success_rate,
            "ema_alpha": self.ema_alpha,
            # Performance
            "recent_response_times": self.response_times[-5:],
            "avg_response_time": (
                sum(self.response_times) / len(self.response_times) if self.response_times else 0.0
            ),
            "last_success_time": getattr(self, "last_success_time", None),
            # Diagnostic hints
            "diagnostic_available": True,
            "needs_diagnostic": self.consecutive_failures >= 3 or self.circuit_state == "OPEN",
        }
