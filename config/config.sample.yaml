# ğŸš€ KissBot Neural V3.0 - Configuration Template
# 
# âš ï¸ IMPORTANT: Ceci est un TEMPLATE !
# 1. Copier ce fichier vers config.yaml
# 2. Remplacer toutes les valeurs "your_*_here" par vos vraies clÃ©s
# 3. NE JAMAIS committer config.yaml avec de vraies clÃ©s !
#
# ğŸ” Structure sÃ©curisÃ©e:
#   config.yaml         â† VOS vraies clÃ©s (gitignored)
#   config.sample.yaml  â† Template public (safe pour Git)
#   config.private.yaml â† Backup de vos vraies clÃ©s
#

apis:
  openai_key: your_openai_key_here
  rawg_key: your_rawg_key_here
  
  # â±ï¸ TIMEOUT API EXTERNE (RAWG, Steam, etc.)
  timeout: 10.0  # Secondes (APIs externes peuvent Ãªtre lentes)

# â±ï¸ TIMEOUTS TRANSPORTS (Phase 2.6)
timeouts:
  irc_send: 5.0       # Timeout envoi message IRC (Ã©vite blocages)
  helix_request: 8.0  # Timeout requÃªte Helix API
  llm_inference: 30.0 # Timeout infÃ©rence LLM (peut Ãªtre long)

bot:
  channel: test_channel
  cooldown: 5
  debug: true
  name: serda_bot
  personality: taquin, cash, second degrÃ©, amateur de roast amical et de tech
  
cache:
  max_size: 1000
  ttl_seconds: 3600

# ğŸ® COMMANDS CONFIGURATION
commands:
  cooldowns:
    ask: 10.0         # !ask <question>
    joke: 10.0        # !joke
    mention: 15.0     # @bot mention
    translate: 5.0    # !translate
    game_current: 5.0 # !gc
    game_info: 5.0    # !gi
  
  cache:
    joke_ttl: 300     # DurÃ©e cache blagues (5 minutes)
    joke_max_size: 100

# ğŸ“¢ STREAM ANNOUNCEMENTS (Phase 3.3)
announcements:
  # Stream monitoring settings
  monitoring:
    enabled: true  # Enable/disable all monitoring (polling + EventSub)
    method: auto   # "auto" (EventSub + polling fallback), "eventsub" (EventSub only), "polling" (polling only)
    polling_interval: 60  # Seconds between Helix API polls (fallback mode)
  
  # Stream ONLINE announcements
  stream_online:
    enabled: true  # Set to false to disable online announcements
    message: "ğŸ”´ @{channel} est maintenant en live ! ğŸ® {title}"
    # Available variables: {channel}, {title}, {game_name}, {viewer_count}
  
  # Stream OFFLINE announcements (usually disabled to avoid spam)
  stream_offline:
    enabled: false  # Set to true to enable offline announcements
    message: "ğŸ’¤ @{channel} est maintenant hors ligne. Ã€ bientÃ´t !"
    # Available variables: {channel}

llm:
  enabled: true
  fallback_mode: fun
  fallback_provider: none
  language: fr  # Langue des rÃ©ponses LLM (fr/en/es/de)
  local_llm: true
  max_tokens_ask: 200
  max_tokens_chill: 150
  model_endpoint: http://127.0.0.1:1234/v1/chat/completions
  model_name: qwen2.5-1.5b-instruct
  openai_model: gpt-3.5-turbo
  personality_only_on_cloud: true
  
  # ğŸ¯ PROVIDER STRATEGY
  # Options: local / cloud / auto
  provider: local
  
  stream_response_debug: "off"  # "on" pour debug streaming
  temperature_ask: 0.5
  temperature_chill: 0.8
  use_personality_on_ask: false
  use_personality_on_mention: true
  
  # ğŸ§  INFERENCE PARAMETERS (optimisÃ©s A/B testing)
  inference:
    ask:
      max_tokens: 200
      temperature: 0.3
      repeat_penalty: 1.1
      stop_tokens: ["\n", "ğŸ”š"]
    
    mention:
      max_tokens: 200
      temperature: 0.7
      repeat_penalty: 1.1
      stop_tokens: ["\n"]
    
    gen_long:
      max_tokens: 100
      temperature: 0.4
      repeat_penalty: 1.2
      stop_tokens: ["ğŸ”š", "\n", "400.", "Exemple :", "En rÃ©sumÃ©,"]
    
    joke:
      max_tokens: 150
      temperature: 0.7
      repeat_penalty: 1.1
      stop_tokens: ["\n"]
    
    cloud:
      max_tokens_short: 90
      max_tokens_long: 60
      temperature_short: 0.4
      temperature_long: 0.8
logging:
  file: serdabot_v2.log
  level: INFO
neural_llm:
  cloud_failure_threshold: 5
  cloud_recovery_time: 600
  ema_alpha: 0.2
  enabled: true
  local_failure_threshold: 3
  local_recovery_time: 300
  max_correlation_history: 100
  min_trials_per_synapse: 3
  
  # â±ï¸ TIMEOUTS HTTPX (4 valeurs obligatoires)
  # httpx.Timeout() EXIGE les 4 paramÃ¨tres
  timeout_connect: 5.0      # TCP connection (court)
  timeout_inference: 30.0   # LLM streaming (long)
  timeout_write: 10.0       # Send payload (moyen)
  timeout_pool: 5.0         # Connection pool (court)
  
  ucb_exploration_factor: 1.4
rawg:
  api_key: your_api_key_here
translation:
  enabled: true
  rate_limit: 5
  target_language: fr

twitch:
  bot_id: '1209350837'  # ID de votre bot (obtenu via Twitch Dev Console)
  broadcaster_id: 44456636  # ID du broadcaster principal
  
  # ğŸ“º Channels oÃ¹ le bot se connecte (IRC + Stream Monitoring)
  # Liste des channels Twitch Ã  rejoindre (sans le #)
  channels:
  - your_channel_here
  # - another_channel
  # - third_channel
  
  prefix: '!'
  client_id: your_client_id_here
  client_secret: your_client_secret_here
  
  # âš ï¸ TOKENS: Les tokens sont stockÃ©s dans .tio.tokens.json (gitignored)
  # Utilisez regenerate_bot_token.py pour gÃ©nÃ©rer les tokens
  # Ne pas mettre de tokens ici ! (obsolÃ¨te depuis Phase 3.3)
