# üöÄ KissBot Neural V3.0 - Configuration Template
# 
# ‚ö†Ô∏è IMPORTANT: Ceci est un TEMPLATE !
# 1. Copier ce fichier vers config.yaml
# 2. Remplacer toutes les valeurs "your_*_here" par vos vraies cl√©s
# 3. NE JAMAIS committer config.yaml avec de vraies cl√©s !
#
# üîê Structure s√©curis√©e:
#   config.yaml         ‚Üê VOS vraies cl√©s (gitignored)
#   config.sample.yaml  ‚Üê Template public (safe pour Git)
#   config.private.yaml ‚Üê Backup de vos vraies cl√©s
#

apis:
  openai_key: your_openai_key_here
  rawg_key: your_rawg_key_here
  
  # ‚è±Ô∏è TIMEOUT API EXTERNE (RAWG, Steam, etc.)
  timeout: 10.0  # Secondes (APIs externes peuvent √™tre lentes)

bot:
  channel: test_channel
  cooldown: 5
  debug: true
  name: serda_bot
  personality: taquin, cash, second degr√©, amateur de roast amical et de tech
  
cache:
  max_size: 1000
  ttl_seconds: 3600

# üéÆ COMMANDS CONFIGURATION
commands:
  cooldowns:
    ask: 10.0         # !ask <question>
    joke: 10.0        # !joke
    mention: 15.0     # @bot mention
    translate: 5.0    # !translate
    game_current: 5.0 # !gc
    game_info: 5.0    # !gi
  
  cache:
    joke_ttl: 300     # Dur√©e cache blagues (5 minutes)
    joke_max_size: 100

llm:
  enabled: true
  fallback_mode: fun
  fallback_provider: none
  language: fr  # Langue des r√©ponses LLM (fr/en/es/de)
  local_llm: true
  max_tokens_ask: 200
  max_tokens_chill: 150
  model_endpoint: http://127.0.0.1:1234/v1/chat/completions
  model_name: qwen2.5-1.5b-instruct
  openai_model: gpt-3.5-turbo
  personality_only_on_cloud: true
  
  # üéØ PROVIDER STRATEGY
  # Options: local / cloud / auto
  provider: local
  
  stream_response_debug: "off"  # "on" pour debug streaming
  temperature_ask: 0.5
  temperature_chill: 0.8
  use_personality_on_ask: false
  use_personality_on_mention: true
  
  # üß† INFERENCE PARAMETERS (optimis√©s A/B testing)
  inference:
    ask:
      max_tokens: 200
      temperature: 0.3
      repeat_penalty: 1.1
      stop_tokens: ["\n", "üîö"]
    
    mention:
      max_tokens: 200
      temperature: 0.7
      repeat_penalty: 1.1
      stop_tokens: ["\n"]
    
    gen_long:
      max_tokens: 100
      temperature: 0.4
      repeat_penalty: 1.2
      stop_tokens: ["üîö", "\n", "400.", "Exemple :", "En r√©sum√©,"]
    
    joke:
      max_tokens: 150
      temperature: 0.7
      repeat_penalty: 1.1
      stop_tokens: ["\n"]
    
    cloud:
      max_tokens_short: 90
      max_tokens_long: 60
      temperature_short: 0.4
      temperature_long: 0.8
logging:
  file: serdabot_v2.log
  level: INFO
neural_llm:
  cloud_failure_threshold: 5
  cloud_recovery_time: 600
  ema_alpha: 0.2
  enabled: true
  local_failure_threshold: 3
  local_recovery_time: 300
  max_correlation_history: 100
  min_trials_per_synapse: 3
  
  # ‚è±Ô∏è TIMEOUTS HTTPX (4 valeurs obligatoires)
  # httpx.Timeout() EXIGE les 4 param√®tres
  timeout_connect: 5.0      # TCP connection (court)
  timeout_inference: 30.0   # LLM streaming (long)
  timeout_write: 10.0       # Send payload (moyen)
  timeout_pool: 5.0         # Connection pool (court)
  
  ucb_exploration_factor: 1.4
rawg:
  api_key: your_api_key_here
translation:
  enabled: true
  rate_limit: 5
  target_language: fr
twitch:
  bot_id: '1209350837'
  channels:
  - el_serda
  client_id: m60vyrfkbrrdo3xp6h4rpneq6bh9d6
  client_secret: your_client_secret_here
  prefix: '!'
  refresh_token: your_refresh_token_here
  token: your_token_here
