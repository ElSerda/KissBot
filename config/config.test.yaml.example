apis:
  openai_key: "YOUR_OPENAI_KEY_HERE"
  rawg_key: "YOUR_RAWG_KEY_HERE"
  timeout: 10.0  # Timeout gÃ©nÃ©ral pour APIs externes (RAWG, OpenAI, etc.)

# ğŸ‰ DRAKON - Ultra-fast fuzzy search (Rust Î”â‚›Â³ V3)
drakon:
  enabled: true  # Enable DRAKON fast-path for game lookup
  url: http://127.0.0.1:8080  # DRAKON HTTP API endpoint
  timeout: 5.0  # Request timeout

# â±ï¸ Timeouts pour les transports (Phase 2.6)
timeouts:
  irc_send: 5.0       # Timeout envoi message IRC
  helix_request: 8.0  # Timeout requÃªte Helix API
  llm_inference: 30.0 # Timeout infÃ©rence LLM (peut Ãªtre long)
bot:
  cooldown: 5
  debug: true
  name: serda_bot
  personality: |
    Nom: serda_bot (bot Twitch). Tech: Python + asyncio + pyTwitchAPI + aiohttp.
    Ton: drÃ´le, sarcastique, direct, geek.
    
    Style de rÃ©ponse:
    - Phrases courtes et percutantes (1-2 max)
    - Emojis stylÃ©s: ğŸ˜ ğŸ”¥ ğŸ’» ğŸ
    - Jeux de mots et vannes amicales
    - Pas d'explications longues, reste punchy
    
    Cas spÃ©cifiques:
    - Question technique sur ton code â†’ "100% Python ğŸ, asyncio pour la perf, pyTwitchAPI pour Twitch !"
    - Question perso (Ã§a va?) â†’ "En mode AFK mental, et toi?" ou "Je carbure aux bits, et toi?"
    - Question vague â†’ Demande de prÃ©cision avec humour
cache:
  max_size: 1000
  ttl_seconds: 3600
commands:
  cooldowns:
    ask: 10.0
    joke: 10.0
    mention: 15.0
    translate: 5.0
    game_current: 5.0
    game_info: 5.0
    wiki: 5.0  # Plus court que !ask car pas de LLM
  cache:
    joke_ttl: 300
    joke_max_size: 100

# ğŸ“š Wikipedia configuration
wikipedia:
  lang: fr  # Langue par dÃ©faut (fr, en, es, etc.)

# ğŸ® Game Lookup configuration
game_lookup:
  min_confidence: 0.7  # Minimum confidence for fuzzy matches (0.0-1.0)
  max_results: 5       # Maximum results to return

# ğŸ“¢ Phase 3.3 - Stream Announcements (online/offline)
announcements:
  # Stream monitoring settings
  monitoring:
    enabled: true  # Enable/disable all monitoring (polling + EventSub)
    method: auto   # "auto" (EventSub + polling fallback), "eventsub" (EventSub only), "polling" (polling only)
    polling_interval: 60  # Seconds between Helix API polls (fallback mode)
  
  # Stream ONLINE announcements
  stream_online:
    enabled: true  # Set to false to disable online announcements
    message: "ğŸ”´ @{channel} est maintenant en live ! ğŸ® {title}"
    # Available variables: {channel}, {title}, {game_name}, {viewer_count}
  
  # Stream OFFLINE announcements (usually disabled to avoid spam)
  stream_offline:
    enabled: false  # Set to true to enable offline announcements
    message: "ğŸ’¤ @{channel} est maintenant hors ligne. Ã€ bientÃ´t !"
    # Available variables: {channel}
llm:
  enabled: true
  fallback_mode: fun
  fallback_provider: none
  language: fr
  local_llm: false
  max_tokens_ask: 200
  max_tokens_chill: 150
  model_endpoint: http://127.0.0.1:1234/v1/chat/completions
  model_name: mistralai/mistral-7b-instruct-v0.3
  openai_model: gpt-3.5-turbo
  personality_only_on_cloud: true
  provider: cloud
  inference:
    ask:
      max_tokens: 200
      temperature: 0.3
      repeat_penalty: 1.1
      stop_tokens:
      - "\n\n"
      - "ğŸ”š"  # Emoji END
    mention:
      max_tokens: 200
      temperature: 0.7
      repeat_penalty: 1.1
      stop_tokens:
      - "\n\n"
    gen_long:
      max_tokens: 100
      temperature: 0.4
      repeat_penalty: 1.2
      stop_tokens:
      - "ğŸ”š"  # Emoji END
      - "\n\n"
      - "400."
      - "Exemple :"
      - "En rÃ©sumÃ©,"
    joke:
      max_tokens: 150
      temperature: 0.7
      repeat_penalty: 1.1
      stop_tokens:
      - '

        '
    cloud:
      max_tokens_short: 90
      max_tokens_long: 60
      temperature_short: 0.4
      temperature_long: 0.8
  stream_response_debug: 'on'
  temperature_ask: 0.5
  temperature_chill: 0.8
  use_personality_on_ask: false
  use_personality_on_mention: true
logging:
  file: serdabot_v2.log
  level: DEBUG
neural_llm:
  cloud_failure_threshold: 5
  cloud_recovery_time: 600
  ema_alpha: 0.2
  enabled: true
  local_failure_threshold: 3
  local_recovery_time: 300
  max_correlation_history: 100
  min_trials_per_synapse: 3
  timeout_connect: 5.0
  timeout_inference: 30.0
  timeout_write: 10.0
  timeout_pool: 5.0
  ucb_exploration_factor: 1.4
rawg:
  api_key: 2be95642f3334633998bad840316f6fa
translation:
  enabled: true
  rate_limit: 5
  target_language: fr
twitch:
  bot_id: '1209350837'
  broadcaster_id: 44456636
  
  # ğŸ“º Channels oÃ¹ le bot se connecte (IRC + Stream Monitoring)
  # Liste des channels Twitch Ã  rejoindre (sans le #)
  channels:
  - your_test_channel
  # - autre_channel
  
  prefix: '!'
  client_id: "YOUR_TWITCH_CLIENT_ID"
  client_secret: "YOUR_TWITCH_CLIENT_SECRET"
  tokens:
    your_bot_account:
      user_id: "YOUR_BOT_USER_ID"
      access_token: "YOUR_BOT_ACCESS_TOKEN"
      refresh_token: "YOUR_BOT_REFRESH_TOKEN"
    your_test_channel:
      user_id: "YOUR_CHANNEL_USER_ID"
      access_token: "YOUR_CHANNEL_ACCESS_TOKEN"
      refresh_token: "YOUR_CHANNEL_REFRESH_TOKEN"
