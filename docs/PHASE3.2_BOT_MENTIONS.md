# Phase 3.2 - Bot Mentions Feature

## ğŸ“‹ RÃ©sumÃ©

Ajout de la dÃ©tection des mentions du bot (@bot_name ou bot_name) pour dÃ©clencher des rÃ©ponses LLM intelligentes, avec rate limiting de 15s par utilisateur.

## ğŸ¯ FonctionnalitÃ©s

### DÃ©tection de Mention

Le bot dÃ©tecte quand il est mentionnÃ© dans le chat et rÃ©pond intelligemment :

**Formats supportÃ©s :**
- `@serda_bot tu penses quoi de python?` â†’ DÃ©tectÃ©
- `serda_bot salut!` â†’ DÃ©tectÃ©  
- `SERDA_BOT comment Ã§a va?` â†’ DÃ©tectÃ© (case-insensitive)
- `hey @serda_bot Ã§a va?` â†’ DÃ©tectÃ© (mention dans le message)

**Formats ignorÃ©s :**
- `@other_bot salut` â†’ IgnorÃ© (mauvais bot)
- `hello world` â†’ IgnorÃ© (pas de mention)
- `!ask python` â†’ TraitÃ© comme commande

### PrioritÃ© des Mentions

Les mentions sont dÃ©tectÃ©es **avant** le routing des commandes. Cela signifie que `@serda_bot !ping` sera traitÃ© comme une mention, pas comme une commande `!ping`.

### Rate Limiting

- **Cooldown:** 15 secondes par utilisateur
- **Comportement:** Silent ignore (pas de message d'erreur)
- **Configurable:** Via `config.commands.cooldowns.mention`

### Context LLM

Les mentions utilisent `context="mention"` au lieu de `context="ask"`, ce qui permet au LLM d'adapter son comportement :
- Mentions = conversations plus naturelles
- !ask = rÃ©ponses plus factuelles

## ğŸ—ï¸ Architecture

### Flow d'ExÃ©cution

```
User Message: "@serda_bot salut"
     â†“
MessageHandler._handle_chat_message()
     â†“
extract_mention_message() â†’ "salut"
     â†“
_handle_mention() checks:
  - LLM disponible? âœ“
  - Rate limit OK? âœ“
     â†“
process_llm_request(context="mention")
     â†“
NeuralPathwayManager
     â†“
Response: "@user Salut ! Comment puis-je t'aider?"
```

### Modules ModifiÃ©s

#### `core/message_handler.py`

**Ajouts :**
```python
from modules.intelligence.core import extract_mention_message

# Rate limiting state
self._mention_last_time: Dict[str, float] = {}
self._mention_cooldown = config.get("commands", {}).get("cooldowns", {}).get("mention", 15.0)

# Detection dans _handle_chat_message()
bot_name = self.config.get("bot_login_name", "serda_bot")
mention_text = extract_mention_message(msg.text, bot_name)
if mention_text:
    await self._handle_mention(msg, mention_text)
    return  # Ne pas traiter comme commande

# Nouvelle mÃ©thode
async def _handle_mention(self, msg: ChatMessage, mention_text: str):
    """Traite une mention du bot avec LLM"""
    # Check LLM disponible
    # Check rate limiting (15s cooldown)
    # Call process_llm_request(context="mention")
    # Format & send response
```

**!help mis Ã  jour :**
```python
if self.llm_handler and self.llm_handler.is_available():
    commands_list += " !ask <question> | Mention @bot_name <message>"
```

#### `intelligence/core.py`

**Fix Case-Insensitive :**
```python
def extract_mention_message(message_content: str, bot_name: str) -> str | None:
    """Extrait le message aprÃ¨s @bot_name ou bot_name (case-insensitive)"""
    # Detection case-insensitive
    content_lower = message_content.lower()
    bot_lower = bot_name.lower()
    
    # Extraction avec regex case-insensitive
    pattern = rf"@?{re.escape(bot_name)}"
    message = re.sub(pattern, "", message_content, count=1, flags=re.IGNORECASE)
    return message.strip() if message else None
```

**Avant le fix :**
- `@SERDA_BOT hello` â†’ Ne marchait pas (replace() case-sensitive)

**AprÃ¨s le fix :**
- `@SERDA_BOT hello` â†’ âœ… Fonctionne

### Configuration

```yaml
bot_login_name: "serda_bot"  # Nom du bot pour dÃ©tection

commands:
  cooldowns:
    mention: 15.0  # Cooldown en secondes pour mentions
```

## âœ… Tests

### Test 1: Extraction de Mention

**Fichier:** `test_mention_detection.py`

**Tests :**
```python
âœ… @serda_bot tu penses quoi de python?
âœ… @serda_bot salut
âœ… @SERDA_BOT coucou (case-insensitive)
âœ… serda_bot c'est quoi ton avis?
âœ… SERDA_BOT hello
âœ… hey @serda_bot comment Ã§a va?
âœ… hello world â†’ None (pas de mention)
âœ… !ask python â†’ None
âœ… @other_bot salut â†’ None
```

**RÃ©sultat:** 9/9 tests passent âœ…

### Test 2: Rate Limiting

**Fichier:** `test_mention_ratelimit.py`

**ScÃ©nario :**
1. Message 1: `@serda_bot hello` â†’ âœ… TraitÃ© (LLM appelÃ©)
2. Message 2 (immÃ©diat): `@serda_bot bonjour` â†’ âœ… BloquÃ© (cooldown)
3. Attente 3.5s (cooldown configurÃ© Ã  3s pour test)
4. Message 3: `@serda_bot Ã§a va?` â†’ âœ… TraitÃ© (cooldown expirÃ©)

**RÃ©sultat:** Rate limiting fonctionne âœ…

### Test 3: IntÃ©gration ComplÃ¨te

**Fichier:** `test_mention_integration.py`

**Tests :**
```python
âœ… @serda_bot tu penses quoi de python? â†’ LLM appelÃ© avec context="mention"
âœ… serda_bot Ã§a va? â†’ LLM appelÃ© avec context="mention"
âœ… hello world â†’ IgnorÃ© (pas de mention)
âœ… @other_bot salut â†’ IgnorÃ© (mauvais bot)
```

**RÃ©sultat:** IntÃ©gration complÃ¨te validÃ©e âœ…

## ğŸ“Š MÃ©triques

### Performance

- **DÃ©tection:** ~0.1ms (extraction regex)
- **Rate limit check:** <0.01ms (dict lookup)
- **LLM response:** 1-3s (dÃ©pend du modÃ¨le)

### Utilisation

**Exemple en production :**
```
[20:15:30] user123: @serda_bot tu penses quoi de python?
[20:15:32] serda_bot: @user123 Python est un langage polyvalent...

[20:15:35] user123: @serda_bot et javascript?
[20:15:36] serda_bot: (silent ignore - cooldown actif)

[20:15:50] user123: @serda_bot et javascript?
[20:15:52] serda_bot: @user123 JavaScript est idÃ©al pour le web...
```

## ğŸ”„ Pattern vs TwitchIO

### Avant (TwitchIO)

```python
# Fonction sÃ©parÃ©e dans commands/intelligence_commands.py
async def handle_mention_v3(bot, message):
    bot_name = getattr(bot, 'bot_login_name', 'serda_bot')
    user_message = extract_mention_message(message.text, bot_name)
    
    if not bot.rate_limiter.is_allowed(user.name, cooldown=15.0):
        return None  # Silent ignore
    
    response = await process_llm_request(...)
    return f"@{user.name} {response}"
```

### Maintenant (pyTwitchAPI)

```python
# IntÃ©grÃ© dans MessageHandler (core/message_handler.py)
async def _handle_chat_message(self, msg: ChatMessage):
    # DÃ©tection avant routing commandes
    bot_name = self.config.get("bot_login_name", "serda_bot")
    mention_text = extract_mention_message(msg.text, bot_name)
    
    if mention_text:
        await self._handle_mention(msg, mention_text)
        return

async def _handle_mention(self, msg: ChatMessage, mention_text: str):
    # Check LLM + rate limiting
    # Call process_llm_request(context="mention")
    # Format & publish response
```

**Avantages :**
- âœ… IntÃ©gration native au MessageHandler
- âœ… Utilise le mÃªme MessageBus que les commandes
- âœ… Rate limiting intÃ©grÃ© (pas de dÃ©pendance externe)
- âœ… Silent ignore sur cooldown (UX propre)
- âœ… Context "mention" distinct de "ask"

## ğŸš€ Prochaines Ã‰tapes

Phase 3.2 est **complÃ¨te** :
- âœ… LLMHandler backend wrapper
- âœ… Commande !ask
- âœ… Mentions @bot_name
- âœ… Rate limiting mentions
- âœ… Tests complets

**Phase 3.3 - EventSub** :
- stream.online/offline events
- Broadcaster OAuth token setup
- Auto-announce dans le chat

## ğŸ“ Notes Techniques

### Pourquoi PrioritÃ© sur Commandes ?

Les mentions sont dÃ©tectÃ©es **avant** le check `!` pour gÃ©rer le cas `@serda_bot !ping` :
- Sans prioritÃ© : TraitÃ© comme `!ping` (mention ignorÃ©e)
- Avec prioritÃ© : TraitÃ© comme mention (comportement attendu)

### Context "mention" vs "ask"

Le `context` est utilisÃ© par NeuralPathwayManager pour adapter le prompt systÃ¨me :
- **mention:** Ton conversationnel, rÃ©ponses plus naturelles
- **ask:** Ton factuel, rÃ©ponses prÃ©cises et courtes

### Silent Ignore sur Cooldown

Contrairement aux commandes (!ping, !ask) qui affichent "â³ Cooldown actif", les mentions sont silencieusement ignorÃ©es pour Ã©viter le spam :
- Utilisateur spam @bot â†’ Pas de rÃ©ponse = indication naturelle
- Commande !ask â†’ Message d'erreur = feedback explicite

Cette distinction respecte l'UX attendue :
- Mentions = conversations naturelles
- Commandes = interactions explicites

## ğŸ‰ Conclusion

La feature de mention du bot est **opÃ©rationnelle** et suit la mÃªme architecture propre que les autres fonctionnalitÃ©s Phase 3 :
- Backend clean (LLMHandler)
- Rate limiting intÃ©grÃ©
- Context LLM adaptÃ©
- Tests complets validÃ©s

Le bot peut maintenant rÃ©pondre aux mentions naturellement tout en respectant les limites de rate limiting pour Ã©viter le spam ! ğŸ¤–ğŸ’¬
