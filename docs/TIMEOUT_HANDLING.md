# Gestion des Timeouts - Phase 2.6

## üéØ Objectif

Prot√©ger le bot contre les blocages caus√©s par:
- Requ√™tes Helix API lentes
- Envoi IRC bloqu√©
- **LLM inf√©rence longue (Phase 3)**
- Probl√®mes r√©seau

Sans gestion timeout, un seul appel bloqu√© peut freezer tout le bot. üßä

---

## ‚è±Ô∏è Configuration

**Fichier**: `config/config.yaml`

```yaml
# ‚è±Ô∏è Timeouts pour les transports (Phase 2.6)
timeouts:
  irc_send: 5.0       # Timeout envoi message IRC
  helix_request: 8.0  # Timeout requ√™te Helix API
  llm_inference: 30.0 # Timeout inf√©rence LLM (peut √™tre long)
```

### Valeurs recommand√©es

| Transport | Timeout | Justification |
|-----------|---------|---------------|
| **IRC Send** | 5s | Message chat = rapide. Si >5s ‚Üí probl√®me r√©seau |
| **Helix API** | 8s | API publique Twitch, devrait r√©pondre <5s normalement |
| **LLM Inference** | 30s | OpenAI peut √™tre lent (GPT-4), local LLM encore plus |

---

## üîß Impl√©mentation

### IRC Client

**Fichier**: `twitchapi/transports/irc_client.py`

```python
async def _handle_outbound_message(self, msg: OutboundMessage) -> None:
    """Phase 2.6: Envoie un message via IRC avec timeout"""
    try:
        # Phase 2.6: Envoyer avec timeout
        await asyncio.wait_for(
            self.chat.send_message(msg.channel, msg.text),
            timeout=self.irc_send_timeout
        )
        LOGGER.info(f"‚úÖ Sent to #{msg.channel}")
        
    except asyncio.TimeoutError:
        LOGGER.error(f"‚è±Ô∏è Timeout envoi IRC √† #{msg.channel} apr√®s {self.irc_send_timeout}s")
    except Exception as e:
        LOGGER.error(f"‚ùå Erreur envoi IRC: {e}")
```

**Comportement**:
- Si timeout ‚Üí Log erreur + **message suivant continue** (pas de blocage)
- User ne voit pas le message (timeout), mais bot reste op√©rationnel
- Alternative: Retry logic (√† impl√©menter en Phase 3 si besoin)

### Helix Client

**Fichier**: `twitchapi/transports/helix_readonly.py`

```python
async def get_stream(self, user_login: str) -> Optional[dict]:
    """R√©cup√®re stream info avec timeout"""
    try:
        # Phase 2.6: Wrap avec timeout
        async def _fetch():
            streams = []
            async for stream in self.twitch.get_streams(user_login=[user_login]):
                streams.append(stream)
            return streams
        
        streams = await asyncio.wait_for(_fetch(), timeout=self.helix_timeout)
        # ... process streams ...
        
    except asyncio.TimeoutError:
        LOGGER.error(f"‚è±Ô∏è Timeout get_stream({user_login}) apr√®s {self.helix_timeout}s")
        return None
    except Exception as e:
        LOGGER.error(f"Erreur get_stream: {e}")
        return None
```

**Comportement**:
- Si timeout ‚Üí Return `None` (comme si stream offline)
- Command handler re√ßoit `None` et peut r√©pondre "Erreur API, r√©essaie plus tard"

### Main.py

**Fichier**: `main.py`

```python
# Phase 2.6: Charger les timeouts depuis config
timeouts = config.get("timeouts", {})
irc_send_timeout = timeouts.get("irc_send", 5.0)
helix_timeout = timeouts.get("helix_request", 8.0)

# Phase 2.6: Helix Read-Only (avec timeout)
helix = HelixReadOnlyClient(twitch_app, bus, helix_timeout=helix_timeout)

# Phase 2.6: IRC Client (avec timeout)
irc_client = IRCClient(
    twitch=twitch_bot,
    bus=bus,
    bot_user_id=bot_user_id,
    bot_login=bot_token.user_login,
    channels=irc_channels,
    irc_send_timeout=irc_send_timeout
)
```

---

## üß™ Sc√©narios de Test

### Test 1: IRC Timeout (simulation)

```python
# Dans IRC Client, temporairement:
async def _handle_outbound_message(self, msg: OutboundMessage):
    await asyncio.sleep(10)  # Simuler blocage
    await self.chat.send_message(msg.channel, msg.text)
```

**R√©sultat attendu**:
```
üì§ Tentative envoi IRC √† #el_serda: pong
‚è±Ô∏è Timeout envoi IRC √† #el_serda apr√®s 5.0s: pong
```

### Test 2: Helix Timeout (r√©seau lent)

Si Twitch API lente:
```
[HELIX] get_stream(el_serda)
‚è±Ô∏è Timeout get_stream(el_serda) apr√®s 8.0s
```

User re√ßoit: "‚ùå Erreur API, r√©essaie plus tard"

### Test 3: LLM Timeout (Phase 3)

Quand LLM int√©gr√©:
```python
# Dans LLM handler:
response = await asyncio.wait_for(
    openai.chat.completions.create(...),
    timeout=llm_timeout
)
```

Si OpenAI prend >30s:
```
‚è±Ô∏è Timeout LLM inference apr√®s 30.0s
```

User re√ßoit: "üß† Mon cerveau lag, r√©essaie !"

---

## üö® Cas Critiques

### Cas 1: Timeout trop court

**Probl√®me**:
```yaml
timeouts:
  helix_request: 0.5  # Trop court !
```

**Sympt√¥me**:
- Timeout √† chaque requ√™te Helix
- Bot r√©pond toujours "Erreur API"

**Solution**: Augmenter timeout √† 8s minimum

### Cas 2: Timeout trop long

**Probl√®me**:
```yaml
timeouts:
  irc_send: 60.0  # Trop long !
```

**Sympt√¥me**:
- Si probl√®me r√©seau IRC ‚Üí Bot bloqu√© 60s
- Messages en queue s'accumulent

**Solution**: IRC devrait √™tre <10s max

### Cas 3: Pas de timeout

**Probl√®me**: Code sans `asyncio.wait_for()`

**Sympt√¥me**:
- Bot freeze compl√®tement
- Plus de r√©ponse √† aucune commande
- N√©cessite red√©marrage

**Impact**: **CRITIQUE** pour LLM Phase 3

---

## üìä M√©triques √† Surveiller

En production, tracker:

1. **Taux de timeout IRC**:
   - Si >5% ‚Üí Probl√®me r√©seau ou Twitch instable
   
2. **Taux de timeout Helix**:
   - Si >1% ‚Üí Twitch API lente ou probl√®me r√©seau
   
3. **Latence moyenne**:
   - IRC: <500ms normalement
   - Helix: <3s normalement
   - LLM: 5-15s (OpenAI), 1-5s (local)

4. **Timeout LLM** (Phase 3):
   - Si >10% ‚Üí Mod√®le trop lent ou prompt trop complexe

---

## üîÆ Phase 3: LLM Integration

Quand LLM sera branch√©, timeout handling devient **CRITIQUE**:

### Sc√©nario sans timeout

```python
# ‚ùå DANGER: Pas de timeout
response = await openai.chat.completions.create(...)
# Si OpenAI freeze ‚Üí Bot freeze
```

User tape `!ask quoi de neuf?` ‚Üí Bot **ne r√©pond jamais** ‚Üí User spam ‚Üí Queue explose

### Sc√©nario avec timeout

```python
# ‚úÖ SAFE: Avec timeout
try:
    response = await asyncio.wait_for(
        openai.chat.completions.create(...),
        timeout=llm_timeout
    )
except asyncio.TimeoutError:
    await bus.publish("chat.outbound", OutboundMessage(
        channel=msg.channel,
        text="üß† Mon cerveau lag, r√©essaie !"
    ))
```

User re√ßoit feedback + bot continue de fonctionner

---

## üéì Best Practices

1. **Toujours wrap les appels externes**:
   - API Twitch (Helix)
   - OpenAI / LLM
   - Bases de donn√©es
   - HTTP requests

2. **Timeouts adaptatifs**:
   - Court pour IRC (5s)
   - Moyen pour API (8s)
   - Long pour LLM (30s)

3. **Log timeout avec contexte**:
   ```python
   LOGGER.error(f"‚è±Ô∏è Timeout {operation} apr√®s {timeout}s: {context}")
   ```

4. **Fallback gracieux**:
   - IRC timeout ‚Üí Log + skip message
   - Helix timeout ‚Üí Return None
   - LLM timeout ‚Üí Message d'erreur friendly

5. **Monitoring production**:
   - Tracker taux de timeout
   - Alerter si >seuil
   - Ajuster timeouts si n√©cessaire

---

## üîó R√©f√©rences

- **asyncio.wait_for()**: https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for
- **Phase 2 Architecture**: `docs/PHASE2_ARCHITECTURE.md`
- **Config**: `config/config.yaml`

---

## ‚úÖ Checklist Phase 2.6

- [x] Config `timeouts` section ajout√©e
- [x] IRC Client timeout handling
- [x] Helix Client timeout handling
- [x] Main.py passe timeouts depuis config
- [x] Tests syntaxe OK
- [x] Documentation cr√©√©e
- [ ] **Test avec bot lanc√©** (prochain step)
- [ ] Test simulation timeout (optionnel)

---

**Phase 2.6 Status**: ‚úÖ CODE COMPLETE, pr√™t pour test live

**Next**: Tester bot avec timeouts + Pr√©parer Phase 3 (LLM + Game Lookup)
