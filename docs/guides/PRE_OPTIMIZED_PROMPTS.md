# ğŸ¯ Pre-Optimized Prompts System

Guide complet pour crÃ©er des commandes avec prompts prÃ©-optimisÃ©s dans KissBot.

---

## ğŸ“– Table des MatiÃ¨res

1. [Concept](#-concept)
2. [Architecture](#-architecture)
3. [Quick Start](#-quick-start)
4. [Guide Complet](#-guide-complet)
5. [Exemples](#-exemples)
6. [Validation DÃ©fensive](#-validation-dÃ©fensive)
7. [Multilingual Support](#-multilingual-support)
8. [Best Practices](#-best-practices)

---

## ğŸ§  Concept

### ProblÃ¨me : Double-Wrapping

Lorsque vous utilisez le pipeline normal, votre prompt est automatiquement enrichi :

```python
# âŒ ProblÃ¨me : Votre prompt optimisÃ© est re-wrappÃ©
prompt = "RÃ©ponds EN 1 PHRASE MAX : raconte une blague"

# Pipeline ajoute automatiquement :
# "RÃ©ponds EN 1 PHRASE MAX, SANS TE PRÃ‰SENTER, comme KissBot (bot sympa). 
#  Max 120 caractÃ¨res : RÃ©ponds EN 1 PHRASE MAX : raconte une blague"
#                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                                 Double instruction !
```

**RÃ©sultat :** Prompt pollu, LLM confus, rÃ©ponses incohÃ©rentes.

### Solution : Pre-Optimized Prompts

```python
# âœ… Solution : Bypass le wrapping automatique
response = await process_llm_request(
    llm_handler=self.llm_handler,
    prompt="RÃ©ponds EN 1 PHRASE MAX : raconte une blague",
    pre_optimized=True,  # â† Skip automatic wrapping
    stimulus_class="gen_short"
)
```

**RÃ©sultat :** Prompt envoyÃ© tel quel au LLM, contrÃ´le total.

---

## ğŸ—ï¸ Architecture

### Flux de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      COMMANDE                               â”‚
â”‚  !joke / !fact / !tip                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ prompt + pre_optimized=True/False
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              process_llm_request()                          â”‚
â”‚           (intelligence/core.py)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚
    pre_optimized?         â”‚
         â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚   YES    â”‚      â”‚    NO    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
         â”‚                 â”‚ + Smart Context wrapping
         â”‚                 â”‚ + Personality injection
         â”‚                 â–¼
         â”‚           process_stimulus()
         â”‚           (Neural Pathway)
         â”‚                 â”‚
         â”‚                 â”‚
         â–¼                 â–¼
    local_synapse.fire()   
    (context="direct")     
         â”‚                 
         â”‚ NO WRAPPING     
         â–¼                 
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
    â”‚   LM STUDIO    â”‚     
    â”‚ Mistral 7B     â”‚     
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     
```

### Composants

1. **Commands Layer** (`commands/intelligence_commands.py`)
   - DÃ©finit les commandes TwitchIO
   - Appelle `process_llm_request()` avec `pre_optimized=True`

2. **Core Layer** (`intelligence/core.py`)
   - GÃ¨re la logique de routing
   - Validation dÃ©fensive (fork-safe)
   - Routing conditionnel (pre-optimized vs normal)

3. **Synapse Layer** (`intelligence/synapses/local_synapse.py`)
   - `context="direct"` â†’ bypass `_optimize_signal_for_local()`
   - Injection langue automatique (`llm.language`)
   - Transmission directe au LLM

---

## ğŸš€ Quick Start

### 1. CrÃ©er la Commande

```python
# commands/intelligence_commands.py

@commands.command(name="joke")
async def joke_command(self, ctx: commands.Context):
    """ğŸ­ Bot raconte une blague courte"""
    
    # Initialiser LLM handler
    if not self._ensure_llm_handler(ctx.bot):
        await ctx.send(f"@{ctx.author.name} âŒ IA indisponible")
        return
    
    # Rate limiting (optionnel)
    if hasattr(ctx.bot, 'rate_limiter'):
        if not ctx.bot.rate_limiter.is_allowed(ctx.author.name, cooldown=10.0):
            remaining = ctx.bot.rate_limiter.get_remaining_cooldown(ctx.author.name, cooldown=10.0)
            await ctx.send(f"@{ctx.author.name} â±ï¸ Cooldown! Attends {remaining:.1f}s")
            return
    
    # Prompt prÃ©-optimisÃ© (validÃ© en POC)
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER, style humoristique : raconte une blague courte"
    
    # Appel avec pre_optimized=True
    response = await process_llm_request(
        llm_handler=self.llm_handler,
        prompt=prompt,
        context="ask",
        user_name=ctx.author.name,
        game_cache=None,
        pre_optimized=True,        # â† Pas de wrapping
        stimulus_class="gen_short"  # â† Force courte
    )
    
    # Envoyer rÃ©ponse
    if response:
        await ctx.send(f"@{ctx.author.name} {response}")
    else:
        await ctx.send(f"@{ctx.author.name} âŒ Erreur IA")
```

### 2. Tester en Local

```bash
# POC test dans tests-local/
python tests-local/test_joke_poc.py

# Valider : latence, langue, contenu
# Expected: <3s, franÃ§ais, cohÃ©rent
```

### 3. Tester en Production

```bash
# Lancer bot
python main.py

# Twitch chat
!joke

# VÃ©rifier logs
tail -f logs/kissbot.log | grep "ğŸ¯ Prompt prÃ©-optimisÃ©"
```

---

## ğŸ“š Guide Complet

### Ã‰tape 1 : Validation POC

**TOUJOURS** valider votre prompt en POC avant production :

```python
# tests-local/test_my_command_poc.py
import asyncio
from modules.intelligence.neural_pathway_manager import NeuralPathwayManager
import yaml

async def test_my_command():
    # Load config
    with open("config/config.yaml") as f:
        config = yaml.safe_load(f)
    
    # Init handler
    llm_handler = NeuralPathwayManager(config)
    
    # Test prompt (itÃ©rez jusqu'Ã  satisfaction)
    prompts = [
        "RÃ©ponds EN 1 PHRASE : partage un fait scientifique",
        "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS : fait scientifique intÃ©ressant",
        "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER : partage un fait scientifique fascinant"
    ]
    
    for prompt in prompts:
        print(f"\nğŸ” Test: {prompt[:50]}...")
        
        response = await llm_handler.local_synapse.fire(
            stimulus=prompt,
            context="direct",  # Bypass wrapping
            stimulus_class="gen_short",
            correlation_id="poc_test"
        )
        
        print(f"âœ… Response: {response}")
        print(f"   Length: {len(response)} chars")

asyncio.run(test_my_command())
```

**CritÃ¨res de validation :**
- âœ… Latence < 3s (Mistral 7B)
- âœ… Langue correcte (franÃ§ais si `llm.language: fr`)
- âœ… Format respectÃ© (1 phrase, pas d'auto-prÃ©sentation)
- âœ… Contenu cohÃ©rent (pas de hallucination)

### Ã‰tape 2 : IntÃ©gration Production

Une fois le prompt validÃ©, intÃ©grez-le dans `commands/intelligence_commands.py` :

```python
@commands.command(name="fact")
async def fact_command(self, ctx: commands.Context):
    """ğŸ”¬ Bot partage un fait scientifique"""
    
    # 1. Validation handler
    if not self._ensure_llm_handler(ctx.bot):
        await ctx.send(f"@{ctx.author.name} âŒ IA indisponible")
        return
    
    # 2. Rate limiting (10s cooldown recommandÃ©)
    if hasattr(ctx.bot, 'rate_limiter'):
        if not ctx.bot.rate_limiter.is_allowed(ctx.author.name, cooldown=10.0):
            remaining = ctx.bot.rate_limiter.get_remaining_cooldown(ctx.author.name, cooldown=10.0)
            await ctx.send(f"@{ctx.author.name} â±ï¸ Cooldown! Attends {remaining:.1f}s")
            return
    
    # 3. Prompt validÃ© en POC
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER : partage un fait scientifique fascinant"
    
    # 4. Appel pre-optimized
    response = await process_llm_request(
        llm_handler=self.llm_handler,
        prompt=prompt,
        context="ask",
        user_name=ctx.author.name,
        game_cache=None,
        pre_optimized=True,
        stimulus_class="gen_short"  # Ou "gen_long" si besoin
    )
    
    # 5. RÃ©ponse Twitch
    if response:
        await ctx.send(f"@{ctx.author.name} ğŸ”¬ {response}")
    else:
        await ctx.send(f"@{ctx.author.name} âŒ Erreur IA")
```

### Ã‰tape 3 : Mise Ã  Jour !help

```python
# commands/utils_commands.py

@commands.command(name="help")
async def help_command(self, ctx: commands.Context):
    """â“ Liste des commandes disponibles"""
    help_text = (
        "ğŸ¤– KissBot - Commandes: "
        "!ping !stats !help !game [nom] !gc [nom] !ask [question] !joke !fact"
        #                                                                  ^^^^^ Ajoutez ici
    )
    await ctx.send(help_text)
```

### Ã‰tape 4 : Tests Unitaires

```python
# tests-local/test_fact_command.py
import pytest
from unittest.mock import AsyncMock
from modules.intelligence.core import process_llm_request

@pytest.mark.asyncio
async def test_fact_pipeline_success():
    """Test pipeline !fact avec prompt prÃ©-optimisÃ©"""
    
    # Mock LLM handler
    llm_handler = AsyncMock()
    llm_handler.local_synapse = AsyncMock()
    llm_handler.local_synapse.fire = AsyncMock(
        return_value="Le cerveau humain contient environ 86 milliards de neurones."
    )
    
    # Execute pipeline
    response = await process_llm_request(
        llm_handler=llm_handler,
        prompt="RÃ©ponds EN 1 PHRASE MAX : partage un fait scientifique",
        context="ask",
        user_name="TestUser",
        game_cache=None,
        pre_optimized=True,
        stimulus_class="gen_short"
    )
    
    # Verify
    assert response is not None
    assert len(response) > 10
    llm_handler.local_synapse.fire.assert_called_once()
```

---

## ğŸ’¡ Exemples

### Example 1: !joke (ImplÃ©mentÃ©)

```python
@commands.command(name="joke")
async def joke_command(self, ctx: commands.Context):
    """ğŸ­ Blague courte"""
    
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER, style humoristique : raconte une blague courte"
    
    response = await process_llm_request(
        llm_handler=self.llm_handler,
        prompt=prompt,
        context="ask",
        user_name=ctx.author.name,
        game_cache=None,
        pre_optimized=True,
        stimulus_class="gen_short"
    )
    
    if response:
        await ctx.send(f"@{ctx.author.name} {response}")
```

**RÃ©sultat :**
```
User: !joke
Bot: @user Un singe et un cochon sont dans la jungle. le singe a un crayon sous la queue...
```

### Example 2: !fact (Ã€ implÃ©menter)

```python
@commands.command(name="fact")
async def fact_command(self, ctx: commands.Context):
    """ğŸ”¬ Fait scientifique"""
    
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER : partage un fait scientifique fascinant"
    
    response = await process_llm_request(
        llm_handler=self.llm_handler,
        prompt=prompt,
        context="ask",
        user_name=ctx.author.name,
        game_cache=None,
        pre_optimized=True,
        stimulus_class="gen_short"
    )
    
    if response:
        await ctx.send(f"@{ctx.author.name} ğŸ”¬ {response}")
```

### Example 3: !tip (Ã€ implÃ©menter)

```python
@commands.command(name="tip")
async def tip_command(self, ctx: commands.Context):
    """ğŸ’¡ Conseil productivitÃ©"""
    
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER : donne un conseil de productivitÃ©"
    
    response = await process_llm_request(
        llm_handler=self.llm_handler,
        prompt=prompt,
        context="ask",
        user_name=ctx.author.name,
        game_cache=None,
        pre_optimized=True,
        stimulus_class="gen_short"
    )
    
    if response:
        await ctx.send(f"@{ctx.author.name} ğŸ’¡ {response}")
```

### Example 4: !quote (Ã€ implÃ©menter)

```python
@commands.command(name="quote")
async def quote_command(self, ctx: commands.Context):
    """ğŸ“œ Citation inspirante"""
    
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER : partage une citation inspirante sur la technologie"
    
    response = await process_llm_request(
        llm_handler=self.llm_handler,
        prompt=prompt,
        context="ask",
        user_name=ctx.author.name,
        game_cache=None,
        pre_optimized=True,
        stimulus_class="gen_short"
    )
    
    if response:
        await ctx.send(f"@{ctx.author.name} ğŸ“œ {response}")
```

---

## ğŸ›¡ï¸ Validation DÃ©fensive

Le systÃ¨me inclut une validation dÃ©fensive pour garantir la fork-safety (ports C++/Rust) :

### Code (`intelligence/core.py`)

```python
async def process_llm_request(
    llm_handler, 
    prompt: str, 
    context: str, 
    user_name: str, 
    game_cache=None,
    pre_optimized: bool = False,
    stimulus_class: str = "gen_short"
) -> str | None:
    
    import logging
    logger = logging.getLogger(__name__)
    
    # âœ… VALIDATION DÃ‰FENSIVE (fork-safe, language-agnostic)
    
    # 1. Null check on llm_handler (prevent AttributeError/segfault)
    if not llm_handler:
        logger.error("âŒ llm_handler est None")
        return None
    
    # 2. Type conversion for pre_optimized (handle None/string/int â†’ bool)
    if not isinstance(pre_optimized, bool):
        logger.warning(f"âš ï¸ pre_optimized type invalide ({type(pre_optimized)}), conversion bool")
        pre_optimized = bool(pre_optimized)
    
    # 3. Whitelist validation for stimulus_class (prevent invalid values)
    valid_classes = ["ping", "gen_short", "gen_long"]
    if stimulus_class not in valid_classes:
        logger.warning(f"âš ï¸ stimulus_class invalide '{stimulus_class}', fallback 'gen_short'")
        stimulus_class = "gen_short"
    
    # ... reste du code
```

### Pourquoi c'est Important ?

**Python (dynamique) :**
- `llm_handler = None` â†’ `AttributeError: 'NoneType' has no attribute 'local_synapse'` (rÃ©cupÃ©rable)
- `pre_optimized = "true"` â†’ InterprÃ©tÃ© comme `True` (truthy)

**C++ (compilÃ©) :**
- `llm_handler = nullptr` â†’ **SEGFAULT** (crash fatal)
- `pre_optimized = "true"` â†’ **ERREUR DE COMPILATION**

**Rust (compilÃ©, sÃ»r) :**
- `llm_handler = None` â†’ **PANIC** (crash contrÃ´lÃ©)
- `pre_optimized = "true"` â†’ **ERREUR DE COMPILATION**

La validation dÃ©fensive garantit que :
1. Les contrats sont clairs (types, valeurs attendues)
2. Les erreurs sont gÃ©rÃ©es gracieusement
3. Les futurs ports C++/Rust ne crashent pas
4. Le code est documentÃ© pour les contributeurs

---

## ğŸŒ Multilingual Support

### Configuration

```yaml
# config/config.yaml
llm:
  language: fr  # Supported: fr, en, es, de
```

### Injection Automatique

Le systÃ¨me injecte automatiquement la directive de langue dans tous les prompts :

```python
# Votre prompt
prompt = "RÃ©ponds EN 1 PHRASE MAX : raconte une blague"

# SystÃ¨me injecte (si language: fr)
# "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS : raconte une blague"
```

**Mapping langue :**
- `fr` â†’ "EN FRANÃ‡AIS"
- `en` â†’ "IN ENGLISH"
- `es` â†’ "EN ESPAÃ‘OL"
- `de` â†’ "AUF DEUTSCH"

### Multi-langue dans Prompts

Si vous voulez forcer une langue spÃ©cifique (ignorer config) :

```python
# Force English (ignore config language)
prompt = "Answer IN 1 SENTENCE MAX IN ENGLISH, NO SELF-INTRODUCTION: tell a short joke"

response = await process_llm_request(
    llm_handler=self.llm_handler,
    prompt=prompt,
    context="ask",
    user_name=ctx.author.name,
    game_cache=None,
    pre_optimized=True,
    stimulus_class="gen_short"
)
```

**Note :** Le systÃ¨me n'injecte PAS de directive langue si `pre_optimized=True` et `context="direct"`, car il bypass `_optimize_signal_for_local()`.

---

## ğŸ¯ Best Practices

### 1. Toujours Valider en POC

```bash
# âŒ MAL : Commiter prompt non testÃ©
git commit -m "Add !fact command"

# âœ… BON : Valider POC d'abord
python tests-local/test_fact_poc.py
# â†’ ItÃ©rer prompts jusqu'Ã  satisfaction
git commit -m "Add !fact command (validated latency 2.1s, French)"
```

### 2. Pattern de Prompt RecommandÃ©

```python
# âœ… Pattern validÃ© Mistral AI
"RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER, style {style} : {instruction}"

# Exemples :
# style=humoristique : raconte une blague courte
# style=scientifique : partage un fait fascinant
# style=inspirant : donne un conseil de productivitÃ©
# style=neutre : explique un concept technique
```

**Pourquoi ce pattern ?**
- âœ… **"EN 1 PHRASE MAX"** â†’ Force concision (limite tokens)
- âœ… **"EN FRANÃ‡AIS"** â†’ Force langue (ou injectÃ© par config)
- âœ… **"SANS TE PRÃ‰SENTER"** â†’ Ã‰vite "Bonjour ! Je suis KissBot..." (hallucination)
- âœ… **"style {style}"** â†’ Guide ton/personnalitÃ©
- âœ… **": {instruction}"** â†’ Instruction claire

### 3. Choisir le Bon Stimulus Class

| Class | Timeout | Max Tokens | Usage |
|-------|---------|------------|-------|
| `"ping"` | 2s | 20 | RÃ©ponses ultra-courtes ("ok", "oui", "non") |
| `"gen_short"` | 4s | 100 | RÃ©ponses courtes (1 phrase, <120 chars) |
| `"gen_long"` | 8s | 150 | RÃ©ponses longues (2-3 phrases, <400 chars) |

```python
# âœ… BON : !joke = gen_short (1 phrase courte)
stimulus_class="gen_short"

# âŒ MAL : !joke = gen_long (trop de tokens allouÃ©s)
stimulus_class="gen_long"  # Gaspillage, LLM peut divaguer
```

### 4. Rate Limiting SystÃ©matique

```python
# âœ… TOUJOURS ajouter rate limiting
if hasattr(ctx.bot, 'rate_limiter'):
    if not ctx.bot.rate_limiter.is_allowed(ctx.author.name, cooldown=10.0):
        remaining = ctx.bot.rate_limiter.get_remaining_cooldown(ctx.author.name, cooldown=10.0)
        await ctx.send(f"@{ctx.author.name} â±ï¸ Cooldown! Attends {remaining:.1f}s")
        return
```

**Cooldowns recommandÃ©s :**
- `!joke`, `!fact`, `!tip` : **10s** (commandes lÃ©gÃ¨res)
- `!ask` : **10s** (commandes LLM standard)
- `@bot mentions` : **15s** (commandes conversationnelles)

### 5. Error Handling Gracieux

```python
# âœ… BON : GÃ©rer les erreurs explicitement
response = await process_llm_request(...)

if response:
    await ctx.send(f"@{ctx.author.name} {response}")
else:
    await ctx.send(f"@{ctx.author.name} âŒ Erreur IA, rÃ©essaye plus tard")

# âŒ MAL : Pas de gestion d'erreur
await ctx.send(f"@{ctx.author.name} {response}")  # Si None â†’ "@user None"
```

### 6. Documentation Inline

```python
# âœ… BON : Documenter le prompt validÃ©
@commands.command(name="joke")
async def joke_command(self, ctx: commands.Context):
    """
    ğŸ­ Commande !joke - Bot raconte une blague courte.
    Pattern optimisÃ© Mistral AI validÃ© (POC: 2.34s, franÃ§ais, 100% succÃ¨s).
    """
    
    # Prompt POC validÃ© : pattern Mistral AI (0.54s, ~19 tokens)
    # pre_optimized=True â†’ bypass wrapping automatique
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER, style humoristique : raconte une blague courte"
    
    response = await process_llm_request(
        llm_handler=self.llm_handler,
        prompt=prompt,
        context="ask",
        user_name=ctx.author.name,
        game_cache=None,
        pre_optimized=True,       # â† Prompt dÃ©jÃ  au format optimal
        stimulus_class="gen_short" # â† Force classification courte
    )
```

### 7. Tests Unitaires

```python
# âœ… TOUJOURS crÃ©er tests unitaires
# tests-local/test_joke_command.py

@pytest.mark.asyncio
async def test_joke_pipeline_success():
    """Test pipeline !joke: process_llm_request avec prompt prÃ©-optimisÃ©"""
    
    llm_handler = AsyncMock()
    llm_handler.local_synapse.fire = AsyncMock(
        return_value="Une blague courte !"
    )
    
    response = await process_llm_request(
        llm_handler=llm_handler,
        prompt="RÃ©ponds EN 1 PHRASE MAX : raconte une blague courte",
        context="ask",
        user_name="TestUser",
        game_cache=None,
        pre_optimized=True,
        stimulus_class="gen_short"
    )
    
    assert response is not None
    assert len(response) > 0
```

---

## ğŸ” Debugging

### Logs de Debug

```bash
# Activer logs dÃ©taillÃ©s
tail -f logs/kissbot.log | grep "ğŸ¯"

# Output attendu :
# 2025-10-30 03:22:15,660 INFO intelligence.core ğŸ¯ Prompt prÃ©-optimisÃ© dÃ©tectÃ© â†’ Appel direct synapse
# 2025-10-30 03:22:19,391 INFO intelligence.synapses.local_synapse ğŸ’¡âœ… [preopt_ask] Success 3.73s - Reward: 1.00
```

### Checklist Debug

- [ ] LLM handler initialisÃ© ? (`_ensure_llm_handler()` return `True`)
- [ ] `pre_optimized=True` dans `process_llm_request()` ?
- [ ] Log "ğŸ¯ Prompt prÃ©-optimisÃ© dÃ©tectÃ©" prÃ©sent ?
- [ ] `stimulus_class` valide ? (`"ping"`, `"gen_short"`, `"gen_long"`)
- [ ] RÃ©ponse LLM non-None ?
- [ ] Langue correcte ? (vÃ©rifier `llm.language` dans config)
- [ ] Latence < 5s ? (Mistral 7B baseline : 2-4s)

### ProblÃ¨mes Communs

**1. RÃ©ponse en anglais alors que `language: fr`**

```yaml
# âŒ ProblÃ¨me : Langue ignorÃ©e
llm:
  language: fr  # Config correcte

# Mais prompt force langue :
prompt = "Answer IN ENGLISH: tell a joke"  # â† Override config
```

**Solution :** Retirer la directive langue du prompt, laisser config injecter.

**2. Double-wrapping malgrÃ© `pre_optimized=True`**

```python
# âŒ ProblÃ¨me : Pas de bypass
response = await process_llm_request(
    prompt=prompt,
    pre_optimized=True,
    context="ask"  # â† ProblÃ¨me : context="ask" active wrapping
)
```

**Solution :** Utiliser `context="direct"` dans synapse ou s'assurer que `pre_optimized=True` active le bypass.

**3. Timeout constant**

```python
# âŒ ProblÃ¨me : Stimulus class trop ambitieux
stimulus_class="gen_long"  # 8s timeout
# Mais LLM gÃ©nÃ¨re 200 tokens â†’ 12s â†’ Timeout

# âœ… Solution : RÃ©duire ambition
stimulus_class="gen_short"  # 4s timeout
# Prompt force "1 PHRASE MAX" â†’ LLM gÃ©nÃ¨re 50 tokens â†’ 2s â†’ Success
```

---

## ğŸ“– Ressources

- **Code source :** `intelligence/core.py` (ligne 44-130)
- **Exemple implÃ©mentÃ© :** `commands/intelligence_commands.py` (`joke_command`)
- **Tests unitaires :** `tests-local/test_joke_command.py`
- **POC validation :** `braindev/test_joke_command.py`

---

## ğŸ“ Conclusion

Le systÃ¨me **pre-optimized prompts** vous donne :
- âœ… **ContrÃ´le total** sur les prompts LLM
- âœ… **Performance optimale** (pas de wrapping inutile)
- âœ… **Validation POC** avant production
- âœ… **Fork-safety** pour ports C++/Rust
- âœ… **Support multilingue** automatique

**Workflow recommandÃ© :**
1. **POC** : Valider prompt en local (`tests-local/`)
2. **IntÃ©gration** : CrÃ©er commande dans `commands/intelligence_commands.py`
3. **Tests** : Ajouter tests unitaires (`tests-local/test_*.py`)
4. **Production** : Tester en live, monitorer logs
5. **Documentation** : Mettre Ã  jour `!help` et README.md

**Next steps :**
- ğŸ­ ImplÃ©menter `!fact`, `!tip`, `!quote`
- ğŸ“Š CrÃ©er registre de commandes (dict mapping)
- ğŸŒ Support multi-langue avancÃ© (dÃ©tection auto)
- ğŸ”§ Optimisation prompts per-modÃ¨le (Mistral vs LLaMA vs Qwen)

---

**Questions ?** Ouvrir une issue sur GitHub ou rejoindre le stream ! ğŸš€
