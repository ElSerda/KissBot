#!/usr/bin/env python3
"""
ðŸ§® Entropy Calculator - Module de Calcul d'Entropie Shannon
Mesure l'incertitude d'une classification pour fallback intelligent
"""

import logging
import math
from typing import Dict, Any


class EntropyCalculator:
    """
    ðŸ§® ENTROPY CALCULATOR - Calcul d'Entropie Shannon

    ThÃ©orie : H(S) = -âˆ‘P(si)logâ‚‚P(si)
    - Entropie faible (< 0.5) â†’ Haute confiance
    - Entropie Ã©levÃ©e (> 1.5) â†’ Incertitude â†’ Fallback
    - Entropie maximale = logâ‚‚(n) pour n classes Ã©quiprobables
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # ðŸ“Š SEUILS D'ENTROPIE (basÃ©s sur thÃ©orie Shannon)
        self.entropy_thresholds = {
            "high_confidence": 0.5,    # < 0.5 â†’ TrÃ¨s confiant
            "moderate_confidence": 1.0, # 0.5-1.0 â†’ Confiance modÃ©rÃ©e
            "low_confidence": 1.9,     # > 1.9 â†’ Incertitude â†’ Fallback (augmentÃ© pour plus de libertÃ©)
            "max_entropy": 2.0         # logâ‚‚(4) pour 4 classes Ã©quiprobables
        }

        # ðŸŽ¯ CLASSES QUANTUMBOT
        self.known_classes = ["ping", "gen_short", "lookup", "gen_long"]

        self.logger.debug("ðŸ§® EntropyCalculator initialisÃ© - Seuils Shannon configurÃ©s")

    def calculate_shannon_entropy(self, probabilities: Dict[str, float]) -> float:
        """
        ðŸŽ¯ Calcul de l'Entropie Shannon H(S) = -âˆ‘P(si)logâ‚‚P(si)

        Args:
            probabilities: Dict avec probabilitÃ©s par classe
                          Ex: {"ping": 0.1, "gen_short": 0.6, "lookup": 0.2, "gen_long": 0.1}

        Returns:
            float: Entropie Shannon (0 = certitude absolue, logâ‚‚(n) = incertitude maximale)
        """
        if not probabilities:
            self.logger.warning("âš ï¸ ProbabilitÃ©s vides - retour entropie maximale")
            return self.entropy_thresholds["max_entropy"]

        # ðŸ” Validation et normalisation
        total_prob = sum(probabilities.values())
        if abs(total_prob - 1.0) > 1e-6:
            self.logger.debug(f"ðŸ“Š Normalisation probabilitÃ©s: {total_prob:.3f} â†’ 1.0")
            probabilities = {k: v / total_prob for k, v in probabilities.items()}

        # ðŸ§® Calcul Shannon: H(S) = -âˆ‘P(si)logâ‚‚P(si)
        entropy = 0.0
        for class_name, prob in probabilities.items():
            if prob > 0:  # Ã‰viter log(0)
                entropy -= prob * math.log2(prob)

        self.logger.debug(f"ðŸ§® Entropie calculÃ©e: {entropy:.3f} (seuils: {self.entropy_thresholds})")
        return entropy

    def evaluate_confidence(self, entropy: float) -> Dict[str, Any]:
        """
        ðŸ“Š Ã‰valuation de la confiance basÃ©e sur l'entropie

        Args:
            entropy: Valeur d'entropie Shannon

        Returns:
            Dict avec Ã©valuation complÃ¨te de confiance
        """
        confidence_level = "unknown"
        should_fallback = False
        confidence_score = 0.0

        if entropy < self.entropy_thresholds["high_confidence"]:
            confidence_level = "high"
            confidence_score = 1.0 - (entropy / self.entropy_thresholds["high_confidence"])
            should_fallback = False
        elif entropy < self.entropy_thresholds["moderate_confidence"]:
            confidence_level = "moderate"
            confidence_score = 1.0 - (entropy / self.entropy_thresholds["moderate_confidence"])
            should_fallback = False
        elif entropy < self.entropy_thresholds["low_confidence"]:
            confidence_level = "low"
            confidence_score = 1.0 - (entropy / self.entropy_thresholds["low_confidence"])
            should_fallback = False
        else:
            confidence_level = "very_low"
            confidence_score = 0.0
            should_fallback = True

        return {
            "entropy": entropy,
            "confidence_level": confidence_level,
            "confidence_score": confidence_score,  # 0.0-1.0
            "should_fallback": should_fallback,
            "is_certain": confidence_level in ["high", "moderate"],
            "threshold_used": self._get_threshold_name(entropy)
        }

    def _get_threshold_name(self, entropy: float) -> str:
        """ðŸŽ¯ DÃ©termine quel seuil a Ã©tÃ© franchi"""
        if entropy < self.entropy_thresholds["high_confidence"]:
            return "high_confidence"
        elif entropy < self.entropy_thresholds["moderate_confidence"]:
            return "moderate_confidence"
        elif entropy < self.entropy_thresholds["low_confidence"]:
            return "low_confidence"
        else:
            return "fallback_required"

    def analyze_distribution(self, probabilities: Dict[str, float]) -> Dict[str, Any]:
        """
        ðŸ” Analyse complÃ¨te d'une distribution de probabilitÃ©s

        Args:
            probabilities: Distribution de probabilitÃ©s par classe

        Returns:
            Analyse complÃ¨te : entropie, confiance, statistiques
        """
        entropy = self.calculate_shannon_entropy(probabilities)
        confidence_eval = self.evaluate_confidence(entropy)

        # ðŸ“Š Statistiques additionnelles
        max_prob = max(probabilities.values()) if probabilities else 0.0
        max_class = max(probabilities.items(), key=lambda x: x[1])[0] if probabilities else None

        # ðŸŽ¯ Ratio de dominance (classe max vs autres)
        other_probs = [v for k, v in probabilities.items() if k != max_class]
        second_max = max(other_probs) if other_probs else 0.0
        dominance_ratio = max_prob / (second_max + 1e-6)  # Ã‰viter division par 0

        return {
            **confidence_eval,
            "probabilities": probabilities,
            "max_probability": max_prob,
            "predicted_class": max_class,
            "dominance_ratio": dominance_ratio,
            "num_classes": len(probabilities),
            "distribution_type": self._classify_distribution_type(probabilities, entropy)
        }

    def _classify_distribution_type(self, probabilities: Dict[str, float], entropy: float) -> str:
        """ðŸ” Classifie le type de distribution"""
        if not probabilities:
            return "empty"

        max_prob = max(probabilities.values())

        if max_prob > 0.8:
            return "concentrated"  # Une classe domine fortement
        elif entropy > 1.8:
            return "uniform"       # Distribution quasi-uniforme
        elif len([p for p in probabilities.values() if p > 0.1]) > 2:
            return "multimodal"    # Plusieurs classes significatives
        else:
            return "bimodal"       # Deux classes principales

    def get_fallback_recommendation(self, probabilities: Dict[str, float]) -> str:
        """
        ðŸŽ¯ Recommandation de fallback en cas d'incertitude Ã©levÃ©e

        Args:
            probabilities: Distribution de probabilitÃ©s

        Returns:
            Classe recommandÃ©e pour fallback
        """
        analysis = self.analyze_distribution(probabilities)

        if analysis["should_fallback"]:
            # ðŸŽ¯ StratÃ©gie de fallback : gen_short est le plus sÃ»r
            self.logger.info(f"ðŸ”„ Fallback recommandÃ©: entropy={analysis['entropy']:.3f} > seuil={self.entropy_thresholds['low_confidence']}")
            return "gen_short"  # RÃ©ponse sÃ»re et polyvalente

        return analysis["predicted_class"]

    def get_entropy_stats(self) -> Dict[str, float]:
        """ðŸ“Š Statistiques et seuils d'entropie configurÃ©s"""
        return {
            "thresholds": self.entropy_thresholds,
            "max_possible_entropy": math.log2(len(self.known_classes)),
            "known_classes_count": len(self.known_classes),
            "theoretical_max": 2.0  # logâ‚‚(4) pour 4 classes
        }


def demo_entropy_calculator():
    """ðŸ§ª DÃ©monstration du EntropyCalculator"""
    print("ðŸ§® DEMONSTRATION ENTROPY CALCULATOR")
    print("=" * 45)

    calculator = EntropyCalculator()

    # ðŸ“Š Cas de test
    test_cases = [
        # Haute confiance
        {"ping": 0.9, "gen_short": 0.05, "lookup": 0.03, "gen_long": 0.02},

        # Confiance modÃ©rÃ©e
        {"ping": 0.1, "gen_short": 0.7, "lookup": 0.1, "gen_long": 0.1},

        # Incertitude Ã©levÃ©e (uniforme)
        {"ping": 0.25, "gen_short": 0.25, "lookup": 0.25, "gen_long": 0.25},

        # Bimodale
        {"ping": 0.45, "gen_short": 0.45, "lookup": 0.05, "gen_long": 0.05}
    ]

    for i, probs in enumerate(test_cases, 1):
        print(f"\nðŸ§ª Test {i}: {probs}")

        analysis = calculator.analyze_distribution(probs)

        print(f"   â†’ Entropie: {analysis['entropy']:.3f}")
        print(f"   â†’ Confiance: {analysis['confidence_level']} ({analysis['confidence_score']:.2f})")
        print(f"   â†’ Fallback: {'OUI' if analysis['should_fallback'] else 'NON'}")
        print(f"   â†’ Classe prÃ©dite: {analysis['predicted_class']}")
        print(f"   â†’ Type distribution: {analysis['distribution_type']}")


if __name__ == "__main__":
    demo_entropy_calculator()
