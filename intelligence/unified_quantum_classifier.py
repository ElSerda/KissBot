#!/usr/bin/env python3
"""
ðŸŒŒ Unified Quantum Classifier - Classification par Intention + Entropie Shannon
Fusion ImprovedClassifier + StaticQuantumClassifier en un seul fichier optimisÃ©
Version: 3.1 (Fusion Safe - Phase 1)
"""

import logging
from typing import Dict, List, Tuple, Any, Optional


class UnifiedQuantumClassifier:
    """
    ðŸŒŒ UNIFIED QUANTUM CLASSIFIER V3.1

    Paradigme Physique/MathÃ©matique :
    - Classification par INTENTION (ping/gen_short/lookup/gen_long)
    - Superposition : Messages existent dans toutes les classes jusqu'Ã  "mesure"
    - Entropie Shannon : Mesure l'incertitude pour fallback intelligent
    - Effondrement : Distribution probabiliste â†’ Classe dÃ©terministe

    Architecture Unified :
    - Base patterns + logique complexitÃ© (ex-ImprovedClassifier)
    - EntropyCalculator + confiance quantique (ex-StaticQuantumClassifier)
    - Cache 29x speedup sur messages rÃ©pÃ©tÃ©s
    - Interface uniforme : classify() â†’ {class, confidence, entropy, ...}

    AmÃ©liorations vs classification par longueur brute :
    - Analyse contextuelle des mentions
    - Classification par intention (pas longueur)
    - Fallback intelligent par complexitÃ© linguistique
    - Distribution probabiliste avec entropie Shannon
    """

    def __init__(self, config: Optional[Dict] = None, patterns_config_path: Optional[str] = None):
        """
        ðŸ—ï¸ Initialisation UnifiedQuantumClassifier avec Enhanced Patterns

        Args:
            config: Configuration optionnelle pour seuils et paramÃ¨tres
            patterns_config_path: Chemin vers fichier patterns YAML
        """
        self.logger = logging.getLogger(__name__)

        # ðŸŽ¯ RÃˆGLES SIMPLIFIÃ‰ES - VERSION SOFT (Reflex minimal + GPT fallback)
        self.classification_rules = {
            "ping": {
                "patterns": [
                    # Salutations uniquement
                    "salut", "coucou", "bonjour", "bonsoir", "hello", "hey",
                    # Tests de prÃ©sence
                    "ping", "test", "alive", "ici", "lÃ ",
                    # Remerciements
                    "merci", "thx", "ty", "thanks", "thank you"
                ],
                "description": "Messages triviaux â†’ Reflex instantanÃ© (0ms)",
                "target_response": "RÃ©ponse rÃ©flexe prÃ©dÃ©finie",
                "priority": "social"
            },
            
            "gen_short": {
                "patterns": [
                    # Fallback pour TOUTES les autres mentions (questions, calculs, logique, etc.)
                    # Pattern vide = catch-all si pas ping et pas !ask
                ],
                "description": "Toutes mentions non-triviales â†’ GPT concis",
                "target_response": "RÃ©ponse concise et crÃ©ative (1-3 phrases)",
                "priority": "question_simple"
            },

            "gen_long": {
                "patterns": ["!ask"],
                "description": "Commande !ask â†’ GPT dÃ©taillÃ©",
                "target_response": "RÃ©ponse dÃ©taillÃ©e et nuancÃ©e (5+ phrases)",
                "priority": "complex_analysis"
            }
        }

        # ðŸŽ¯ Enhanced Patterns Loader (override si config YAML fourni)
        if patterns_config_path:
            from .enhanced_patterns_loader import EnhancedPatternsLoader
            self.patterns_loader = EnhancedPatternsLoader(patterns_config_path)
            self.classification_rules = self.patterns_loader.get_classification_rules()

        # ðŸ” MOTS INDICATEURS DE COMPLEXITÃ‰
        self.complex_indicators = [
            "pourquoi", "comment", "analyse", "explique", "dÃ©veloppe", "dÃ©taille", "dÃ©tails", "dÃ©tail",
            "thÃ©orie", "principe", "fonctionnement", "mÃ©canisme", "architecture", "fonctionne",
            "explication", "mÃ©thode", "mÃ©thodes", "stratÃ©gie", "stratÃ©gies", "technique", "techniques",
            "procÃ©dure", "procÃ©dures", "algorithme", "algorithmes", "approche", "approches",
            "mÃ©thodologie", "mÃ©thodologies", "concept", "concepts", "notion", "notions"
        ]

        # ðŸ§® Calculateur d'entropie Shannon
        from .entropy_calculator import EntropyCalculator
        self.entropy_calculator = EntropyCalculator()

        # âš™ï¸ Configuration quantique
        self.config = config or {}
        self.quantum_config = self.config.get("quantum_classifier", {})

        # ðŸ“Š Seuils configurables (avec defaults intelligents)
        self.confidence_thresholds = {
            "high_confidence": self.quantum_config.get("high_confidence_threshold", 0.7),
            "entropy_fallback": self.quantum_config.get("entropy_fallback_threshold", 1.5),
            "minimum_probability": self.quantum_config.get("minimum_probability", 0.1)
        }

        # ðŸŽ¯ Fallback strategy
        self.fallback_class = self.quantum_config.get("fallback_class", "gen_short")

        # ðŸš€ Cache pour messages rÃ©pÃ©tÃ©s (pog, !discord, etc.) - 29x speedup
        self._classify_cache: Dict[Tuple[str, str], Dict[str, Any]] = {}
        self._cache_maxsize = 256

        self.logger.info("ðŸŒŒ UnifiedQuantumClassifier V3.1 initialized (Fusion Safe)")

    def classify(self, stimulus: str, context: str = "") -> Dict[str, Any]:
        """
        ðŸŽ¯ CLASSIFICATION QUANTIQUE COMPLÃˆTE (Main API)

        Processus :
        1. Superposition â†’ Calcul probabilitÃ©s toutes classes
        2. Entropie â†’ Mesure incertitude distribution
        3. Ã‰valuation â†’ Confiance + besoin fallback
        4. Effondrement â†’ Classe finale dÃ©terministe

        Args:
            stimulus: Message utilisateur
            context: Contexte optionnel

        Returns:
            Dict avec classification complÃ¨te :
            {
                "class": "gen_long",
                "confidence": 0.85,
                "entropy": 0.64,
                "is_certain": True,
                "should_fallback": False,
                "probabilities": {"ping": 0.1, "gen_short": 0.2, ...},
                "distribution_type": "concentrated",
                "method": "quantum_classification"
            }
        """
        # ðŸš€ Check cache (0ms pour messages rÃ©pÃ©tÃ©s)
        cache_key = (stimulus, context)
        if cache_key in self._classify_cache:
            return self._classify_cache[cache_key]

        # ðŸŒŒ Phase 1: SUPERPOSITION - Calcul probabilitÃ©s
        probabilities, classification_metadata = self.classify_with_probabilities(stimulus, context)

        # ðŸ§® Phase 2: ENTROPIE - Analyse incertitude
        entropy_analysis = self.entropy_calculator.analyze_distribution(probabilities)

        # ðŸŽ¯ Phase 3: Ã‰VALUATION - Confiance et fallback
        confidence_eval = self._evaluate_quantum_confidence(probabilities, entropy_analysis)

        # âš¡ Phase 4: EFFONDREMENT - Classe finale
        final_class = self._quantum_collapse(probabilities, entropy_analysis, confidence_eval)

        # ðŸ“Š Construction rÃ©sultat quantique complet
        quantum_result = {
            # ðŸŽ¯ RÃ©sultat principal
            "class": final_class,
            "confidence": confidence_eval["confidence_score"],
            "entropy": entropy_analysis["entropy"],

            # ðŸŒŒ Ã‰tat quantique
            "is_certain": confidence_eval["is_certain"],
            "should_fallback": entropy_analysis["should_fallback"],
            "probabilities": probabilities,

            # ðŸ“Š Analyses dÃ©taillÃ©es
            "distribution_type": entropy_analysis["distribution_type"],
            "dominance_ratio": entropy_analysis["dominance_ratio"],
            "confidence_level": entropy_analysis["confidence_level"],

            # ðŸ” MÃ©tadonnÃ©es techniques
            "method": "quantum_classification",
            "classification_reason": classification_metadata["classification_reason"],
            "metadata": classification_metadata,
            "entropy_analysis": entropy_analysis,
            "quantum_confidence": confidence_eval
        }

        self.logger.debug(f"ðŸŒŒ Quantum: '{stimulus}' â†’ {final_class} (entropy: {entropy_analysis['entropy']:.3f}, conf: {confidence_eval['confidence_score']:.3f})")

        # ðŸš€ Stocker en cache (max 256 messages, FIFO)
        if len(self._classify_cache) >= self._cache_maxsize:
            self._classify_cache.pop(next(iter(self._classify_cache)))
        self._classify_cache[cache_key] = quantum_result

        return quantum_result

    def classify_with_probabilities(self, stimulus: str, context: str = "") -> Tuple[Dict[str, float], Dict]:
        """
        ðŸŒŒ CLASSIFICATION QUANTIQUE - Retourne probabilitÃ©s pour chaque classe

        Cette mÃ©thode calcule des probabilitÃ©s pour chaque classe au lieu d'une classification binaire.
        Permet la superposition quantique avant effondrement vers une classe spÃ©cifique.

        Args:
            stimulus: Message utilisateur
            context: Contexte optionnel

        Returns:
            Tuple[Dict[str, float], Dict]: (probabilitÃ©s_par_classe, mÃ©tadonnÃ©es)

        Exemple:
            probabilities, metadata = classifier.classify_with_probabilities("explique moi Python")
            # probabilities = {"ping": 0.1, "gen_short": 0.2, "lookup": 0.3, "gen_long": 0.4}
        """
        stimulus_lower = stimulus.lower().strip()

        # ðŸ“Š Initialisation des scores par classe (3 classes)
        class_scores = {
            "ping": 0.0,
            "gen_short": 0.0,
            "gen_long": 0.0
        }

        # ðŸ” MÃ©tadonnÃ©es dÃ©taillÃ©es
        metadata = {
            "word_count": len(stimulus.split()),
            "has_question": "?" in stimulus,
            "has_mention": any(mention in stimulus_lower for mention in ["@", "serda_bot"]),
            "complex_words": self._detect_complex_words(stimulus_lower),
            "classification_method": "probabilistic"
        }

        # 1. ðŸŽ¯ DÃ‰TECTION !ASK (prioritÃ© absolue)
        if "!ask" in stimulus_lower or context == "ask":
            class_scores["gen_long"] = 1.0
            metadata["classification_reason"] = "explicit_ask_command"
            return class_scores, metadata

        # 2. ðŸŽ¤ DÃ‰TECTION REFLEX (messages triviaux)
        ping_matches = sum(1 for pattern in self.classification_rules["ping"]["patterns"] if pattern in stimulus_lower)
        if ping_matches > 0:
            class_scores["ping"] = 1.0
            metadata["classification_reason"] = f"reflex_trivial_match_{ping_matches}"
            return class_scores, metadata

        # 3. ðŸŒ FALLBACK â†’ gen_short (TOUT le reste = GPT)
        # Si pas !ask et pas reflex, alors c'est une mention normale â†’ GPT concis
        class_scores["gen_short"] = 1.0
        metadata["classification_reason"] = "fallback_gpt_short"
        
        # MÃ©tadonnÃ©es finales
        metadata.update({
            "raw_scores": class_scores.copy(),
            "max_probability": 1.0,
            "predicted_class": "gen_short"
        })
        
        return class_scores, metadata

    def _evaluate_quantum_confidence(self, probabilities: Dict[str, float], entropy_analysis: Dict) -> Dict:
        """
        ðŸ“Š Ã‰valuation quantique de la confiance

        Combine :
        - ProbabilitÃ© maximum (dominance classe)
        - Entropie Shannon (incertitude distribution)
        - Ratio de dominance (Ã©cart entre classes)

        SACRED CODE - Ne pas modifier sans tests complets
        Formule empiriquement validÃ©e: 70% Shannon + 20% probability + 10% dominance
        """
        max_probability = entropy_analysis["max_probability"]
        entropy = entropy_analysis["entropy"]
        dominance_ratio = entropy_analysis["dominance_ratio"]

        # ðŸ“Š Score de confiance multi-facteurs
        # Facteur 1: ProbabilitÃ© dominante (0-1)
        prob_factor = max_probability

        # Facteur 2: Confiance Shannon normalisÃ©e EXACTE
        # Formule: 1 - H(S)/H_max oÃ¹ H_max = logâ‚‚(3) â‰ˆ 1.585 pour 3 classes
        H_max = 1.585  # Maximum thÃ©orique pour 3 classes (ping, gen_short, gen_long)
        shannon_confidence = max(0.0, 1.0 - (entropy / H_max))

        # Facteur 3: Dominance normalisÃ©e (0-1)
        dominance_factor = min(1.0, dominance_ratio / 10.0)  # Cap Ã  ratio 10:1

        # ðŸ§® Score final : Shannon (70%) + probabilitÃ© (20%) + dominance (10%)
        confidence_score = (
            shannon_confidence * 0.7 +   # 70% formule Shannon pure
            prob_factor * 0.2 +          # 20% probabilitÃ© max
            dominance_factor * 0.1       # 10% dominance
        )

        # ðŸŽ¯ Classification confiance
        if confidence_score >= self.confidence_thresholds["high_confidence"]:
            confidence_level = "high"
            is_certain = True
        elif confidence_score >= 0.5:
            confidence_level = "moderate"
            is_certain = True
        else:
            confidence_level = "low"
            is_certain = False

        return {
            "confidence_score": confidence_score,
            "confidence_level": confidence_level,
            "is_certain": is_certain,
            "factors": {
                "probability": prob_factor,
                "shannon_confidence": shannon_confidence,
                "dominance": dominance_factor
            }
        }

    def _quantum_collapse(self, probabilities: Dict[str, float], entropy_analysis: Dict, confidence_eval: Dict) -> str:
        """
        âš¡ EFFONDREMENT QUANTIQUE - Superposition â†’ Ã‰tat dÃ©terministe

        StratÃ©gie :
        1. Si entropie > seuil â†’ Fallback intelligent
        2. Si confiance faible â†’ Fallback ou classe dominante selon contexte
        3. Sinon â†’ Classe avec probabilitÃ© maximale
        """
        predicted_class = entropy_analysis["predicted_class"]
        entropy = entropy_analysis["entropy"]
        should_fallback = entropy_analysis["should_fallback"]

        # ðŸ”„ StratÃ©gie 1: Fallback entropie Ã©levÃ©e
        if should_fallback:
            fallback = self.entropy_calculator.get_fallback_recommendation(probabilities)
            self.logger.debug(f"ðŸ”„ Quantum fallback: entropy {entropy:.3f} > {self.confidence_thresholds['entropy_fallback']}")
            return fallback

        # ðŸŽ¯ StratÃ©gie 2: Confiance faible mais entropie acceptable
        if not confidence_eval["is_certain"]:
            max_prob = entropy_analysis["max_probability"]
            if max_prob < self.confidence_thresholds["minimum_probability"]:
                self.logger.debug(f"ðŸ”„ Confidence fallback: max_prob {max_prob:.3f} < {self.confidence_thresholds['minimum_probability']}")
                return self.fallback_class

        # âœ… StratÃ©gie 3: Classification normale
        return predicted_class

    def _detect_complex_words(self, stimulus_lower: str) -> List[str]:
        """ðŸ” DÃ©tecte les mots indicateurs de complexitÃ©"""
        found_complex = []
        for word in self.complex_indicators:
            if word in stimulus_lower:
                found_complex.append(word)
        return found_complex

    def classify_with_entropy(self, stimulus: str, context: str = "") -> Tuple[str, float]:
        """
        ðŸŽ¯ Classification + Entropie (compatible Neural V2.0)

        Returns:
            Tuple[str, float]: (classe, entropie)
        """
        result = self.classify(stimulus, context)
        return result["class"], result["entropy"]
