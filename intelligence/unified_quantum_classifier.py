#!/usr/bin/env python3
"""
ðŸŒŒ Unified Quantum Classifier - Classification par Intention + Entropie Shannon
Fusion ImprovedClassifier + StaticQuantumClassifier en un seul fichier optimisÃ©
Version: 3.1 (Fusion Safe - Phase 1)
"""

import logging
from typing import Dict, List, Tuple, Any, Optional


class UnifiedQuantumClassifier:
    """
    ðŸŒŒ UNIFIED QUANTUM CLASSIFIER V3.1

    Paradigme Physique/MathÃ©matique :
    - Classification par INTENTION (ping/gen_short/lookup/gen_long)
    - Superposition : Messages existent dans toutes les classes jusqu'Ã  "mesure"
    - Entropie Shannon : Mesure l'incertitude pour fallback intelligent
    - Effondrement : Distribution probabiliste â†’ Classe dÃ©terministe

    Architecture Unified :
    - Base patterns + logique complexitÃ© (ex-ImprovedClassifier)
    - EntropyCalculator + confiance quantique (ex-StaticQuantumClassifier)
    - Cache 29x speedup sur messages rÃ©pÃ©tÃ©s
    - Interface uniforme : classify() â†’ {class, confidence, entropy, ...}

    AmÃ©liorations vs classification par longueur brute :
    - Analyse contextuelle des mentions
    - Classification par intention (pas longueur)
    - Fallback intelligent par complexitÃ© linguistique
    - Distribution probabiliste avec entropie Shannon
    """

    def __init__(self, config: Optional[Dict] = None, patterns_config_path: Optional[str] = None):
        """
        ðŸ—ï¸ Initialisation UnifiedQuantumClassifier avec Enhanced Patterns

        Args:
            config: Configuration optionnelle pour seuils et paramÃ¨tres
            patterns_config_path: Chemin vers fichier patterns YAML
        """
        self.logger = logging.getLogger(__name__)

        # ðŸŽ¯ RÃˆGLES DE CLASSIFICATION PAR INTENTION (3 CLASSES)
        self.classification_rules = {
            "ping": {
                "patterns": ["ping", "test", "alive", "salut", "coucou", "bonjour", "merci", "ok", "thx", "ty"],
                "description": "Salutations, confirmations, rÃ©actions simples",
                "target_response": "Court, amical, rÃ©actif (1-2 phrases)",
                "priority": "social"
            },

            "gen_short": {
                "patterns": [
                    # Salutations avec mention (ex: "bonsoir serda_bot")
                    "bonsoir",
                    # Questions courtes
                    "comment", "pourquoi", "quand", "oÃ¹", "aide", "help", "peux-tu", "peux tu", "comment faire", "comment Ã§a",
                    # Patterns factuels (ex-lookup fusionnÃ©)
                    "qui est", "c'est quoi", "qu'est-ce que", "dÃ©finition", "game info", "steam info", "qu'est ce que", "info sur"
                ],
                "description": "Questions courtes, demandes d'aide, recherches factuelles simples",
                "target_response": "RÃ©ponse concise mais informative (2-4 phrases)",
                "priority": "question_simple"
            },

            "gen_long": {
                "patterns": ["!ask", "explique", "raconte", "dÃ©veloppe", "dÃ©taille", "analyse", "parle moi de", "dis moi tout", "explique moi", "comment Ã§a marche", "comment fonctionne"],
                "description": "Questions complexes, analyses approfondies",
                "target_response": "RÃ©ponse dÃ©taillÃ©e et nuancÃ©e (5+ phrases)",
                "priority": "complex_analysis"
            }
        }

        # ðŸŽ¯ Enhanced Patterns Loader (override si config YAML fourni)
        if patterns_config_path:
            from .enhanced_patterns_loader import EnhancedPatternsLoader
            self.patterns_loader = EnhancedPatternsLoader(patterns_config_path)
            self.classification_rules = self.patterns_loader.get_classification_rules()

        # ðŸ” MOTS INDICATEURS DE COMPLEXITÃ‰
        self.complex_indicators = [
            "pourquoi", "comment", "analyse", "explique", "dÃ©veloppe", "dÃ©taille", "dÃ©tails", "dÃ©tail",
            "thÃ©orie", "principe", "fonctionnement", "mÃ©canisme", "architecture", "fonctionne",
            "explication", "mÃ©thode", "mÃ©thodes", "stratÃ©gie", "stratÃ©gies", "technique", "techniques",
            "procÃ©dure", "procÃ©dures", "algorithme", "algorithmes", "approche", "approches",
            "mÃ©thodologie", "mÃ©thodologies", "concept", "concepts", "notion", "notions"
        ]

        # ðŸ§® Calculateur d'entropie Shannon
        from .entropy_calculator import EntropyCalculator
        self.entropy_calculator = EntropyCalculator()

        # âš™ï¸ Configuration quantique
        self.config = config or {}
        self.quantum_config = self.config.get("quantum_classifier", {})

        # ðŸ“Š Seuils configurables (avec defaults intelligents)
        self.confidence_thresholds = {
            "high_confidence": self.quantum_config.get("high_confidence_threshold", 0.7),
            "entropy_fallback": self.quantum_config.get("entropy_fallback_threshold", 1.5),
            "minimum_probability": self.quantum_config.get("minimum_probability", 0.1)
        }

        # ðŸŽ¯ Fallback strategy
        self.fallback_class = self.quantum_config.get("fallback_class", "gen_short")

        # ðŸš€ Cache pour messages rÃ©pÃ©tÃ©s (pog, !discord, etc.) - 29x speedup
        self._classify_cache: Dict[Tuple[str, str], Dict[str, Any]] = {}
        self._cache_maxsize = 256

        self.logger.info("ðŸŒŒ UnifiedQuantumClassifier V3.1 initialized (Fusion Safe)")

    def classify(self, stimulus: str, context: str = "") -> Dict[str, Any]:
        """
        ðŸŽ¯ CLASSIFICATION QUANTIQUE COMPLÃˆTE (Main API)

        Processus :
        1. Superposition â†’ Calcul probabilitÃ©s toutes classes
        2. Entropie â†’ Mesure incertitude distribution
        3. Ã‰valuation â†’ Confiance + besoin fallback
        4. Effondrement â†’ Classe finale dÃ©terministe

        Args:
            stimulus: Message utilisateur
            context: Contexte optionnel

        Returns:
            Dict avec classification complÃ¨te :
            {
                "class": "gen_long",
                "confidence": 0.85,
                "entropy": 0.64,
                "is_certain": True,
                "should_fallback": False,
                "probabilities": {"ping": 0.1, "gen_short": 0.2, ...},
                "distribution_type": "concentrated",
                "method": "quantum_classification"
            }
        """
        # ðŸš€ Check cache (0ms pour messages rÃ©pÃ©tÃ©s)
        cache_key = (stimulus, context)
        if cache_key in self._classify_cache:
            return self._classify_cache[cache_key]

        # ðŸŒŒ Phase 1: SUPERPOSITION - Calcul probabilitÃ©s
        probabilities, classification_metadata = self.classify_with_probabilities(stimulus, context)

        # ðŸ§® Phase 2: ENTROPIE - Analyse incertitude
        entropy_analysis = self.entropy_calculator.analyze_distribution(probabilities)

        # ðŸŽ¯ Phase 3: Ã‰VALUATION - Confiance et fallback
        confidence_eval = self._evaluate_quantum_confidence(probabilities, entropy_analysis)

        # âš¡ Phase 4: EFFONDREMENT - Classe finale
        final_class = self._quantum_collapse(probabilities, entropy_analysis, confidence_eval)

        # ðŸ“Š Construction rÃ©sultat quantique complet
        quantum_result = {
            # ðŸŽ¯ RÃ©sultat principal
            "class": final_class,
            "confidence": confidence_eval["confidence_score"],
            "entropy": entropy_analysis["entropy"],

            # ðŸŒŒ Ã‰tat quantique
            "is_certain": confidence_eval["is_certain"],
            "should_fallback": entropy_analysis["should_fallback"],
            "probabilities": probabilities,

            # ðŸ“Š Analyses dÃ©taillÃ©es
            "distribution_type": entropy_analysis["distribution_type"],
            "dominance_ratio": entropy_analysis["dominance_ratio"],
            "confidence_level": entropy_analysis["confidence_level"],

            # ðŸ” MÃ©tadonnÃ©es techniques
            "method": "quantum_classification",
            "classification_reason": classification_metadata["classification_reason"],
            "metadata": classification_metadata,
            "entropy_analysis": entropy_analysis,
            "quantum_confidence": confidence_eval
        }

        self.logger.debug(f"ðŸŒŒ Quantum: '{stimulus}' â†’ {final_class} (entropy: {entropy_analysis['entropy']:.3f}, conf: {confidence_eval['confidence_score']:.3f})")

        # ðŸš€ Stocker en cache (max 256 messages, FIFO)
        if len(self._classify_cache) >= self._cache_maxsize:
            self._classify_cache.pop(next(iter(self._classify_cache)))
        self._classify_cache[cache_key] = quantum_result

        return quantum_result

    def classify_with_probabilities(self, stimulus: str, context: str = "") -> Tuple[Dict[str, float], Dict]:
        """
        ðŸŒŒ CLASSIFICATION QUANTIQUE - Retourne probabilitÃ©s pour chaque classe

        Cette mÃ©thode calcule des probabilitÃ©s pour chaque classe au lieu d'une classification binaire.
        Permet la superposition quantique avant effondrement vers une classe spÃ©cifique.

        Args:
            stimulus: Message utilisateur
            context: Contexte optionnel

        Returns:
            Tuple[Dict[str, float], Dict]: (probabilitÃ©s_par_classe, mÃ©tadonnÃ©es)

        Exemple:
            probabilities, metadata = classifier.classify_with_probabilities("explique moi Python")
            # probabilities = {"ping": 0.1, "gen_short": 0.2, "lookup": 0.3, "gen_long": 0.4}
        """
        stimulus_lower = stimulus.lower().strip()

        # ðŸ“Š Initialisation des scores par classe (3 classes)
        class_scores = {
            "ping": 0.0,
            "gen_short": 0.0,
            "gen_long": 0.0
        }

        # ðŸ” MÃ©tadonnÃ©es dÃ©taillÃ©es
        metadata = {
            "word_count": len(stimulus.split()),
            "has_question": "?" in stimulus,
            "has_mention": any(mention in stimulus_lower for mention in ["@", "serda_bot"]),
            "complex_words": self._detect_complex_words(stimulus_lower),
            "classification_method": "probabilistic"
        }

        # 1. ðŸŽ¯ CONTEXT OVERRIDE (prioritÃ© absolue)
        if context == "ask":
            # NOTE: Cette classification en gen_long est redondante car local_synapse.py (ligne 313)
            # utilise directement context="ask" AVANT de vÃ©rifier stimulus_class.
            # GardÃ© comme sÃ©curitÃ© dÃ©fensive si context="ask" n'est pas passÃ© correctement.
            class_scores["gen_long"] = 1.0
            metadata["classification_reason"] = "context_override_ask_redundant"
            return class_scores, metadata

        # 2. ðŸŽ¤ ANALYSE DES MENTIONS
        if metadata["has_mention"]:
            # Bonus pour mentions mais pas exclusif
            class_scores["gen_short"] += 0.3

            # Analyse du contenu de la mention
            for class_name, rule in self.classification_rules.items():
                mention_matches = sum(1 for pattern in rule["patterns"] if pattern in stimulus_lower)
                if mention_matches > 0:
                    class_scores[class_name] += mention_matches * 0.4

        # 3. ðŸ“‹ SCORE PAR PATTERNS D'INTENTION
        for class_name, rule in self.classification_rules.items():
            pattern_matches = sum(1 for pattern in rule["patterns"] if pattern in stimulus_lower)
            if pattern_matches > 0:
                # Score basÃ© sur nombre de patterns + prioritÃ©
                base_score = pattern_matches * 0.5

                # Bonus selon prioritÃ© de la classe
                priority = str(rule.get("priority", ""))
                priority_bonus = {
                    "social": 0.1,           # ping
                    "question_simple": 0.2, # gen_short
                    "factual": 0.3,         # lookup
                    "complex_analysis": 0.4  # gen_long
                }.get(priority, 0.0)

                class_scores[class_name] += base_score + priority_bonus

        # 4. ðŸ§  ANALYSE DE COMPLEXITÃ‰ LINGUISTIQUE
        word_count = metadata.get("word_count", 0)
        word_count_int = int(word_count) if isinstance(word_count, (int, float)) else 0
        complex_words = metadata.get("complex_words", [])
        complex_words_count = len(complex_words) if isinstance(complex_words, list) else 0
        has_question = metadata.get("has_question", False)
        has_question_bool = bool(has_question) if isinstance(has_question, bool) else False

        # Ajustements basÃ©s sur la complexitÃ©
        if word_count_int <= 3 and not has_question_bool and complex_words_count == 0:
            class_scores["ping"] += 0.6  # Message trÃ¨s court
        elif complex_words_count >= 2:
            class_scores["gen_long"] += 0.7  # Plusieurs mots complexes
        elif complex_words_count == 1 and word_count_int > 6:
            class_scores["gen_long"] += 0.5  # Un mot complexe + message long
        elif complex_words_count == 1:
            class_scores["gen_long"] += 0.8  # Mot complexe isolÃ© â†’ analyse approfondie
        elif word_count_int > 12:
            class_scores["gen_long"] += 0.4  # Message trÃ¨s long

        # Bonus questions courtes
        if has_question_bool and word_count_int <= 8 and complex_words_count <= 1:
            class_scores["gen_short"] += 0.3

        # 5. ðŸ“Š NORMALISATION EN PROBABILITÃ‰S
        total_score = sum(class_scores.values())

        if total_score > 0:
            probabilities = {k: v / total_score for k, v in class_scores.items()}
        else:
            # Fallback uniforme si aucun pattern (3 classes: 33.3% chacune)
            probabilities = {k: 1.0/3.0 for k in class_scores.keys()}

        # 6. ðŸŽ¯ SURCHARGE INTELLIGENTE
        max_class = max(probabilities.items(), key=lambda x: x[1])[0]

        # Surcharge gen_short â†’ gen_long si 2+ mots complexes
        if max_class == "gen_short" and complex_words_count >= 2:
            # Redistribue la probabilitÃ© vers gen_long
            boost = probabilities["gen_short"] * 0.6
            probabilities["gen_long"] += boost
            probabilities["gen_short"] -= boost
            metadata["classification_reason"] = "probability_boost_complexity"
        else:
            metadata["classification_reason"] = f"probability_match_{max_class}"

        # 7. ðŸ“ˆ MÃ‰TADONNÃ‰ES ENRICHIES
        metadata.update({
            "raw_scores": class_scores.copy(),
            "total_raw_score": total_score,
            "max_probability": max(probabilities.values()),
            "predicted_class": max(probabilities.items(), key=lambda x: x[1])[0]
        })

        return probabilities, metadata

    def _evaluate_quantum_confidence(self, probabilities: Dict[str, float], entropy_analysis: Dict) -> Dict:
        """
        ðŸ“Š Ã‰valuation quantique de la confiance

        Combine :
        - ProbabilitÃ© maximum (dominance classe)
        - Entropie Shannon (incertitude distribution)
        - Ratio de dominance (Ã©cart entre classes)

        SACRED CODE - Ne pas modifier sans tests complets
        Formule empiriquement validÃ©e: 70% Shannon + 20% probability + 10% dominance
        """
        max_probability = entropy_analysis["max_probability"]
        entropy = entropy_analysis["entropy"]
        dominance_ratio = entropy_analysis["dominance_ratio"]

        # ðŸ“Š Score de confiance multi-facteurs
        # Facteur 1: ProbabilitÃ© dominante (0-1)
        prob_factor = max_probability

        # Facteur 2: Confiance Shannon normalisÃ©e EXACTE
        # Formule: 1 - H(S)/H_max oÃ¹ H_max = logâ‚‚(3) â‰ˆ 1.585 pour 3 classes
        H_max = 1.585  # Maximum thÃ©orique pour 3 classes (ping, gen_short, gen_long)
        shannon_confidence = max(0.0, 1.0 - (entropy / H_max))

        # Facteur 3: Dominance normalisÃ©e (0-1)
        dominance_factor = min(1.0, dominance_ratio / 10.0)  # Cap Ã  ratio 10:1

        # ðŸ§® Score final : Shannon (70%) + probabilitÃ© (20%) + dominance (10%)
        confidence_score = (
            shannon_confidence * 0.7 +   # 70% formule Shannon pure
            prob_factor * 0.2 +          # 20% probabilitÃ© max
            dominance_factor * 0.1       # 10% dominance
        )

        # ðŸŽ¯ Classification confiance
        if confidence_score >= self.confidence_thresholds["high_confidence"]:
            confidence_level = "high"
            is_certain = True
        elif confidence_score >= 0.5:
            confidence_level = "moderate"
            is_certain = True
        else:
            confidence_level = "low"
            is_certain = False

        return {
            "confidence_score": confidence_score,
            "confidence_level": confidence_level,
            "is_certain": is_certain,
            "factors": {
                "probability": prob_factor,
                "shannon_confidence": shannon_confidence,
                "dominance": dominance_factor
            }
        }

    def _quantum_collapse(self, probabilities: Dict[str, float], entropy_analysis: Dict, confidence_eval: Dict) -> str:
        """
        âš¡ EFFONDREMENT QUANTIQUE - Superposition â†’ Ã‰tat dÃ©terministe

        StratÃ©gie :
        1. Si entropie > seuil â†’ Fallback intelligent
        2. Si confiance faible â†’ Fallback ou classe dominante selon contexte
        3. Sinon â†’ Classe avec probabilitÃ© maximale
        """
        predicted_class = entropy_analysis["predicted_class"]
        entropy = entropy_analysis["entropy"]
        should_fallback = entropy_analysis["should_fallback"]

        # ðŸ”„ StratÃ©gie 1: Fallback entropie Ã©levÃ©e
        if should_fallback:
            fallback = self.entropy_calculator.get_fallback_recommendation(probabilities)
            self.logger.debug(f"ðŸ”„ Quantum fallback: entropy {entropy:.3f} > {self.confidence_thresholds['entropy_fallback']}")
            return fallback

        # ðŸŽ¯ StratÃ©gie 2: Confiance faible mais entropie acceptable
        if not confidence_eval["is_certain"]:
            max_prob = entropy_analysis["max_probability"]
            if max_prob < self.confidence_thresholds["minimum_probability"]:
                self.logger.debug(f"ðŸ”„ Confidence fallback: max_prob {max_prob:.3f} < {self.confidence_thresholds['minimum_probability']}")
                return self.fallback_class

        # âœ… StratÃ©gie 3: Classification normale
        return predicted_class

    def _detect_complex_words(self, stimulus_lower: str) -> List[str]:
        """ðŸ” DÃ©tecte les mots indicateurs de complexitÃ©"""
        found_complex = []
        for word in self.complex_indicators:
            if word in stimulus_lower:
                found_complex.append(word)
        return found_complex

    def classify_with_entropy(self, stimulus: str, context: str = "") -> Tuple[str, float]:
        """
        ðŸŽ¯ Classification + Entropie (compatible Neural V2.0)

        Returns:
            Tuple[str, float]: (classe, entropie)
        """
        result = self.classify(stimulus, context)
        return result["class"], result["entropy"]


# ðŸ§ª FONCTION DE TEST
def test_unified_quantum_classifier():
    """ðŸ§ª Tests de validation du classifier unifiÃ©"""
    classifier = UnifiedQuantumClassifier()

    test_cases = [
        ("bonsoir serda_bot", "", "gen_short"),  # Mention casual
        ("salut !", "", "ping"),  # Salutation simple
        ("@serda_bot comment Ã§a va ?", "", "gen_short"),  # Mention + question simple
        ("!ask explique moi quantum physics", "", "gen_long"),  # Context override
        ("qui est Einstein ?", "", "gen_short"),  # Ex-lookup â†’ gen_short
        ("c'est quoi un bot ?", "", "gen_short"),  # Ex-lookup â†’ gen_short
        ("raconte moi une histoire", "", "gen_long"),  # GÃ©nÃ©ration longue
        ("ping", "", "ping"),  # Test simple
        ("merci bien", "", "ping"),  # RÃ©action courte
        ("peux-tu m'aider avec Python ?", "", "gen_short"),  # Demande d'aide
        ("@serda_bot pourquoi le ciel est bleu ?", "", "gen_short"),  # Mention + question
        ("dÃ©veloppe sur l'intelligence artificielle", "", "gen_long"),  # Analyse complexe
    ]

    print("ðŸ§ª TEST UNIFIED QUANTUM CLASSIFIER V3.1")
    print("=" * 50)

    success_count = 0
    for stimulus, context, expected in test_cases:
        result = classifier.classify(stimulus, context)
        classification = result["class"]
        status = "âœ…" if classification == expected else "âŒ"
        success_count += 1 if classification == expected else 0

        print(f"{status} '{stimulus}' â†’ {classification} (attendu: {expected})")
        print(f"    Entropy: {result['entropy']:.3f}, Confidence: {result['confidence']:.3f} ({result['confidence_level']})")
        if classification != expected:
            print(f"    Raison: {result['classification_reason']}")

    print(f"\nðŸ“Š RÃ©sultat: {success_count}/{len(test_cases)} tests rÃ©ussis ({success_count/len(test_cases)*100:.1f}%)")

    # Test cache performance
    print("\nðŸš€ TEST CACHE PERFORMANCE")
    import time

    # Premier appel (sans cache)
    start = time.time()
    for _ in range(100):
        classifier.classify("pog")
    no_cache_time = time.time() - start

    # DeuxiÃ¨me appel (avec cache)
    start = time.time()
    for _ in range(100):
        classifier.classify("pog")
    cache_time = time.time() - start

    speedup = no_cache_time / cache_time if cache_time > 0 else float('inf')
    print(f"Sans cache: {no_cache_time*1000:.2f}ms, Avec cache: {cache_time*1000:.2f}ms")
    print(f"Speedup: {speedup:.1f}x")

    return success_count == len(test_cases)


def demo_unified_quantum_classifier():
    """ðŸ§ª DÃ©monstration UnifiedQuantumClassifier"""
    print("ðŸŒŒ DEMONSTRATION UNIFIED QUANTUM CLASSIFIER V3.1")
    print("=" * 55)

    # Configuration de test
    config = {
        "quantum_classifier": {
            "high_confidence_threshold": 0.75,
            "entropy_fallback_threshold": 1.5,
            "minimum_probability": 0.1,
            "fallback_class": "gen_short"
        }
    }

    classifier = UnifiedQuantumClassifier(config)

    # ðŸ§ª Cas de test quantiques
    test_cases = [
        ("ping", "Haute confiance"),
        ("bonsoir serda_bot", "Mention casual"),
        ("explique moi la physique quantique", "ComplexitÃ© Ã©levÃ©e"),
        ("c'est quoi Python ?", "Lookup factuel"),
        ("salut aide comment pourquoi", "Incertitude Ã©levÃ©e â†’ Fallback"),
        ("@serda_bot raconte moi une histoire", "Mention + gÃ©nÃ©ration"),
    ]

    for stimulus, description in test_cases:
        print(f"\nðŸ§ª {description}: '{stimulus}'")

        result = classifier.classify(stimulus)

        print(f"   ðŸŽ¯ Classe: {result['class']}")
        print(f"   ðŸ“Š Confiance: {result['confidence_level']} ({result['confidence']:.3f})")
        print(f"   ðŸ§® Entropie: {result['entropy']:.3f}")
        print(f"   ðŸ”„ Fallback: {'OUI' if result['should_fallback'] else 'NON'}")
        print(f"   ðŸŒŒ Distribution: {result['distribution_type']}")

        # Top 2 probabilitÃ©s
        sorted_probs = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:2]
        print(f"   ðŸ“ˆ Top classes: {sorted_probs[0][0]}({sorted_probs[0][1]:.2f}), {sorted_probs[1][0]}({sorted_probs[1][1]:.2f})")


if __name__ == "__main__":
    # Tests autonomes
    test_unified_quantum_classifier()
    print("\n" + "=" * 55 + "\n")
    demo_unified_quantum_classifier()
