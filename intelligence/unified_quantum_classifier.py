#!/usr/bin/env python3
"""
üåå Unified Quantum Classifier - Classification par Intention + Entropie Shannon
Fusion ImprovedClassifier + StaticQuantumClassifier en un seul fichier optimis√©
Version: 3.1 (Fusion Safe - Phase 1)
"""

import logging
from typing import Dict, List, Tuple, Any, Optional


class UnifiedQuantumClassifier:
    """
    üåå UNIFIED QUANTUM CLASSIFIER V3.1

    Paradigme Physique/Math√©matique :
    - Classification par INTENTION (ping/gen_short/lookup/gen_long)
    - Superposition : Messages existent dans toutes les classes jusqu'√† "mesure"
    - Entropie Shannon : Mesure l'incertitude pour fallback intelligent
    - Effondrement : Distribution probabiliste ‚Üí Classe d√©terministe

    Architecture Unified :
    - Base patterns + logique complexit√© (ex-ImprovedClassifier)
    - EntropyCalculator + confiance quantique (ex-StaticQuantumClassifier)
    - Cache 29x speedup sur messages r√©p√©t√©s
    - Interface uniforme : classify() ‚Üí {class, confidence, entropy, ...}

    Am√©liorations vs classification par longueur brute :
    - Analyse contextuelle des mentions
    - Classification par intention (pas longueur)
    - Fallback intelligent par complexit√© linguistique
    - Distribution probabiliste avec entropie Shannon
    """

    def __init__(self, config: Optional[Dict] = None, patterns_config_path: Optional[str] = None):
        """
        üèóÔ∏è Initialisation UnifiedQuantumClassifier avec Enhanced Patterns

        Args:
            config: Configuration optionnelle pour seuils et param√®tres
            patterns_config_path: Chemin vers fichier patterns YAML
        """
        self.logger = logging.getLogger(__name__)

        # üéØ R√àGLES DE CLASSIFICATION PAR INTENTION (3 CLASSES)
        self.classification_rules = {
            "ping": {
                "patterns": ["ping", "test", "alive", "salut", "coucou", "bonjour", "merci", "ok", "thx", "ty"],
                "description": "Salutations, confirmations, r√©actions simples",
                "target_response": "Court, amical, r√©actif (1-2 phrases)",
                "priority": "social"
            },

            "gen_short": {
                "patterns": [
                    # Salutations avec mention (ex: "bonsoir serda_bot")
                    "bonsoir",
                    # Questions courtes
                    "comment", "pourquoi", "quand", "o√π", "aide", "help", "peux-tu", "peux tu", "comment faire", "comment √ßa",
                    # Patterns factuels (ex-lookup fusionn√©)
                    "qui est", "c'est quoi", "qu'est-ce que", "d√©finition", "game info", "steam info", "qu'est ce que", "info sur"
                ],
                "description": "Questions courtes, demandes d'aide, recherches factuelles simples",
                "target_response": "R√©ponse concise mais informative (2-4 phrases)",
                "priority": "question_simple"
            },

            "gen_long": {
                "patterns": ["!ask", "explique", "raconte", "d√©veloppe", "d√©taille", "analyse", "parle moi de", "dis moi tout", "explique moi", "comment √ßa marche", "comment fonctionne"],
                "description": "Questions complexes, analyses approfondies",
                "target_response": "R√©ponse d√©taill√©e et nuanc√©e (5+ phrases)",
                "priority": "complex_analysis"
            }
        }

        # üéØ Enhanced Patterns Loader (override si config YAML fourni)
        if patterns_config_path:
            from .enhanced_patterns_loader import EnhancedPatternsLoader
            self.patterns_loader = EnhancedPatternsLoader(patterns_config_path)
            self.classification_rules = self.patterns_loader.get_classification_rules()

        # üîç MOTS INDICATEURS DE COMPLEXIT√â
        self.complex_indicators = [
            "pourquoi", "comment", "analyse", "explique", "d√©veloppe", "d√©taille", "d√©tails", "d√©tail",
            "th√©orie", "principe", "fonctionnement", "m√©canisme", "architecture", "fonctionne",
            "explication", "m√©thode", "m√©thodes", "strat√©gie", "strat√©gies", "technique", "techniques",
            "proc√©dure", "proc√©dures", "algorithme", "algorithmes", "approche", "approches",
            "m√©thodologie", "m√©thodologies", "concept", "concepts", "notion", "notions"
        ]

        # üßÆ Calculateur d'entropie Shannon
        from .entropy_calculator import EntropyCalculator
        self.entropy_calculator = EntropyCalculator()

        # ‚öôÔ∏è Configuration quantique
        self.config = config or {}
        self.quantum_config = self.config.get("quantum_classifier", {})

        # üìä Seuils configurables (avec defaults intelligents)
        self.confidence_thresholds = {
            "high_confidence": self.quantum_config.get("high_confidence_threshold", 0.7),
            "entropy_fallback": self.quantum_config.get("entropy_fallback_threshold", 1.5),
            "minimum_probability": self.quantum_config.get("minimum_probability", 0.1)
        }

        # üéØ Fallback strategy
        self.fallback_class = self.quantum_config.get("fallback_class", "gen_short")

        # üöÄ Cache pour messages r√©p√©t√©s (pog, !discord, etc.) - 29x speedup
        self._classify_cache: Dict[Tuple[str, str], Dict[str, Any]] = {}
        self._cache_maxsize = 256

        self.logger.info("üåå UnifiedQuantumClassifier V3.1 initialized (Fusion Safe)")

    def classify(self, stimulus: str, context: str = "") -> Dict[str, Any]:
        """
        üéØ CLASSIFICATION QUANTIQUE COMPL√àTE (Main API)

        Processus :
        1. Superposition ‚Üí Calcul probabilit√©s toutes classes
        2. Entropie ‚Üí Mesure incertitude distribution
        3. √âvaluation ‚Üí Confiance + besoin fallback
        4. Effondrement ‚Üí Classe finale d√©terministe

        Args:
            stimulus: Message utilisateur
            context: Contexte optionnel

        Returns:
            Dict avec classification compl√®te :
            {
                "class": "gen_long",
                "confidence": 0.85,
                "entropy": 0.64,
                "is_certain": True,
                "should_fallback": False,
                "probabilities": {"ping": 0.1, "gen_short": 0.2, ...},
                "distribution_type": "concentrated",
                "method": "quantum_classification"
            }
        """
        # üöÄ Check cache (0ms pour messages r√©p√©t√©s)
        cache_key = (stimulus, context)
        if cache_key in self._classify_cache:
            return self._classify_cache[cache_key]

        # üåå Phase 1: SUPERPOSITION - Calcul probabilit√©s
        probabilities, classification_metadata = self.classify_with_probabilities(stimulus, context)

        # üßÆ Phase 2: ENTROPIE - Analyse incertitude
        entropy_analysis = self.entropy_calculator.analyze_distribution(probabilities)

        # üéØ Phase 3: √âVALUATION - Confiance et fallback
        confidence_eval = self._evaluate_quantum_confidence(probabilities, entropy_analysis)

        # ‚ö° Phase 4: EFFONDREMENT - Classe finale
        final_class = self._quantum_collapse(probabilities, entropy_analysis, confidence_eval)

        # üìä Construction r√©sultat quantique complet
        quantum_result = {
            # üéØ R√©sultat principal
            "class": final_class,
            "confidence": confidence_eval["confidence_score"],
            "entropy": entropy_analysis["entropy"],

            # üåå √âtat quantique
            "is_certain": confidence_eval["is_certain"],
            "should_fallback": entropy_analysis["should_fallback"],
            "probabilities": probabilities,

            # üìä Analyses d√©taill√©es
            "distribution_type": entropy_analysis["distribution_type"],
            "dominance_ratio": entropy_analysis["dominance_ratio"],
            "confidence_level": entropy_analysis["confidence_level"],

            # üîç M√©tadonn√©es techniques
            "method": "quantum_classification",
            "classification_reason": classification_metadata["classification_reason"],
            "metadata": classification_metadata,
            "entropy_analysis": entropy_analysis,
            "quantum_confidence": confidence_eval
        }

        self.logger.debug(f"üåå Quantum: '{stimulus}' ‚Üí {final_class} (entropy: {entropy_analysis['entropy']:.3f}, conf: {confidence_eval['confidence_score']:.3f})")

        # üöÄ Stocker en cache (max 256 messages, FIFO)
        if len(self._classify_cache) >= self._cache_maxsize:
            self._classify_cache.pop(next(iter(self._classify_cache)))
        self._classify_cache[cache_key] = quantum_result

        return quantum_result

    def classify_with_probabilities(self, stimulus: str, context: str = "") -> Tuple[Dict[str, float], Dict]:
        """
        üåå CLASSIFICATION QUANTIQUE - Retourne probabilit√©s pour chaque classe

        Cette m√©thode calcule des probabilit√©s pour chaque classe au lieu d'une classification binaire.
        Permet la superposition quantique avant effondrement vers une classe sp√©cifique.

        Args:
            stimulus: Message utilisateur
            context: Contexte optionnel

        Returns:
            Tuple[Dict[str, float], Dict]: (probabilit√©s_par_classe, m√©tadonn√©es)

        Exemple:
            probabilities, metadata = classifier.classify_with_probabilities("explique moi Python")
            # probabilities = {"ping": 0.1, "gen_short": 0.2, "lookup": 0.3, "gen_long": 0.4}
        """
        stimulus_lower = stimulus.lower().strip()

        # üìä Initialisation des scores par classe (3 classes)
        class_scores = {
            "ping": 0.0,
            "gen_short": 0.0,
            "gen_long": 0.0
        }

        # üîç M√©tadonn√©es d√©taill√©es
        metadata = {
            "word_count": len(stimulus.split()),
            "has_question": "?" in stimulus,
            "has_mention": any(mention in stimulus_lower for mention in ["@", "serda_bot"]),
            "complex_words": self._detect_complex_words(stimulus_lower),
            "classification_method": "probabilistic"
        }

        # 1. üéØ CONTEXT OVERRIDE (priorit√© absolue)
        if context == "ask":
            # NOTE: Cette classification en gen_long est redondante car local_synapse.py (ligne 313)
            # utilise directement context="ask" AVANT de v√©rifier stimulus_class.
            # Gard√© comme s√©curit√© d√©fensive si context="ask" n'est pas pass√© correctement.
            class_scores["gen_long"] = 1.0
            metadata["classification_reason"] = "context_override_ask_redundant"
            return class_scores, metadata

        # 2. üé§ ANALYSE DES MENTIONS
        if metadata["has_mention"]:
            # Bonus pour mentions mais pas exclusif
            class_scores["gen_short"] += 0.3

            # Analyse du contenu de la mention
            for class_name, rule in self.classification_rules.items():
                mention_matches = sum(1 for pattern in rule["patterns"] if pattern in stimulus_lower)
                if mention_matches > 0:
                    class_scores[class_name] += mention_matches * 0.4

        # 3. üìã SCORE PAR PATTERNS D'INTENTION
        for class_name, rule in self.classification_rules.items():
            pattern_matches = sum(1 for pattern in rule["patterns"] if pattern in stimulus_lower)
            if pattern_matches > 0:
                # Score bas√© sur nombre de patterns + priorit√©
                base_score = pattern_matches * 0.5

                # Bonus selon priorit√© de la classe
                priority = str(rule.get("priority", ""))
                priority_bonus = {
                    "social": 0.1,           # ping
                    "question_simple": 0.2, # gen_short
                    "factual": 0.3,         # lookup
                    "complex_analysis": 0.4  # gen_long
                }.get(priority, 0.0)

                class_scores[class_name] += base_score + priority_bonus

        # 4. üß† ANALYSE DE COMPLEXIT√â LINGUISTIQUE
        word_count = metadata.get("word_count", 0)
        word_count_int = int(word_count) if isinstance(word_count, (int, float)) else 0
        complex_words = metadata.get("complex_words", [])
        complex_words_count = len(complex_words) if isinstance(complex_words, list) else 0
        has_question = metadata.get("has_question", False)
        has_question_bool = bool(has_question) if isinstance(has_question, bool) else False

        # Ajustements bas√©s sur la complexit√©
        if word_count_int <= 3 and not has_question_bool and complex_words_count == 0:
            class_scores["ping"] += 0.6  # Message tr√®s court
        elif complex_words_count >= 2:
            class_scores["gen_long"] += 0.7  # Plusieurs mots complexes
        elif complex_words_count == 1 and word_count_int > 6:
            class_scores["gen_long"] += 0.5  # Un mot complexe + message long
        elif complex_words_count == 1:
            class_scores["gen_long"] += 0.8  # Mot complexe isol√© ‚Üí analyse approfondie
        elif word_count_int > 12:
            class_scores["gen_long"] += 0.4  # Message tr√®s long

        # Bonus questions courtes
        if has_question_bool and word_count_int <= 8 and complex_words_count <= 1:
            class_scores["gen_short"] += 0.3

        # 5. üìä NORMALISATION EN PROBABILIT√âS
        total_score = sum(class_scores.values())

        if total_score > 0:
            probabilities = {k: v / total_score for k, v in class_scores.items()}
        else:
            # Fallback uniforme si aucun pattern (3 classes: 33.3% chacune)
            probabilities = {k: 1.0/3.0 for k in class_scores.keys()}

        # 6. üéØ SURCHARGE INTELLIGENTE
        max_class = max(probabilities.items(), key=lambda x: x[1])[0]

        # Surcharge gen_short ‚Üí gen_long si 2+ mots complexes
        if max_class == "gen_short" and complex_words_count >= 2:
            # Redistribue la probabilit√© vers gen_long
            boost = probabilities["gen_short"] * 0.6
            probabilities["gen_long"] += boost
            probabilities["gen_short"] -= boost
            metadata["classification_reason"] = "probability_boost_complexity"
        else:
            metadata["classification_reason"] = f"probability_match_{max_class}"

        # 7. üìà M√âTADONN√âES ENRICHIES
        metadata.update({
            "raw_scores": class_scores.copy(),
            "total_raw_score": total_score,
            "max_probability": max(probabilities.values()),
            "predicted_class": max(probabilities.items(), key=lambda x: x[1])[0]
        })

        return probabilities, metadata

    def _evaluate_quantum_confidence(self, probabilities: Dict[str, float], entropy_analysis: Dict) -> Dict:
        """
        üìä √âvaluation quantique de la confiance

        Combine :
        - Probabilit√© maximum (dominance classe)
        - Entropie Shannon (incertitude distribution)
        - Ratio de dominance (√©cart entre classes)

        SACRED CODE - Ne pas modifier sans tests complets
        Formule empiriquement valid√©e: 70% Shannon + 20% probability + 10% dominance
        """
        max_probability = entropy_analysis["max_probability"]
        entropy = entropy_analysis["entropy"]
        dominance_ratio = entropy_analysis["dominance_ratio"]

        # üìä Score de confiance multi-facteurs
        # Facteur 1: Probabilit√© dominante (0-1)
        prob_factor = max_probability

        # Facteur 2: Confiance Shannon normalis√©e EXACTE
        # Formule: 1 - H(S)/H_max o√π H_max = log‚ÇÇ(3) ‚âà 1.585 pour 3 classes
        H_max = 1.585  # Maximum th√©orique pour 3 classes (ping, gen_short, gen_long)
        shannon_confidence = max(0.0, 1.0 - (entropy / H_max))

        # Facteur 3: Dominance normalis√©e (0-1)
        dominance_factor = min(1.0, dominance_ratio / 10.0)  # Cap √† ratio 10:1

        # üßÆ Score final : Shannon (70%) + probabilit√© (20%) + dominance (10%)
        confidence_score = (
            shannon_confidence * 0.7 +   # 70% formule Shannon pure
            prob_factor * 0.2 +          # 20% probabilit√© max
            dominance_factor * 0.1       # 10% dominance
        )

        # üéØ Classification confiance
        if confidence_score >= self.confidence_thresholds["high_confidence"]:
            confidence_level = "high"
            is_certain = True
        elif confidence_score >= 0.5:
            confidence_level = "moderate"
            is_certain = True
        else:
            confidence_level = "low"
            is_certain = False

        return {
            "confidence_score": confidence_score,
            "confidence_level": confidence_level,
            "is_certain": is_certain,
            "factors": {
                "probability": prob_factor,
                "shannon_confidence": shannon_confidence,
                "dominance": dominance_factor
            }
        }

    def _quantum_collapse(self, probabilities: Dict[str, float], entropy_analysis: Dict, confidence_eval: Dict) -> str:
        """
        ‚ö° EFFONDREMENT QUANTIQUE - Superposition ‚Üí √âtat d√©terministe

        Strat√©gie :
        1. Si entropie > seuil ‚Üí Fallback intelligent
        2. Si confiance faible ‚Üí Fallback ou classe dominante selon contexte
        3. Sinon ‚Üí Classe avec probabilit√© maximale
        """
        predicted_class = entropy_analysis["predicted_class"]
        entropy = entropy_analysis["entropy"]
        should_fallback = entropy_analysis["should_fallback"]

        # üîÑ Strat√©gie 1: Fallback entropie √©lev√©e
        if should_fallback:
            fallback = self.entropy_calculator.get_fallback_recommendation(probabilities)
            self.logger.debug(f"üîÑ Quantum fallback: entropy {entropy:.3f} > {self.confidence_thresholds['entropy_fallback']}")
            return fallback

        # üéØ Strat√©gie 2: Confiance faible mais entropie acceptable
        if not confidence_eval["is_certain"]:
            max_prob = entropy_analysis["max_probability"]
            if max_prob < self.confidence_thresholds["minimum_probability"]:
                self.logger.debug(f"üîÑ Confidence fallback: max_prob {max_prob:.3f} < {self.confidence_thresholds['minimum_probability']}")
                return self.fallback_class

        # ‚úÖ Strat√©gie 3: Classification normale
        return predicted_class

    def _detect_complex_words(self, stimulus_lower: str) -> List[str]:
        """üîç D√©tecte les mots indicateurs de complexit√©"""
        found_complex = []
        for word in self.complex_indicators:
            if word in stimulus_lower:
                found_complex.append(word)
        return found_complex

    def classify_with_entropy(self, stimulus: str, context: str = "") -> Tuple[str, float]:
        """
        üéØ Classification + Entropie (compatible Neural V2.0)

        Returns:
            Tuple[str, float]: (classe, entropie)
        """
        result = self.classify(stimulus, context)
        return result["class"], result["entropy"]


# üß™ FONCTION DE TEST
def test_unified_quantum_classifier():
    """üß™ Tests de validation du classifier unifi√©"""
    classifier = UnifiedQuantumClassifier()

    test_cases = [
        ("bonsoir serda_bot", "", "gen_short"),  # Mention casual
        ("salut !", "", "ping"),  # Salutation simple
        ("@serda_bot comment √ßa va ?", "", "gen_short"),  # Mention + question simple
        ("!ask explique moi quantum physics", "", "gen_long"),  # Context override
        ("qui est Einstein ?", "", "gen_short"),  # Ex-lookup ‚Üí gen_short
        ("c'est quoi un bot ?", "", "gen_short"),  # Ex-lookup ‚Üí gen_short
        ("raconte moi une histoire", "", "gen_long"),  # G√©n√©ration longue
        ("ping", "", "ping"),  # Test simple
        ("merci bien", "", "ping"),  # R√©action courte
        ("peux-tu m'aider avec Python ?", "", "gen_short"),  # Demande d'aide
        ("@serda_bot pourquoi le ciel est bleu ?", "", "gen_short"),  # Mention + question
        ("d√©veloppe sur l'intelligence artificielle", "", "gen_long"),  # Analyse complexe
    ]

    print("üß™ TEST UNIFIED QUANTUM CLASSIFIER V3.1")
    print("=" * 50)

    success_count = 0
    for stimulus, context, expected in test_cases:
        result = classifier.classify(stimulus, context)
        classification = result["class"]
        status = "‚úÖ" if classification == expected else "‚ùå"
        success_count += 1 if classification == expected else 0

        print(f"{status} '{stimulus}' ‚Üí {classification} (attendu: {expected})")
        print(f"    Entropy: {result['entropy']:.3f}, Confidence: {result['confidence']:.3f} ({result['confidence_level']})")
        if classification != expected:
            print(f"    Raison: {result['classification_reason']}")

    print(f"\nüìä R√©sultat: {success_count}/{len(test_cases)} tests r√©ussis ({success_count/len(test_cases)*100:.1f}%)")

    # Test cache performance
    print("\nüöÄ TEST CACHE PERFORMANCE")
    import time

    # Premier appel (sans cache)
    start = time.time()
    for _ in range(100):
        classifier.classify("pog")
    no_cache_time = time.time() - start

    # Deuxi√®me appel (avec cache)
    start = time.time()
    for _ in range(100):
        classifier.classify("pog")
    cache_time = time.time() - start

    speedup = no_cache_time / cache_time if cache_time > 0 else float('inf')
    print(f"Sans cache: {no_cache_time*1000:.2f}ms, Avec cache: {cache_time*1000:.2f}ms")
    print(f"Speedup: {speedup:.1f}x")

    return success_count == len(test_cases)


def demo_unified_quantum_classifier():
    """üß™ D√©monstration UnifiedQuantumClassifier"""
    print("üåå DEMONSTRATION UNIFIED QUANTUM CLASSIFIER V3.1")
    print("=" * 55)

    # Configuration de test
    config = {
        "quantum_classifier": {
            "high_confidence_threshold": 0.75,
            "entropy_fallback_threshold": 1.5,
            "minimum_probability": 0.1,
            "fallback_class": "gen_short"
        }
    }

    classifier = UnifiedQuantumClassifier(config)

    # üß™ Cas de test quantiques
    test_cases = [
        ("ping", "Haute confiance"),
        ("bonsoir serda_bot", "Mention casual"),
        ("explique moi la physique quantique", "Complexit√© √©lev√©e"),
        ("c'est quoi Python ?", "Lookup factuel"),
        ("salut aide comment pourquoi", "Incertitude √©lev√©e ‚Üí Fallback"),
        ("@serda_bot raconte moi une histoire", "Mention + g√©n√©ration"),
    ]

    for stimulus, description in test_cases:
        print(f"\nüß™ {description}: '{stimulus}'")

        result = classifier.classify(stimulus)

        print(f"   üéØ Classe: {result['class']}")
        print(f"   üìä Confiance: {result['confidence_level']} ({result['confidence']:.3f})")
        print(f"   üßÆ Entropie: {result['entropy']:.3f}")
        print(f"   üîÑ Fallback: {'OUI' if result['should_fallback'] else 'NON'}")
        print(f"   üåå Distribution: {result['distribution_type']}")

        # Top 2 probabilit√©s
        sorted_probs = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:2]
        print(f"   üìà Top classes: {sorted_probs[0][0]}({sorted_probs[0][1]:.2f}), {sorted_probs[1][0]}({sorted_probs[1][1]:.2f})")


if __name__ == "__main__":
    # Tests autonomes
    test_unified_quantum_classifier()
    print("\n" + "=" * 55 + "\n")
    demo_unified_quantum_classifier()
