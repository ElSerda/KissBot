"""
Intelligence Core - Logique m√©tier pure (facile √† tester)
S√©paration business logic / framework TwitchIO
"""

from rapidfuzz import fuzz


def find_game_in_cache(user_query: str, game_cache, threshold: float = 80.0) -> dict | None:
    """
    Trouve un jeu dans le cache en utilisant fuzzy matching.

    Args:
        user_query: Texte de l'utilisateur (ex: "brottato", "parle moi de brotato")
        game_cache: Instance GameCache
        threshold: Seuil de similarit√© (80% par d√©faut)

    Returns:
        dict: Donn√©es du jeu si trouv√©, None sinon
    """
    if not game_cache or not user_query:
        return None

    user_query_lower = user_query.lower()
    best_match = None
    best_score = 0.0

    # Adapt√© pour GameCache quantum
    for cache_key, quantum_state in game_cache.quantum_states.items():
        superpositions = quantum_state.get("superpositions", [])
        
        # Prendre la premi√®re superposition (meilleur score)
        if not superpositions:
            continue
            
        game_data = superpositions[0].get("game", {})
        game_name = game_data.get("name", "")

        if not game_name:
            continue

        # Utiliser token_set_ratio pour g√©rer ordres de mots et fautes
        # Ex: "brottato game" match "Brotato" avec score √©lev√©
        score = float(fuzz.token_set_ratio(game_name.lower(), user_query_lower))

        if score >= threshold and score > best_score:
            best_score = score
            best_match = game_data

    return best_match


async def process_llm_request(
    llm_handler,
    prompt: str,
    context: str,
    user_name: str,
    game_cache=None,
    pre_optimized: bool = False,  # Nouveau param: prompt d√©j√† optimis√© ?
    stimulus_class: str = "gen_short",  # Classe de stimulus pour prompts pr√©-optimis√©s
    channel_id: str = ""  # ID du channel pour personnalit√© custom
) -> str | None:
    """
    Traite une requ√™te LLM - Logique m√©tier pure.

    Args:
        llm_handler: Instance LLMHandler
        prompt: Question/message de l'utilisateur
        context: Context ("ask" ou "mention")
        user_name: Nom de l'utilisateur
        game_cache: Cache des jeux (optionnel pour enrichissement contexte)
        pre_optimized: Si True, le prompt est d√©j√† au format optimal (skip wrapping)
        stimulus_class: Classe de stimulus si pre_optimized=True ("ping"/"gen_short"/"gen_long")

    Returns:
        str: R√©ponse format√©e (tronqu√©e si n√©cessaire) ou None si erreur
    """
    import logging
    logger = logging.getLogger(__name__)

    # ‚úÖ VALIDATION D√âFENSIVE (fork-safe, language-agnostic)
    if not llm_handler:
        logger.error("‚ùå llm_handler est None")
        return None

    # Normalisation pre_optimized: garantir bool
    if not isinstance(pre_optimized, bool):
        logger.warning(f"‚ö†Ô∏è pre_optimized type invalide ({type(pre_optimized)}), conversion bool")
        pre_optimized = bool(pre_optimized)

    # Validation stimulus_class: whitelist stricte
    valid_classes = ["ping", "gen_short", "gen_long"]
    if stimulus_class not in valid_classes:
        logger.warning(f"‚ö†Ô∏è stimulus_class invalide '{stimulus_class}', fallback 'gen_short'")
        stimulus_class = "gen_short"

    try:
        # ‚úÖ PROMPT PR√â-OPTIMIS√â : Appel direct synapse (bypass wrapping)
        if pre_optimized:
            logger.info("üéØ Prompt pr√©-optimis√© d√©tect√© ‚Üí Appel direct synapse")
            if hasattr(llm_handler, 'local_synapse'):
                response = await llm_handler.local_synapse.fire(
                    stimulus=prompt,
                    context="direct",  # Skip _optimize_signal_for_local()
                    stimulus_class=stimulus_class,
                    correlation_id=f"preopt_{context}"
                )
            else:
                # Fallback: utiliser process_stimulus
                logger.warning("‚ö†Ô∏è local_synapse non disponible, fallback process_stimulus")
                response = await llm_handler.process_stimulus(
                    stimulus=prompt,
                    context=context
                )
        else:
            # üéÆ PROMPT BRUT : Pipeline normal avec enrichissement + wrapping
            enriched_prompt = (
                await enrich_prompt_with_game_context(prompt, game_cache) if game_cache else prompt
            )

            response = await llm_handler.process_stimulus(
                stimulus=enriched_prompt, context=context, channel_id=channel_id
            )

        if not response:
            return None

        # Truncate si trop long (Twitch: 500 chars - [ASK] 6 chars = 494 max)
        # S√©curit√© IRC √† 450 chars (marge 44 chars pour edge cases)
        if len(response) > 450:
            response = response[:447] + "..."

        return response

    except Exception:
        # Log l'erreur mais ne crash pas
        return None


async def enrich_prompt_with_game_context(prompt: str, game_cache) -> str:
    """
    SMART CONTEXT 2.0: Auto-enrichissement r√©volutionnaire !
    NOUVELLE LOGIQUE: Si jeu d√©tect√© dans prompt ‚Üí enrichir automatiquement
    Plus besoin de keywords - d√©tection bas√©e sur contenu r√©el !

    Args:
        prompt: Question originale de l'user
        game_cache: Instance GameCache

    Returns:
        str: Prompt enrichi ou prompt original si aucun jeu d√©tect√©
    """
    if not game_cache or not prompt:
        return prompt

    # üîç DEBUG: Log Smart Context 2.0
    import logging

    logger = logging.getLogger(__name__)
    logger.info(f"üß† Smart Context 2.0: Analyse prompt '{prompt[:50]}...' pour jeux disponibles")

    # üöÄ R√âVOLUTION: Chercher jeu d'abord, keywords apr√®s !
    game_info = find_game_in_cache(prompt, game_cache)

    if game_info:
        logger.info(
            f"üéÆ Smart Context 2.0 AUTO-ACTIV√â: Jeu '{game_info.get('name')}' d√©tect√© ! Keywords plus n√©cessaires !"
        )
    else:
        logger.info(
            f"‚ùå Smart Context 2.0: Aucun jeu d√©tect√© dans '{prompt[:30]}...', prompt original conserv√©"
        )
        return prompt

    # üéÆ Si jeu trouv√© ‚Üí ENRICHIR AUTOMATIQUEMENT (peu importe l'intention)
    if game_info:
        # üîç DEBUG: Log Smart Context activation
        logger.info(
            f"üéÆ Smart Context ACTIV√â: Jeu '{game_info.get('name')}' d√©tect√© pour prompt '{prompt[:50]}...'"
        )

        # üéØ Extraire les donn√©es du jeu pour enrichissement
        name = game_info.get("name", "")
        year = game_info.get("year", "")
        
        # Plateformes (max 3)
        platforms = (
            ", ".join(game_info.get("platforms", [])[:3]) if game_info.get("platforms") else ""
        )
        
        # Genres avec traduction fran√ßaise (max 3)
        genres_text = ""
        genres_fr = []  # üêõ FIX: Initialisation avant le if pour √©viter UnboundLocalError
        if game_info.get("genres"):
            genres = game_info.get("genres", [])[:3]
            genre_translations = {
                "Action": "Action",
                "RPG": "RPG",
                "Indie": "Ind√©pendant",
                "Casual": "D√©contract√©",
                "Adventure": "Aventure",
                "Simulation": "Simulation",
                "Strategy": "Strat√©gie",
                "Shooter": "Tir",
                "Racing": "Course",
            }
            genres_fr = [genre_translations.get(g, g) for g in genres if g and isinstance(g, str)]
            genres_text = ", ".join(genres_fr) if genres_fr else ""
        
        # Description (priorit√©: summary > description_raw > description)
        description = ""
        if game_info.get("summary"):
            description = game_info.get("summary", "")[:150]
        elif game_info.get("description_raw"):
            description = game_info.get("description_raw", "")[:150]
        elif game_info.get("description"):
            description = game_info.get("description", "")[:150]

        # üéØ STRAT√âGIE ADAPTATIVE : Prompt diff√©rent selon richesse des donn√©es
        has_rich_data = bool(genres_text and description)  # Genres ET description = donn√©es riches
        
        if has_rich_data:
            # üíé Donn√©es compl√®tes ‚Üí Prompt DIRECTIF (utilise tout)
            directif_prompt = f"""[CONTEXTE STRICT :
- Nom : {name}
- Ann√©e : {year}
- Plateformes : {platforms}
- Genres : {genres_text}
- Description : {description}
OBLIGATOIRE : Utilise TOUTES ces infos dans ta r√©ponse.]

Question : {prompt}"""
        else:
            # üì¶ Donn√©es partielles ‚Üí Prompt SUGGESTIF (indique !gameinfo)
            context_parts = [f"Nom : {name}"]
            if year:
                context_parts.append(f"Ann√©e : {year}")
            if platforms:
                context_parts.append(f"Plateformes : {platforms}")
            if genres_text:
                context_parts.append(f"Genres : {genres_text}")
            if description:
                context_parts.append(f"Description : {description}")
            
            context_str = "\n- ".join(context_parts)
            directif_prompt = f"""[CONTEXTE PARTIEL :
- {context_str}

Note : Donn√©es limit√©es disponibles. Si la question n√©cessite plus d'infos (gameplay, graphismes, m√©caniques), sugg√®re d'utiliser !gameinfo {name} pour enrichir le cache.]

Question : {prompt}"""

        return directif_prompt

    return prompt


def extract_question_from_command(message_content: str) -> str | None:
    """
    Extrait la question d'une commande !ask.

    Args:
        message_content: Contenu complet du message "!ask <question>"

    Returns:
        str: Question extraite ou None si invalide
    """
    parts = message_content.strip().split(maxsplit=1)
    if len(parts) < 2:
        return None
    return parts[1].strip()


def extract_mention_message(message_content: str, bot_name: str) -> str | None:
    """
    Extrait le message d'une mention @bot ou bot_name.

    Args:
        message_content: Contenu complet du message contenant bot_name
        bot_name: Nom du bot (case-insensitive)

    Returns:
        str: Message extrait (avec bot_name retir√©) ou None si bot_name absent
        
    R√®gles de d√©tection:
    - bot_name ou @bot_name N'IMPORTE O√ô dans le message = mention
    - Le syst√®me neural (UCB bandit) g√®re ensuite le contexte et la pertinence
    """
    import re
    
    content_lower = message_content.lower()
    bot_lower = bot_name.lower()
    
    # V√©rifier si bot_name ou @bot_name est pr√©sent (case-insensitive)
    if f"@{bot_lower}" not in content_lower and bot_lower not in content_lower:
        return None
    
    # Pattern: @bot_name ou bot_name (word boundary pour √©viter "leserda_bot")
    pattern = rf"@?{re.escape(bot_name)}\b"
    message = re.sub(pattern, "", message_content, count=1, flags=re.IGNORECASE)
    message = message.strip()
    
    # Retourner le message m√™me si vide (mention sans texte = ping)
    return message if message else "ping"
