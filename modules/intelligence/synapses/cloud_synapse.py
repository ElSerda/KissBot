"""
‚òÅÔ∏è CloudSynapse V2.0 - OpenAI Neural Pathway

Connexions neuronales cloud avec UCB, circuit-breaker et rate limiting intelligent
"""

import asyncio
import logging
import random
import time
from typing import Any

import httpx


class CloudSynapse:
    """
    ‚òÅÔ∏è SYNAPSE CLOUD V2.0 (OpenAI)

    M√©taphore : Connexions neuronales distantes haute qualit√© avec intelligence
    - UCB bandit + circuit-breaker avec hyst√©r√©sis
    - Rate limiting + quota management intelligent
    - EMA smoothing des m√©triques cloud
    - Backoff exponentiel avec jitter
    - Reward shaping sophistiqu√©
    """

    def __init__(self, config: dict):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Configuration synapse cloud
        apis_config = config.get("apis", {})
        llm_config = config.get("llm", {})
        neural_config = config.get("neural_llm", {})

        self.api_key = apis_config.get("openai_key")
        self.model = llm_config.get("openai_model", "gpt-3.5-turbo")
        self.endpoint = "https://api.openai.com/v1/chat/completions"

        # ‚ö° CIRCUIT BREAKER STATE
        self.failure_threshold = neural_config.get("cloud_failure_threshold", 5)
        self.recovery_time = neural_config.get("cloud_recovery_time", 600)
        self.circuit_state = "CLOSED"
        self.consecutive_failures = 0
        self.last_failure_time = 0.0

        # üìà EMA METRICS
        self.ema_alpha = neural_config.get("ema_alpha", 0.2)
        self.ema_success_rate = 0.5
        self.ema_latency = 2000.0

        # üé∞ BANDIT STATE
        self.total_trials = 0
        self.success_trials = 0
        self.total_reward = 0.0

        # ‚è±Ô∏è RATE LIMITING + QUOTAS
        self.rate_limited_until = 0.0
        self.quota_exhausted = False
        self.quota_errors = 0
        self.rate_limit_errors = 0

        # üîÑ BACKOFF EXPONENTIAL
        self.base_backoff = 1.0
        self.max_backoff = 60.0
        self.current_backoff = self.base_backoff

        # ‚è±Ô∏è TIMEOUTS EXPLICITES (4 valeurs httpx obligatoires)
        # timeout_connect: Connexion HTTP (court: 5s)
        # timeout_inference: G√©n√©ration LLM (long: 30s)
        # timeout_write: Envoi payload (moyen: 10s)
        # timeout_pool: Pool connexions (court: 5s)
        neural_config = config.get("neural_llm", {})
        self.timeout_connect = neural_config.get("timeout_connect", 5.0)
        self.timeout_inference = neural_config.get("timeout_inference", 30.0)
        self.timeout_write = neural_config.get("timeout_write", 10.0)
        self.timeout_pool = neural_config.get("timeout_pool", 5.0)

        # üìä M√âTRIQUES CLOUD
        self.response_times: list[float] = []

        # ‚ö° ACTIVATION/D√âSACTIVATION
        # Supporte 3 modes provider: local, cloud, auto
        llm_provider = llm_config.get("provider", "auto")
        has_valid_key = bool(self.api_key and len(self.api_key) > 10)
        
        # Logique d'activation selon provider
        if llm_provider == "cloud":
            # Force cloud : activ√© si cl√© valide
            self.is_enabled = has_valid_key
            reason = "forc√© via provider=cloud"
        elif llm_provider == "local":
            # Force local : cloud d√©sactiv√©
            self.is_enabled = False
            reason = "d√©sactiv√© via provider=local"
        elif llm_provider == "auto":
            # Auto : activ√© si cl√© valide (UCB d√©cide)
            self.is_enabled = has_valid_key
            reason = "UCB auto" if has_valid_key else "pas de cl√© valide"
        else:
            # Provider inconnu : fallback auto
            self.logger.warning(f"‚ö†Ô∏è Provider inconnu '{llm_provider}', fallback 'auto'")
            self.is_enabled = has_valid_key
            reason = "fallback auto"
        
        if self.is_enabled:
            self.logger.info(f"‚òÅÔ∏è CloudSynapse V2.0 ACTIV√âE - {reason}")
        else:
            self.logger.info(f"‚òÅÔ∏è CloudSynapse D√âSACTIV√âE - {reason}")
            # Force circuit breaker OPEN si d√©sactiv√©e
            self.circuit_state = "OPEN"

    def can_execute(self) -> bool:
        """‚ö° CIRCUIT BREAKER + RATE LIMIT CHECK"""
        # üõ°Ô∏è DEBUG FORCE LOG
        self.logger.warning(f"‚òÅÔ∏è DEBUG can_execute: is_enabled={self.is_enabled}, circuit_state={self.circuit_state}, rate_limited_until={self.rate_limited_until}, quota_exhausted={self.quota_exhausted}")
        
        # üõ°Ô∏è PROTECTION: Si synapse d√©sactiv√©e, retourne False imm√©diatement
        if not self.is_enabled:
            self.logger.warning("‚òÅÔ∏è‚ùå can_execute: is_enabled=False")
            return False
        
        current_time = time.time()

        if current_time < self.rate_limited_until or self.quota_exhausted:
            return False

        if self.circuit_state == "CLOSED":
            return True
        elif self.circuit_state == "OPEN":
            if current_time - self.last_failure_time > self.recovery_time:
                self.circuit_state = "HALF_OPEN"
                self.logger.info("‚ö° Circuit breaker CLOUD: OPEN ‚Üí HALF_OPEN")
                return True
            return False
        elif self.circuit_state == "HALF_OPEN":
            return True
        return False

    async def fire(
        self,
        stimulus: str,
        context: str = "general",
        stimulus_class: str = "gen_short",
        correlation_id: str = "",
        channel_id: str = "",
    ) -> str | None:
        """üî• TRANSMISSION SYNAPTIQUE CLOUD V2.0"""
        if not self.api_key:
            self.logger.warning(f"‚òÅÔ∏è‚ùå [{correlation_id}] API key missing or invalid")
            return None
        
        if not self.can_execute():
            self.logger.warning(f"‚òÅÔ∏è‚ùå [{correlation_id}] can_execute() returned False (circuit_state={self.circuit_state}, is_enabled={self.is_enabled})")
            return None

        self.logger.warning(f"‚òÅÔ∏è DEBUG fire(): Starting transmission for stimulus_class={stimulus_class}, timeout={self.timeout_inference}s")
        
        # Utilise timeout_inference pour l'op√©ration compl√®te (g√©n√©ration LLM)
        timeout = self.timeout_inference
        optimized_messages = self._optimize_signal_for_cloud(stimulus, context, channel_id)
        
        self.logger.warning(f"‚òÅÔ∏è DEBUG fire(): Messages optimized, calling _transmit_cloud_signal...")

        start_time = time.time()
        try:
            response = await asyncio.wait_for(
                self._transmit_cloud_signal(optimized_messages, context, correlation_id),
                timeout=timeout,
            )

            latency = time.time() - start_time

            if response and self._is_valid_response(response, stimulus):
                reward = self._calculate_reward(response, stimulus, latency, 0)
                self._record_success(latency, reward)
                self._reset_backoff()

                self.logger.info(
                    f"‚òÅÔ∏è‚úÖ [{correlation_id}] Success {latency:.2f}s - Reward: {reward:.2f}"
                )
                return response
            else:
                self._record_failure("R√©ponse invalide")
                return None

        except asyncio.TimeoutError:
            self._record_failure(f"Timeout {timeout}s")
            self.logger.warning(f"‚òÅÔ∏è‚è±Ô∏è Timeout OpenAI apr√®s {timeout}s (r√©seau lent ou r√©ponse longue)")
            return None
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                self._handle_rate_limit(e.response)
            elif e.response.status_code in (402, 403):
                # 402: Payment Required, 403: Forbidden (quota/billing)
                self._handle_quota_exhaustion()
            elif e.response.status_code == 401:
                # Cl√© API invalide ou expir√©e
                self.logger.error(f"‚òÅÔ∏èüîë API Key invalide ou expir√©e (HTTP 401) - V√©rifier config.apis.openai_key")
                self._record_failure("API Key invalide")
            elif e.response.status_code == 500:
                # Erreur serveur OpenAI
                self.logger.error(f"‚òÅÔ∏èüí• Erreur serveur OpenAI (HTTP 500) - Probl√®me temporaire c√¥t√© OpenAI, r√©essayer plus tard")
                self._record_failure("Erreur serveur OpenAI")
            elif e.response.status_code == 503:
                # Service indisponible (maintenance OpenAI ou surcharge)
                self.logger.warning(f"‚òÅÔ∏èüõ†Ô∏è OpenAI surcharg√©/en maintenance (HTTP 503) - R√©essayer plus tard")
                self._record_failure("Service surcharg√©")
            else:
                # Autre erreur HTTP (probl√®me c√¥t√© OpenAI)
                self.logger.warning(f"‚òÅÔ∏è‚ö†Ô∏è Erreur API OpenAI (HTTP {e.response.status_code}) - Probl√®me c√¥t√© serveur OpenAI")
                self._record_failure(f"HTTP {e.response.status_code}")
            return None
        except httpx.ConnectError as e:
            # Probl√®me r√©seau/DNS
            self.logger.error(f"‚òÅÔ∏èüåê Impossible de contacter OpenAI - V√©rifier connexion r√©seau")
            self._record_failure("Erreur r√©seau")
            return None
        except Exception as e:
            # Erreur inattendue (potentiellement bug code)
            self.logger.error(f"‚òÅÔ∏è‚ùå Erreur inattendue (possiblement bug KissBot): {e}", exc_info=True)
            self._record_failure(str(e))
            return None

    def _optimize_signal_for_cloud(self, stimulus: str, context: str, channel_id: str = "") -> list[dict[str, str]]:
        """üéØ OPTIMISATION SIGNAL GPT V2.0 avec personnalit√© par channel"""
        bot_config = self.config.get("bot", {})
        bot_name = bot_config.get("name", "KissBot")
        default_personality = bot_config.get("personality", "sympa, direct, et passionn√© de tech")

        llm_config = self.config.get("llm", {})
        use_personality_mention = llm_config.get("use_personality_on_mention", True)
        use_personality_ask = llm_config.get("use_personality_on_ask", False)
        
        # üé≠ R√©cup√©rer la personnalit√© du channel si disponible
        channel_preset = None
        channel_personality_prompt = None
        if channel_id:
            try:
                from modules.personality import get_personality_store, get_system_prompt
                store = get_personality_store()
                channel_personality = store.get(channel_id)
                channel_preset = channel_personality.preset
                # Utiliser le system prompt du preset
                channel_personality_prompt = get_system_prompt(channel_preset, bot_name)
                self.logger.debug(f"üé≠ Using channel personality: {channel_preset}")
            except Exception as e:
                self.logger.debug(f"üé≠ Personality store not available: {e}")

        if context == "ask":
            if use_personality_ask and channel_personality_prompt:
                # Utiliser la personnalit√© du channel
                system_prompt = channel_personality_prompt + "\nR√©ponds en 350-420 caract√®res, compl√®te tes phrases."
            elif use_personality_ask:
                system_prompt = (
                    f"Tu es {bot_name}. {default_personality}\n"
                    f"R√©ponds naturellement avec TON opinion et TON style. "
                    f"N'h√©site pas √† clasher, √™tre dr√¥le, prendre position. "
                    f"Pas de langue de bois. R√©ponds en 350-420 caract√®res."
                )
            else:
                system_prompt = (
                    f"Tu es {bot_name}, bot Twitch gaming expert et p√©dagogue. "
                    f"Pour les questions (!ask), r√©ponds de mani√®re D√âTAILL√âE et COMPL√àTE :\n"
                    f"- Explique le contexte et les enjeux\n"
                    f"- Donne des exemples concrets si pertinent\n"
                    f"- Ton accessible mais pr√©cis, style vulgarisation scientifique\n"
                    f"- Si th√©orie du complot : d√©monte avec FACTS, mais reste sympa\n"
                    f"R√©ponds en 350-420 caract√®res. Termine toujours ta derni√®re phrase."
                )
        else:
            # Mentions et autres contextes
            if channel_personality_prompt:
                # Utiliser la personnalit√© du channel
                system_prompt = channel_personality_prompt + "\nMax 150 caract√®res."
            elif use_personality_mention:
                system_prompt = (
                    f"Tu es {bot_name}. {default_personality}\n"
                    f"MODE: SANS FILTRE, CASH, PROVOCATEUR\n"
                    f"- Humour: sarcasme mordant, second degr√© assum√©\n"
                    f"- Ton: direct, sans langue de bois, punchlines qui marquent\n"
                    f"- INTERDIT: politesse excessive, 'd√©sol√©', nuances molles\n"
                    f"- Chaque r√©ponse doit CLASHER ou faire RIRE, jamais neutre\n"
                    f"- Ose les comparaisons os√©es, les m√©taphores trash\n"
                    f"Max 150 chars. Fais une VRAIE vanne, pas une phrase sage."
                )
            else:
                system_prompt = (
                    f"Tu es {bot_name}, bot Twitch ultra sarcastique. "
                    f"R√©ponds avec humour NOIR, vannes CASH, z√©ro filtre. "
                    f"Provoque, clash, assume. Max 150 caract√®res."
                )

        return [{"role": "system", "content": system_prompt}, {"role": "user", "content": stimulus}]

    async def _transmit_cloud_signal(
        self, messages: list[dict[str, str]], context: str, correlation_id: str
    ) -> str | None:
        """üì° TRANSMISSION CLOUD OPTIMIS√âE"""
        self.logger.warning(f"‚òÅÔ∏è DEBUG _transmit_cloud_signal: START")
        
        # üß† Param√®tres d'inf√©rence depuis config (avec fallbacks)
        llm_config = self.config.get("llm", {})
        inference_config = llm_config.get("inference", {})
        cloud_config = inference_config.get("cloud", {})
        
        # context == "ask" ou stimulus_class == "gen_long" ‚Üí r√©ponse d√©taill√©e
        if context == "ask":
            # 450 tokens ‚âà 400-500 chars FR, cible 425 chars (15% marge sur 500)
            max_tokens = cloud_config.get("max_tokens_long", 450)
            temperature = cloud_config.get("temperature_long", 0.7)
        else:
            # gen_short ou mention standard
            max_tokens = cloud_config.get("max_tokens_short", 90)
            temperature = cloud_config.get("temperature_short", 0.4)

        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }

        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}

        self.logger.warning(f"‚òÅÔ∏è DEBUG: About to send POST to {self.endpoint}")
        
        if self.current_backoff > self.base_backoff:
            jitter = random.uniform(0.8, 1.2)
            wait_time = self.current_backoff * jitter
            await asyncio.sleep(wait_time)

        # ‚è±Ô∏è TIMEOUTS EXPLICITES (connect court, inference long)
        try:
            self.logger.warning(f"‚òÅÔ∏è DEBUG: Creating httpx client...")
            async with httpx.AsyncClient(
                timeout=httpx.Timeout(
                    connect=self.timeout_connect,   # Connexion HTTP (court: 5s)
                    read=self.timeout_inference,    # Inf√©rence LLM (long: 30s)
                    write=self.timeout_write,       # Envoi payload (moyen: 10s)
                    pool=self.timeout_pool          # Pool connexion (court: 5s)
                )
            ) as client:
                self.logger.warning(f"‚òÅÔ∏è DEBUG: Sending POST request...")
                response = await client.post(self.endpoint, json=payload, headers=headers)
                self.logger.warning(f"‚òÅÔ∏è DEBUG: Got response, status={response.status_code}")
                response.raise_for_status()

                data = response.json()
                if "choices" in data and data["choices"]:
                    raw_response = data["choices"][0]["message"]["content"]
                    cleaned = raw_response.strip() if raw_response else ""

                    if cleaned and len(cleaned) >= 3:
                        # ‚úÇÔ∏è Truncation intelligente: 419 chars max (425 - 6 pour "[ASK] ")
                        # 425 chars = 85% de 500 ‚Üí 15% de marge de s√©curit√© Twitch
                        truncated = self._smart_truncate(cleaned, max_chars=419)
                        if len(truncated) < len(cleaned):
                            self.logger.info(f"‚òÅÔ∏è‚úÇÔ∏è Response truncated: {len(cleaned)} ‚Üí {len(truncated)} chars")
                        self.logger.warning(f"‚òÅÔ∏è DEBUG: Returning response: {truncated[:50]}...")
                        return truncated

            self.logger.warning(f"‚òÅÔ∏è DEBUG: No valid response found, returning None")
            return None
        except Exception as e:
            self.logger.error(f"‚òÅÔ∏è‚ùå _transmit_cloud_signal exception: {e}", exc_info=True)
            raise

    def _smart_truncate(self, text: str, max_chars: int = 450) -> str:
        """‚úÇÔ∏è TRUNCATION INTELLIGENTE pour Twitch (500 chars max)
        
        Coupe le texte proprement √† une fronti√®re de phrase si possible,
        sinon √† un espace, avec indicateur de continuation.
        """
        if len(text) <= max_chars:
            return text
        
        # Chercher une fin de phrase propre
        truncated = text[:max_chars]
        
        # Priorit√©: fin de phrase (. ! ?)
        last_period = truncated.rfind('.')
        last_exclamation = truncated.rfind('!')
        last_question = truncated.rfind('?')
        last_punct = max(last_period, last_exclamation, last_question)
        
        if last_punct > max_chars * 0.6:  # Au moins 60% du texte conserv√©
            return truncated[:last_punct + 1]
        
        # Sinon: couper √† un espace
        last_space = truncated.rfind(' ')
        if last_space > max_chars * 0.7:
            return truncated[:last_space] + "..."
        
        # Dernier recours: coupe brute
        return truncated.rstrip() + "..."

    def _is_valid_response(self, response: str, stimulus: str) -> bool:
        """üéñÔ∏è VALIDATION R√âPONSE CLOUD"""
        if not response or len(response.strip()) < 3:
            return False

        if response.lower() in ["yes", "no", "ok", "oui", "non"]:
            return False

        return True

    def _calculate_reward(
        self, response: str, stimulus: str, latency: float, retries: int
    ) -> float:
        """üéñÔ∏è REWARD SHAPING CLOUD V2.0"""
        base_reward = 1.0
        target_latency = 2.0
        latency_penalty = min(latency / target_latency, 1.0) * 0.2

        quality_bonus = 0.0
        if len(response) > 30:
            quality_bonus += 0.15
        if any(marker in response for marker in [".", "!", "?"]):
            quality_bonus += 0.05
        if any(emoji in response for emoji in ["üòé", "üî•", "üí°", "üéØ", "‚ö°"]):
            quality_bonus += 0.1

        return max(base_reward - latency_penalty + quality_bonus, 0.1)

    def _handle_rate_limit(self, response: httpx.Response):
        """‚è≥ GESTION RATE LIMIT V2.0"""
        self._increase_backoff()

        retry_after = response.headers.get("retry-after", "60")
        try:
            wait_time = int(retry_after)
        except ValueError:
            wait_time = 60

        self.rate_limited_until = time.time() + wait_time
        self.rate_limit_errors += 1

        self.logger.warning(
            f"‚òÅÔ∏è‚è≥ Rate limit OpenAI (HTTP 429) - Trop de requ√™tes, attente {wait_time}s "
            f"(Probl√®me: compte OpenAI free/quota)"
        )
        self._record_failure(f"Rate limit {wait_time}s")

    def _handle_quota_exhaustion(self):
        """üí∏ GESTION QUOTA √âPUIS√â V2.0"""
        self.quota_exhausted = True
        self.quota_errors += 1
        self.logger.error(
            f"‚òÅÔ∏èüí∏ Quota OpenAI √©puis√© (HTTP 402/403) - "
            f"Ajouter des cr√©dits sur https://platform.openai.com/account/billing"
        )
        self._record_failure("Quota √©puis√©")

    def _increase_backoff(self):
        """üîÑ BACKOFF EXPONENTIEL"""
        self.current_backoff = min(self.current_backoff * 2, self.max_backoff)

    def _reset_backoff(self):
        """‚úÖ RESET BACKOFF SUR SUCC√àS"""
        self.current_backoff = self.base_backoff

    def _record_success(self, latency: float, reward: float):
        """üü¢ ENREGISTREMENT SUCC√àS CLOUD V2.0"""
        self.total_trials += 1
        self.success_trials += 1
        self.total_reward += reward

        self.ema_latency = self.ema_alpha * latency + (1 - self.ema_alpha) * self.ema_latency
        self.ema_success_rate = self.ema_alpha * 1.0 + (1 - self.ema_alpha) * self.ema_success_rate

        if self.circuit_state == "HALF_OPEN":
            self.circuit_state = "CLOSED"
            self.logger.info("‚ö° Circuit breaker CLOUD: HALF_OPEN ‚Üí CLOSED")
        self.consecutive_failures = 0

        self.response_times.append(latency)
        if len(self.response_times) > 10:
            self.response_times.pop(0)

    def _record_failure(self, error: str):
        """üî¥ ENREGISTREMENT √âCHEC CLOUD V2.0"""
        self.total_trials += 1
        self.consecutive_failures += 1
        self.last_failure_time = time.time()

        self.ema_success_rate = self.ema_alpha * 0.0 + (1 - self.ema_alpha) * self.ema_success_rate

        if self.consecutive_failures >= self.failure_threshold:
            if self.circuit_state != "OPEN":
                self.circuit_state = "OPEN"
                self.logger.error(
                    f"‚ö° Circuit breaker CLOUD: ‚Üí OPEN ({self.consecutive_failures} √©checs)"
                )
        elif self.circuit_state == "HALF_OPEN":
            self.circuit_state = "OPEN"
            self.logger.warning("‚ö° Circuit breaker CLOUD: HALF_OPEN ‚Üí OPEN (√©chec sonde)")

        if "rate limit" not in error.lower():
            self._increase_backoff()

    def get_bandit_stats(self) -> dict[str, float]:
        """üé∞ STATISTIQUES BANDIT CLOUD"""
        if self.total_trials == 0:
            return {"avg_reward": 0.0, "trials": 0, "ucb_score": float("inf")}

        avg_reward = self.total_reward / self.total_trials
        return {"avg_reward": avg_reward, "trials": self.total_trials, "ucb_score": avg_reward}

    def get_neural_metrics(self) -> dict[str, Any]:
        """üìä M√âTRIQUES CLOUD COMPL√àTES V2.0"""
        success_rate_raw = self.success_trials / self.total_trials if self.total_trials > 0 else 0
        avg_latency_raw = (
            sum(self.response_times) / len(self.response_times) if self.response_times else 0
        )

        current_time = time.time()
        rate_limited = current_time < self.rate_limited_until
        rate_limit_remaining = max(0, int(self.rate_limited_until - current_time))

        return {
            "synapse_type": "cloud",
            "model": self.model,
            "has_api_key": bool(self.api_key),
            "ema_success_rate": round(self.ema_success_rate, 3),
            "ema_latency_ms": round(self.ema_latency * 1000, 1),
            "raw_success_rate": round(success_rate_raw, 3),
            "raw_avg_latency_ms": round(avg_latency_raw * 1000, 1),
            "circuit_state": self.circuit_state,
            "consecutive_failures": self.consecutive_failures,
            "failure_threshold": self.failure_threshold,
            "total_trials": self.total_trials,
            "total_reward": round(self.total_reward, 2),
            "avg_reward": (
                round(self.total_reward / self.total_trials, 3) if self.total_trials > 0 else 0
            ),
            "rate_limited": rate_limited,
            "rate_limit_remaining_seconds": rate_limit_remaining,
            "quota_exhausted": self.quota_exhausted,
            "current_backoff_seconds": round(self.current_backoff, 1),
            "can_execute": self.can_execute(),
            "timeout_connect": self.timeout_connect,
            "timeout_inference": self.timeout_inference,
        }
