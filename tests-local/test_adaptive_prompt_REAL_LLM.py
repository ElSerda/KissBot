"""
ğŸ”¥ TEST RÃ‰EL avec le modÃ¨le LLM local (LM Studio)
On va voir comment le LLM rÃ©agit aux prompts PARTIEL vs STRICT
"""
import sys
import os
import asyncio
import yaml

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backends.game_cache import GameCache
from intelligence.core import process_llm_request
from intelligence.neural_pathway_manager import NeuralPathwayManager


async def test_real_llm_adaptive_prompts():
    """Test le systÃ¨me d'enrichissement adaptatif avec le VRAI LLM local."""
    
    print("\n" + "="*80)
    print("ğŸ”¥ TEST RÃ‰EL : LLM local avec prompts adaptatifs")
    print("="*80)
    
    # Load config
    config_path = "config/config.yaml"
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Setup LLM handler (REAL Neural Pathway Manager)
    print("\nğŸš€ Initialisation du Neural Pathway Manager (LM Studio)...")
    llm_handler = NeuralPathwayManager(config)
    
    # Setup cache
    import tempfile
    temp_dir = tempfile.mkdtemp()
    cache_file = os.path.join(temp_dir, "test_games.json")
    cache = GameCache(config={"cache": {"duration_hours": 1}}, cache_file=cache_file)
    
    print(f"âœ… LLM connectÃ© : {config['llm']['model_endpoint']}")
    print(f"âœ… ModÃ¨le : {config['llm']['model_name']}")
    
    # ========================================
    # TEST 1 : DonnÃ©es PAUVRES (juste nom)
    # ========================================
    print("\n" + "="*80)
    print("ğŸ“¦ TEST 1 : DonnÃ©es PAUVRES (juste nom 'Brotato')")
    print("-" * 80)
    
    cache.set("brotato", {"name": "Brotato"})
    
    print("\nğŸ’¬ Question : 'C'est quoi le gameplay de Brotato?'")
    print("ğŸ“ Prompt envoyÃ© : ORIGINAL (pas assez de donnÃ©es)")
    print("\nâ³ Attente rÃ©ponse LLM...\n")
    
    response1 = await process_llm_request(
        llm_handler=llm_handler,
        prompt="C'est quoi le gameplay de Brotato?",
        context="ask",
        user_name="testuser",
        game_cache=cache
    )
    
    print("ğŸ¤– RÃ©ponse LLM :")
    print(f"   {response1}")
    print("\nğŸ’¡ Observation : Le LLM utilise ses connaissances gÃ©nÃ©rales")
    
    # ========================================
    # TEST 2 : DonnÃ©es MOYENNES (nom + annÃ©e)
    # ========================================
    print("\n" + "="*80)
    print("ğŸ“Š TEST 2 : DonnÃ©es MOYENNES (nom + annÃ©e 'Hollow Knight')")
    print("-" * 80)
    
    cache.set("hollow_knight", {
        "name": "Hollow Knight",
        "year": "2017",
        "platforms": ["PC", "Switch"]
    })
    
    print("\nğŸ’¬ Question : 'Parle-moi de Hollow Knight'")
    print("ğŸ“ Prompt envoyÃ© : CONTEXTE PARTIEL (annÃ©e + plateformes)")
    print("ğŸ¯ Directive : 'SuggÃ¨re !gameinfo si besoin de plus d'infos'")
    print("\nâ³ Attente rÃ©ponse LLM...\n")
    
    response2 = await process_llm_request(
        llm_handler=llm_handler,
        prompt="Parle-moi de Hollow Knight",
        context="ask",
        user_name="testuser",
        game_cache=cache
    )
    
    print("ğŸ¤– RÃ©ponse LLM :")
    print(f"   {response2}")
    print("\nğŸ’¡ Observation : Le LLM devrait mentionner 2017 et suggÃ©rer !gameinfo")
    
    # ========================================
    # TEST 3 : DonnÃ©es RICHES (genres + description)
    # ========================================
    print("\n" + "="*80)
    print("ğŸ’ TEST 3 : DonnÃ©es RICHES (genres + description 'Celeste')")
    print("-" * 80)
    
    cache.set("celeste", {
        "name": "Celeste",
        "year": "2018",
        "platforms": ["PC", "Switch", "PS4", "Xbox"],
        "genres": ["Platformer", "Indie", "Adventure"],
        "description": "A challenging platformer about climbing a mountain while battling anxiety and self-doubt"
    })
    
    print("\nğŸ’¬ Question : 'C'est quoi Celeste?'")
    print("ğŸ“ Prompt envoyÃ© : CONTEXTE STRICT (toutes les infos)")
    print("ğŸ¯ Directive : 'OBLIGATOIRE : Utilise TOUTES ces infos'")
    print("\nâ³ Attente rÃ©ponse LLM...\n")
    
    response3 = await process_llm_request(
        llm_handler=llm_handler,
        prompt="C'est quoi Celeste?",
        context="ask",
        user_name="testuser",
        game_cache=cache
    )
    
    print("ğŸ¤– RÃ©ponse LLM :")
    print(f"   {response3}")
    print("\nğŸ’¡ Observation : Le LLM devrait mentionner 2018, plateformes, genres, thÃ¨me anxiÃ©tÃ©")
    
    # ========================================
    # TEST 4 : Jeu INCONNU (pas en cache)
    # ========================================
    print("\n" + "="*80)
    print("âŒ TEST 4 : Jeu INCONNU (pas en cache 'Factorio')")
    print("-" * 80)
    
    print("\nğŸ’¬ Question : 'C'est quoi Factorio?'")
    print("ğŸ“ Prompt envoyÃ© : ORIGINAL (jeu non dÃ©tectÃ©)")
    print("\nâ³ Attente rÃ©ponse LLM...\n")
    
    response4 = await process_llm_request(
        llm_handler=llm_handler,
        prompt="C'est quoi Factorio?",
        context="ask",
        user_name="testuser",
        game_cache=cache
    )
    
    print("ğŸ¤– RÃ©ponse LLM :")
    print(f"   {response4}")
    print("\nğŸ’¡ Observation : Le LLM utilise ses connaissances gÃ©nÃ©rales (pas de contexte)")
    
    # Cleanup
    import shutil
    shutil.rmtree(temp_dir)
    
    print("\n" + "="*80)
    print("âœ… Test terminÃ© ! Analyse les diffÃ©rences de rÃ©ponses selon l'enrichissement")
    print("="*80 + "\n")
    
    print("\nğŸ“Š RÃ‰SUMÃ‰ DES STRATÃ‰GIES :")
    print("=" * 80)
    print("1ï¸âƒ£  DonnÃ©es PAUVRES (juste nom) â†’ ORIGINAL â†’ LLM libre")
    print("2ï¸âƒ£  DonnÃ©es MOYENNES (nom+annÃ©e) â†’ PARTIEL â†’ LLM guidÃ© + suggÃ¨re !gameinfo")
    print("3ï¸âƒ£  DonnÃ©es RICHES (genres+desc) â†’ STRICT â†’ LLM contraint (UTILISE TOUT)")
    print("4ï¸âƒ£  Jeu INCONNU â†’ ORIGINAL â†’ LLM libre")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    print("\nğŸ® Assurez-vous que LM Studio tourne sur http://127.0.0.1:1234\n")
    asyncio.run(test_real_llm_adaptive_prompts())
