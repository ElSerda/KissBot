"""
ğŸ§ª Test Optimal Mentions gen_short - Matrice complÃ¨te

Teste les mentions courtes (salutations, questions personnelles):
- Questions: "Ã§a va?", "t'es qui?", "tu fais quoi?", etc.
- Combinaisons: max_tokens (150, 200, 250) Ã— temperature (0.5, 0.7, 0.9)
- Objectif: â‰¤200 chars, rÃ©ponses punchy, naturelles et variÃ©es
"""

import asyncio
import yaml
from pathlib import Path
import sys

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from intelligence.synapses.local_synapse import LocalSynapse


def load_config():
    with open('config/config.yaml', 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


async def test_combination(synapse, question, max_tokens, temperature, repeat_penalty, test_num, total):
    """Test une combinaison spÃ©cifique"""
    print(f"\n{'='*70}")
    print(f"ğŸ§ª TEST {test_num}/{total}: max_tokens={max_tokens} | temp={temperature} | penalty={repeat_penalty}")
    print(f"{'='*70}")
    
    # Utiliser le fire() avec context="mention" et stimulus_class="gen_short"
    response = await synapse.fire(
        stimulus=question,
        context="mention",
        stimulus_class="gen_short",
        correlation_id=f"test_gen_short_{test_num}"
    )
    
    if not response:
        print("âŒ Pas de rÃ©ponse gÃ©nÃ©rÃ©e")
        return None
    
    length = len(response)
    has_emoji = any(char in response for char in "ğŸ˜ğŸ”¥ğŸ’»ğŸ®ğŸ‘âš¡ğŸ’¡")
    phrase_count = response.count('.') + response.count('!') + response.count('?')
    
    # Validation
    is_valid = length <= 200 and phrase_count <= 2
    
    print(f"ğŸ“ Longueur: {length} caractÃ¨res {'âœ…' if length <= 200 else 'âŒ'}")
    print(f"ğŸ“Š Phrases: ~{phrase_count} {'âœ…' if phrase_count <= 2 else 'âŒ'}")
    print(f"ğŸ˜ Emojis: {'OUI âœ…' if has_emoji else 'NON'}")
    
    if length > 200:
        print(f"âš ï¸ DÃ‰PASSEMENT: +{length - 200} caractÃ¨res")
    
    print(f"\nğŸ“ RÃ©ponse complÃ¨te:")
    print(f"   {response}")
    
    # Score de qualitÃ©
    score = 100
    if length > 200:
        score -= (length - 200) // 5  # -1 point par 5 chars de dÃ©passement
    if phrase_count > 2:
        score -= (phrase_count - 2) * 10
    if not has_emoji:
        score -= 10  # Emojis = fun sur Twitch
    if 50 <= length <= 150:
        score += 10  # Bon Ã©quilibre (punchy mais pas trop court)
    
    score = max(0, score)
    
    print(f"\nâ­ Score: {score}/100 {'âœ…' if score >= 80 else 'âŒ'}")
    
    await asyncio.sleep(0.5)
    
    return {
        "max_tokens": max_tokens,
        "temperature": temperature,
        "repeat_penalty": repeat_penalty,
        "length": length,
        "phrases": phrase_count,
        "has_emoji": has_emoji,
        "score": score,
        "response": response,
        "valid": is_valid
    }


async def run_full_matrix_test():
    print("=" * 70)
    print("ğŸ¯ TEST MATRICE: Mentions gen_short (salutations/questions perso)")
    print("=" * 70)
    print("\nğŸ“‹ OBJECTIFS:")
    print("  - â‰¤200 caractÃ¨res (punchy)")
    print("  - 1-2 phrases MAX")
    print("  - Naturel, fun, avec emojis")
    print("  - VariÃ©tÃ© dans les rÃ©ponses")
    print("\n" + "=" * 70)
    
    config = load_config()
    synapse = LocalSynapse(config)
    print("âœ… LocalSynapse initialized\n")
    
    # Questions typiques gen_short
    questions = [
        "Ã§a va?",
        "t'es qui?",
        "tu fais quoi?",
        "tu stream quoi?",
        "t'es cool?",
    ]
    
    print(f"ğŸ“¢ Questions test ({len(questions)} variantes):")
    for q in questions:
        print(f"   â€¢ {q}")
    print()
    
    # Combinaisons Ã  tester
    # Config actuelle: max_tokens=200, temperature=0.7, repeat_penalty=1.1
    configs = [
        # Config actuelle (baseline)
        {"max_tokens": 200, "temperature": 0.7, "repeat_penalty": 1.1},
        
        # Variations max_tokens
        {"max_tokens": 150, "temperature": 0.7, "repeat_penalty": 1.1},
        {"max_tokens": 250, "temperature": 0.7, "repeat_penalty": 1.1},
        
        # Variations temperature (crÃ©ativitÃ©)
        {"max_tokens": 200, "temperature": 0.5, "repeat_penalty": 1.1},
        {"max_tokens": 200, "temperature": 0.9, "repeat_penalty": 1.1},
        
        # Variations repeat_penalty (variÃ©tÃ©)
        {"max_tokens": 200, "temperature": 0.7, "repeat_penalty": 1.0},
        {"max_tokens": 200, "temperature": 0.7, "repeat_penalty": 1.3},
        
        # Combos optimisÃ©s
        {"max_tokens": 150, "temperature": 0.8, "repeat_penalty": 1.2},
        {"max_tokens": 180, "temperature": 0.6, "repeat_penalty": 1.15},
    ]
    
    results = []
    test_num = 0
    total_tests = len(questions) * len(configs)
    
    # Tester chaque question avec chaque config
    for question in questions:
        print(f"\n{'â”€' * 70}")
        print(f"ğŸ“ Question: '{question}'")
        print(f"{'â”€' * 70}")
        
        for cfg in configs:
            test_num += 1
            
            # Modifier temporairement la config du synapse
            # Note: On devrait crÃ©er des synapses avec configs diffÃ©rentes
            # Pour simplifier, on teste avec fire() qui utilise les params par dÃ©faut
            result = await test_combination(
                synapse, question, 
                cfg["max_tokens"], 
                cfg["temperature"], 
                cfg["repeat_penalty"],
                test_num, 
                total_tests
            )
            
            if result:
                result['question'] = question
                results.append(result)
    
    # Analyse finale
    print("\n" + "=" * 70)
    print("ğŸ“Š ANALYSE FINALE")
    print("=" * 70)
    
    if results:
        total_tests = len(results)
        avg_score = sum(r['score'] for r in results) / total_tests
        avg_length = sum(r['length'] for r in results) / total_tests
        over_200_count = sum(1 for r in results if r['length'] > 200)
        with_emoji_count = sum(1 for r in results if r['has_emoji'])
        
        print(f"\nğŸ“ˆ STATISTIQUES GLOBALES ({total_tests} tests):")
        print(f"   â€¢ Score moyen: {avg_score:.1f}/100")
        print(f"   â€¢ Longueur moyenne: {avg_length:.0f} caractÃ¨res")
        print(f"   â€¢ >200 chars: {over_200_count}/{total_tests} ({over_200_count/total_tests*100:.1f}%)")
        print(f"   â€¢ Avec emojis: {with_emoji_count}/{total_tests} ({with_emoji_count/total_tests*100:.1f}%)")
        
        # Trier par score
        results.sort(key=lambda x: x['score'], reverse=True)
        
        print("\nğŸ† TOP 5 CONFIGURATIONS:")
        for i, r in enumerate(results[:5], 1):
            print(f"\n{i}. Score: {r['score']}/100")
            print(f"   - Question: '{r['question']}'")
            print(f"   - max_tokens: {r['max_tokens']}")
            print(f"   - temperature: {r['temperature']}")
            print(f"   - repeat_penalty: {r['repeat_penalty']}")
            print(f"   - Longueur: {r['length']} chars")
            print(f"   - RÃ©ponse: {r['response'][:80]}{'...' if len(r['response']) > 80 else ''}")
        
        # Analyse par configuration (agrÃ©gation)
        print("\nğŸ“Š PERFORMANCE PAR CONFIGURATION:")
        config_stats = {}
        for r in results:
            key = (r['max_tokens'], r['temperature'], r['repeat_penalty'])
            if key not in config_stats:
                config_stats[key] = []
            config_stats[key].append(r['score'])
        
        config_avg = [(k, sum(v)/len(v), len(v)) for k, v in config_stats.items()]
        config_avg.sort(key=lambda x: x[1], reverse=True)
        
        print("\nğŸ¯ TOP 3 CONFIGS (moyenne sur toutes questions):")
        for i, (config, avg, count) in enumerate(config_avg[:3], 1):
            tokens, temp, penalty = config
            print(f"{i}. max_tokens={tokens} | temp={temp} | penalty={penalty}")
            print(f"   â†’ Score moyen: {avg:.1f}/100 (sur {count} tests)")
        
        best_config = config_avg[0][0]
        print(f"\nğŸ’¡ CONFIGURATION OPTIMALE RECOMMANDÃ‰E:")
        print(f"   max_tokens: {best_config[0]}")
        print(f"   temperature: {best_config[1]}")
        print(f"   repeat_penalty: {best_config[2]}")
        print(f"   Score moyen: {config_avg[0][1]:.1f}/100")
        
        # Config actuelle vs meilleure
        current_config = (200, 0.7, 1.1)
        if current_config in config_stats:
            current_avg = sum(config_stats[current_config]) / len(config_stats[current_config])
            best_avg = config_avg[0][1]
            print(f"\nğŸ“‰ COMPARAISON:")
            print(f"   Config actuelle (200, 0.7, 1.1): {current_avg:.1f}/100")
            print(f"   Meilleure config: {best_avg:.1f}/100")
            improvement = best_avg - current_avg
            if improvement > 5:
                print(f"   âœ… AmÃ©lioration: +{improvement:.1f} points")
            elif improvement < -5:
                print(f"   âš ï¸ RÃ©gression: {improvement:.1f} points")
            else:
                print(f"   â¡ï¸ Ã‰quivalent ({improvement:+.1f} points)")
    
    print("\n" + "=" * 70)
    print("âœ… TEST TERMINÃ‰")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(run_full_matrix_test())
