"""
ðŸ§ª Test validation: stream_response_debug="on" config
"""

import yaml
from intelligence.synapses.local_synapse import LocalSynapse


def test_debug_streaming_bool():
    """Test avec debug_streaming: true (bool)"""
    config = {
        "llm": {
            "debug_streaming": True,
            "model_endpoint": "http://127.0.0.1:1234/v1/chat/completions",
            "model_name": "test-model",
            "local_llm": True,
            "language": "fr"
        },
        "neural_llm": {}
    }
    
    synapse = LocalSynapse(config)
    assert synapse.debug_streaming == True, "debug_streaming=True devrait activer debug"
    print("âœ… debug_streaming: true â†’ debug activÃ©")


def test_stream_response_debug_on():
    """Test avec stream_response_debug: "on" (string)"""
    config = {
        "llm": {
            "stream_response_debug": "on",
            "model_endpoint": "http://127.0.0.1:1234/v1/chat/completions",
            "model_name": "test-model",
            "local_llm": True,
            "language": "fr"
        },
        "neural_llm": {}
    }
    
    synapse = LocalSynapse(config)
    assert synapse.debug_streaming == True, "stream_response_debug='on' devrait activer debug"
    print("âœ… stream_response_debug: 'on' â†’ debug activÃ©")


def test_stream_response_debug_off():
    """Test avec stream_response_debug: "off" (dÃ©sactivÃ©)"""
    config = {
        "llm": {
            "stream_response_debug": "off",
            "model_endpoint": "http://127.0.0.1:1234/v1/chat/completions",
            "model_name": "test-model",
            "local_llm": True,
            "language": "fr"
        },
        "neural_llm": {}
    }
    
    synapse = LocalSynapse(config)
    assert synapse.debug_streaming == False, "stream_response_debug='off' devrait dÃ©sactiver debug"
    print("âœ… stream_response_debug: 'off' â†’ debug dÃ©sactivÃ©")


def test_no_config():
    """Test sans config debug (dÃ©faut dÃ©sactivÃ©)"""
    config = {
        "llm": {
            "model_endpoint": "http://127.0.0.1:1234/v1/chat/completions",
            "model_name": "test-model",
            "local_llm": True,
            "language": "fr"
        },
        "neural_llm": {}
    }
    
    synapse = LocalSynapse(config)
    assert synapse.debug_streaming == False, "Sans config, debug devrait Ãªtre dÃ©sactivÃ©"
    print("âœ… Aucune config debug â†’ debug dÃ©sactivÃ© (dÃ©faut)")


def test_both_configs():
    """Test avec les deux configs (prioritÃ© OR)"""
    config = {
        "llm": {
            "debug_streaming": False,  # False
            "stream_response_debug": "on",  # Mais "on" ici
            "model_endpoint": "http://127.0.0.1:1234/v1/chat/completions",
            "model_name": "test-model",
            "local_llm": True,
            "language": "fr"
        },
        "neural_llm": {}
    }
    
    synapse = LocalSynapse(config)
    assert synapse.debug_streaming == True, "OR logic: l'un des deux Ã  True devrait activer"
    print("âœ… debug_streaming=False OR stream_response_debug='on' â†’ debug activÃ©")


if __name__ == "__main__":
    print("=" * 60)
    print("ðŸ§ª TEST CONFIG: stream_response_debug")
    print("=" * 60 + "\n")
    
    test_debug_streaming_bool()
    test_stream_response_debug_on()
    test_stream_response_debug_off()
    test_no_config()
    test_both_configs()
    
    print("\n" + "=" * 60)
    print("âœ… TOUS LES TESTS CONFIG RÃ‰USSIS !")
    print("=" * 60)
    print("\nðŸ’¡ Utilisation recommandÃ©e en production:")
    print('   stream_response_debug: "on"  # Voir chunks en live')
    print('   stream_response_debug: "off" # Production propre')
