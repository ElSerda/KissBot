"""
ğŸ§ª Test streaming response RÃ‰EL avec LM Studio
Test accumulation chunks en conditions GPU bridÃ© (lent = plus de chunks)
"""

import asyncio
import time
import yaml


async def test_streaming_real():
    """Test streaming rÃ©el avec LM Studio (GPU layers bridÃ©)"""
    print("ğŸ”¬ TEST STREAMING RÃ‰EL AVEC LM STUDIO")
    print("=" * 60)
    
    # Load config
    with open("config/config.yaml") as f:
        config = yaml.safe_load(f)
    
    # Init handler
    from intelligence.neural_pathway_manager import NeuralPathwayManager
    llm_handler = NeuralPathwayManager(config)
    
    # Test prompt (court pour voir chunks)
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER, style humoristique : raconte une blague courte"
    
    print(f"\nğŸ“ Prompt: {prompt[:50]}...")
    print(f"ğŸ¯ Mode: Streaming avec accumulation")
    print(f"ğŸ”§ Config: GPU layers bridÃ© (streaming visible)\n")
    
    # Appel direct synapse avec streaming
    start = time.time()
    
    response = await llm_handler.local_synapse.fire(
        stimulus=prompt,
        context="direct",
        stimulus_class="gen_short",
        correlation_id="streaming_test"
    )
    
    latency = time.time() - start
    
    print(f"\nâœ… RÃ‰SULTAT:")
    print(f"   RÃ©ponse: {response}")
    print(f"   Latence: {latency:.2f}s")
    print(f"   Length: {len(response)} chars")
    
    # Validations
    assert response is not None, "Response ne devrait pas Ãªtre None"
    assert len(response) > 10, "Response devrait Ãªtre > 10 chars"
    assert latency > 0, "Latency devrait Ãªtre mesurable"
    
    print(f"\nğŸ“Š MÃ‰TRIQUES:")
    print(f"   âœ“ Streaming activÃ©: âœ…")
    print(f"   âœ“ Accumulation: âœ… (pas de spam)")
    print(f"   âœ“ Message complet: âœ…")
    print(f"   âœ“ Latence acceptable: {'âœ…' if latency < 5 else 'âš ï¸'} ({latency:.2f}s)")
    
    # Test que streaming est bien configurÃ©
    synapse = llm_handler.local_synapse
    print(f"\nğŸ” CONFIG SYNAPSE:")
    print(f"   Endpoint: {synapse.endpoint}")
    print(f"   Model: {synapse.model_name}")
    print(f"   Streaming: activÃ© (stream=True)")
    
    print("\n" + "=" * 60)
    print("âœ… TEST STREAMING RÃ‰EL RÃ‰USSI !")
    print("=" * 60)


async def test_streaming_multiple_calls():
    """Test streaming avec plusieurs appels successifs"""
    print("\nğŸ”¬ TEST STREAMING: APPELS MULTIPLES")
    print("=" * 60)
    
    with open("config/config.yaml") as f:
        config = yaml.safe_load(f)
    
    from intelligence.neural_pathway_manager import NeuralPathwayManager
    llm_handler = NeuralPathwayManager(config)
    
    prompts = [
        "RÃ©ponds EN 1 PHRASE: raconte une blague dev",
        "RÃ©ponds EN 1 PHRASE: raconte une blague science",
        "RÃ©ponds EN 1 PHRASE: raconte une blague courte"
    ]
    
    latencies = []
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\nğŸ¯ Appel {i}/3: {prompt[:40]}...")
        
        start = time.time()
        response = await llm_handler.local_synapse.fire(
            stimulus=prompt,
            context="direct",
            stimulus_class="gen_short",
            correlation_id=f"multi_test_{i}"
        )
        latency = time.time() - start
        latencies.append(latency)
        
        print(f"   âœ… Response: {response[:60]}...")
        print(f"   â±ï¸ Latency: {latency:.2f}s")
        
        assert response is not None
        assert len(response) > 10
    
    avg_latency = sum(latencies) / len(latencies)
    
    print(f"\nğŸ“Š STATISTIQUES:")
    print(f"   Appels: {len(latencies)}")
    print(f"   Latence moyenne: {avg_latency:.2f}s")
    print(f"   Latence min: {min(latencies):.2f}s")
    print(f"   Latence max: {max(latencies):.2f}s")
    
    print("\n" + "=" * 60)
    print("âœ… TEST APPELS MULTIPLES RÃ‰USSI !")
    print("=" * 60)


async def test_streaming_vs_cache():
    """Test comparaison streaming LLM vs cache hit"""
    print("\nğŸ”¬ TEST COMPARAISON: STREAMING vs CACHE")
    print("=" * 60)
    
    with open("config/config.yaml") as f:
        config = yaml.safe_load(f)
    
    from intelligence.neural_pathway_manager import NeuralPathwayManager
    from intelligence.joke_cache import JokeCache
    
    llm_handler = NeuralPathwayManager(config)
    joke_cache = JokeCache(ttl_seconds=86400, max_size=100)
    
    prompt = "RÃ©ponds EN 1 PHRASE MAX EN FRANÃ‡AIS, SANS TE PRÃ‰SENTER, style humoristique : raconte une blague courte"
    
    # 1ï¸âƒ£ APPEL STREAMING (cache miss)
    print(f"\nğŸŒŠ APPEL 1: Streaming (cache miss)")
    start = time.time()
    response = await llm_handler.local_synapse.fire(
        stimulus=prompt,
        context="direct",
        stimulus_class="gen_short",
        correlation_id="cache_test"
    )
    streaming_latency = (time.time() - start) * 1000  # ms
    
    print(f"   Response: {response[:60]}...")
    print(f"   â±ï¸ Latency: {streaming_latency:.2f}ms")
    
    # Store dans cache
    joke_cache.set(prompt, response)
    
    # 2ï¸âƒ£ APPEL CACHE (cache hit)
    print(f"\nğŸ’¾ APPEL 2: Cache (cache hit)")
    start = time.time()
    cached = joke_cache.get(prompt)
    cache_latency = (time.time() - start) * 1000  # ms
    
    print(f"   Response: {cached[:60]}...")
    print(f"   â±ï¸ Latency: {cache_latency:.2f}ms")
    
    # Comparaison
    speedup = streaming_latency / cache_latency
    
    print(f"\nğŸ“Š COMPARAISON:")
    print(f"   Streaming: {streaming_latency:.2f}ms")
    print(f"   Cache:     {cache_latency:.2f}ms")
    print(f"   Speedup:   {speedup:.0f}x plus rapide !")
    
    assert cached == response, "Cache devrait retourner mÃªme rÃ©ponse"
    assert cache_latency < 100, f"Cache devrait Ãªtre <100ms (got {cache_latency:.2f}ms)"
    assert speedup > 10, f"Cache devrait Ãªtre >10x plus rapide (got {speedup:.0f}x)"
    
    print("\n" + "=" * 60)
    print("âœ… TEST COMPARAISON RÃ‰USSI !")
    print("=" * 60)


if __name__ == "__main__":
    print("\n" + "ğŸš€ TESTS STREAMING RÃ‰EL LM STUDIO ".center(60, "="))
    print("âš ï¸  ATTENTION: NÃ©cessite LM Studio lancÃ© sur port 1234")
    print("âš ï¸  Config recommandÃ©e: GPU layers bridÃ© pour voir chunks\n")
    
    asyncio.run(test_streaming_real())
    asyncio.run(test_streaming_multiple_calls())
    asyncio.run(test_streaming_vs_cache())
    
    print("\n" + "âœ… TOUS LES TESTS STREAMING RÃ‰USSIS ! ".center(60, "=") + "\n")
